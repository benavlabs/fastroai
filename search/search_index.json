{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastroAI","text":"<p> Lightweight AI orchestration built on PydanticAI. </p> <p> </p> <p> FastroAI wraps PydanticAI with production essentials: cost tracking in microcents, multi-step pipelines, and tools that handle failures gracefully. You get everything PydanticAI offers, plus the infrastructure you'd build yourself anyway. </p> <p>Note: FastroAI is experimental, it was extracted into a package from code that we had in production in different contexts. We built it for ourselves but you're free to use and contribute. The API may change between versions and you'll probably find bugs, we're here to fix them. Use in production at your own risk (we do).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Cost Tracking: Automatic cost calculation in microcents. Integer math, no floating-point drift.</li> <li>Pipelines: DAG-based workflows with automatic parallelization and dependency resolution.</li> <li>Safe Tools: Timeout, retry, and graceful error handling for AI tools.</li> <li>Tracing: Protocol-based integration with any observability platform.</li> <li>Structured Output: Type-safe responses with Pydantic models.</li> <li>Streaming: Real-time responses with cost tracking on the final chunk.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+: Modern async/await and type hints.</li> <li>AI API Key: OpenAI, Anthropic, Google, or other provider.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-install-fastroai","title":"1. Install FastroAI","text":"pipuvpoetry <pre><code>pip install fastroai\n</code></pre> <pre><code>uv add fastroai\n</code></pre> <pre><code>poetry add fastroai\n</code></pre>"},{"location":"#2-set-your-api-key","title":"2. Set Your API Key","text":"OpenAIAnthropicGoogle <pre><code>export OPENAI_API_KEY=\"sk-your-key-here\"\n</code></pre> <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-your-key-here\"\n</code></pre> <pre><code>export GOOGLE_API_KEY=\"your-key-here\"\n</code></pre>"},{"location":"#3-run-your-first-agent","title":"3. Run Your First Agent","text":"<pre><code>import asyncio\nfrom fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\nasync def main():\n    response = await agent.run(\"What is 2 + 2?\")\n\n    print(response.content)\n    print(f\"Tokens: {response.input_tokens} in, {response.output_tokens} out\")\n    print(f\"Cost: ${response.cost_dollars:.6f}\")\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>2 + 2 equals 4.\nTokens: 24 in, 8 out\nCost: $0.000120\n</code></pre> <p>That's it. You have an AI agent with automatic cost tracking.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#single-agent-calls","title":"Single Agent Calls","text":"<p><code>FastroAgent</code> is a thin wrapper around PydanticAI's Agent. It adds automatic cost tracking and a consistent response format, but otherwise stays out of your way. All PydanticAI features work exactly as documented:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\nresponse = await agent.run(\"What is the capital of France?\")\nprint(response.content)\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\n</code></pre>"},{"location":"#structured-output","title":"Structured Output","text":"<p>Get Pydantic models back instead of strings:</p> <pre><code>from pydantic import BaseModel\nfrom fastroai import FastroAgent\n\nclass MovieReview(BaseModel):\n    title: str\n    rating: int\n    summary: str\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    output_type=MovieReview,\n)\n\nresponse = await agent.run(\"Review the movie Inception\")\nprint(response.output.title)   # \"Inception\"\nprint(response.output.rating)  # 9\n</code></pre>"},{"location":"#multi-step-pipelines","title":"Multi-Step Pipelines","text":"<p>Real applications often need multiple AI calls: extract entities, then classify them, then generate a response. You could chain these manually with <code>await</code>, but then you're writing boilerplate for dependency ordering, parallel execution, and cost aggregation.</p> <p>Pipelines handle this. Declare your steps and dependencies, and FastroAI runs them in the right order, parallelizes independent steps, and tracks costs across the whole workflow. All FastroAI features flow through: you get cost tracking per step and per pipeline, plus distributed tracing across the entire flow.</p> <p>For simple DAG workflows, this is less verbose than pydantic-graph and far simpler than durable execution frameworks like Temporal. It's enough for most AI orchestration needs:</p> <pre><code>from fastroai import Pipeline, step, StepContext, FastroAgent\n\nextractor = FastroAgent(model=\"openai:gpt-4o-mini\", system_prompt=\"Extract entities.\")\nclassifier = FastroAgent(model=\"openai:gpt-4o-mini\", system_prompt=\"Classify documents.\")\n\n@step\nasync def extract(ctx: StepContext[None]) -&gt; str:\n    document = ctx.get_input(\"document\")\n    response = await ctx.run(extractor, f\"Extract entities: {document}\")\n    return response.content\n\n@step(timeout=30.0, retries=2)\nasync def classify(ctx: StepContext[None]) -&gt; str:\n    entities = ctx.get_dependency(\"extract\")\n    response = await ctx.run(classifier, f\"Classify based on: {entities}\")\n    return response.content\n\npipeline = Pipeline(\n    name=\"document_processor\",\n    steps={\"extract\": extract, \"classify\": classify},\n    dependencies={\"classify\": [\"extract\"]},\n)\n\nresult = await pipeline.execute({\"document\": \"Apple announced...\"}, deps=None)\nprint(f\"Total cost: ${result.usage.total_cost_dollars:.6f}\")\n</code></pre>"},{"location":"#safe-tools","title":"Safe Tools","text":"<p>When you give an AI agent tools that call external APIs, those APIs will eventually fail. They'll time out, return errors, or hang indefinitely. With regular tools, this crashes your entire request and the user sees an error page.</p> <p><code>@safe_tool</code> wraps tools with timeout, retry, and graceful error handling. When something fails, instead of raising an exception, the AI receives an error message it can work with:</p> <pre><code>from fastroai import safe_tool\n\n@safe_tool(timeout=10, max_retries=2)\nasync def fetch_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(f\"https://api.weather.com/{location}\")\n        return resp.text\n</code></pre> <p>If the API times out, the AI sees \"Tool timed out after 2 attempts\" and can respond: \"I'm having trouble checking the weather right now. Would you like me to try again?\" Your request doesn't crash, you don't lose the prompt tokens, and the user gets a response.</p>"},{"location":"#response-fields","title":"Response Fields","text":"<p>Every <code>ChatResponse</code> includes:</p> Field Type Description <code>content</code> <code>str</code> The response text <code>output</code> <code>OutputT</code> Typed output (same as content for string agents) <code>input_tokens</code> <code>int</code> Prompt tokens consumed <code>output_tokens</code> <code>int</code> Completion tokens generated <code>cost_microcents</code> <code>int</code> Cost in 1/1,000,000 of a dollar <code>cost_dollars</code> <code>float</code> Cost in dollars (for display) <code>processing_time_ms</code> <code>int</code> Wall-clock time <code>trace_id</code> <code>str</code> Tracing correlation ID <p>Use <code>cost_microcents</code> when aggregating costs across many calls. Use <code>cost_dollars</code> for display.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"#fastroai-template","title":"FastroAI Template","text":"<p>Looking for a complete AI SaaS starter? FastroAI Template includes authentication, payments, background tasks, and more built on top of this library.</p>          Start Learning               Browse Guides"},{"location":"changelog/","title":"FastroAI Changelog","text":""},{"location":"changelog/#introduction","title":"Introduction","text":"<p>The Changelog documents all notable changes made to FastroAI. This includes new features, bug fixes, and improvements. It's organized by version and date, providing a clear history of the library's development.</p>"},{"location":"changelog/#030-dec-17-2025","title":"[0.3.0] - Dec 17, 2025","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>LogfireTracer by @igorbenav</li> <li>Built-in tracer implementation for Pydantic's Logfire observability platform</li> <li>Implements the <code>Tracer</code> protocol - drop-in replacement for <code>SimpleTracer</code></li> <li>Automatic span creation with <code>_tags=[\"fastroai\"]</code> for easy filtering in Logfire dashboard</li> <li>Metric logging via <code>logfire.info()</code> with trace correlation</li> <li>Error logging with full exception info via <code>logfire.error()</code></li> <li> <p>Clear <code>ImportError</code> when logfire package is not installed</p> </li> <li> <p>Optional Dependency by @igorbenav</p> </li> <li>Install with <code>pip install fastroai[logfire]</code> to enable Logfire support</li> <li>Logfire remains optional - core functionality works without it</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Added LogfireTracer to built-in tracers section in tracing guide</li> <li>Added LogfireTracer to API reference</li> <li>Updated README with Logfire installation instructions</li> </ul>"},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>LogfireTracer implementation by @igorbenav</li> </ul> <p>Full Changelog: https://github.com/benavlabs/fastroai/compare/v0.2.0...v0.3.0</p>"},{"location":"changelog/#020-dec-16-2025","title":"[0.2.0] - Dec 16, 2025","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>FastroAgent - PydanticAI wrapper with automatic cost calculation and tracing</li> <li>Pipeline - DAG-based workflow orchestration with automatic parallelization</li> <li>@step decorator - Concise function-based pipeline step definitions</li> <li>@safe_tool - Production-safe tool decorator with timeout and retry</li> <li>CostCalculator - Precise cost tracking using microcents (integer arithmetic)</li> <li>Tracer Protocol - Protocol-based tracing interface for observability integration</li> <li>SimpleTracer - Logging-based tracer for development</li> <li>NoOpTracer - No-op tracer for testing or disabled tracing</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Complete documentation site with guides, API reference, and recipes</li> <li>MkDocs Material theme with dark/light mode support</li> </ul> <p>Full Changelog: https://github.com/benavlabs/fastroai/compare/v0.1.0...v0.2.0</p>"},{"location":"changelog/#010-dec-15-2025","title":"[0.1.0] - Dec 15, 2025","text":"<p>Initial release.</p> <ul> <li>Core FastroAgent functionality</li> <li>Basic pipeline support</li> <li>Cost calculation primitives</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for FastroAI's public API.</p> <p>All classes, functions, and protocols documented here are considered stable and follow semantic versioning.</p> <p>Looking for explanations?</p> <p>This is a reference, not a tutorial. For explanations and examples, see the Guides.</p>"},{"location":"api/#core-components","title":"Core Components","text":"<ul> <li> <p> Agent</p> <p>FastroAgent, AgentConfig, ChatResponse, StreamChunk</p> <p> Learn more</p> </li> <li> <p> Pipelines</p> <p>Pipeline, BaseStep, StepContext, configurations</p> <p> Learn more</p> </li> <li> <p> Tools</p> <p>@safe_tool decorator, SafeToolset, FunctionToolsetBase</p> <p> Learn more</p> </li> <li> <p> Usage</p> <p>CostCalculator with microcents precision</p> <p> Learn more</p> </li> <li> <p> Tracing</p> <p>Tracer protocol, SimpleTracer, LogfireTracer, NoOpTracer</p> <p> Learn more</p> </li> </ul>"},{"location":"api/#quick-import-reference","title":"Quick Import Reference","text":"<pre><code>from fastroai import (\n    # Agent\n    FastroAgent,\n    AgentConfig,\n    ChatResponse,\n    StreamChunk,\n\n    # Pipelines\n    Pipeline,\n    PipelineResult,\n    PipelineConfig,\n    BaseStep,\n    StepContext,\n    StepConfig,\n    step,\n    ConversationState,\n    ConversationStatus,\n\n    # Tools\n    safe_tool,\n    SafeToolset,\n    FunctionToolsetBase,\n\n    # Tracing\n    Tracer,\n    SimpleTracer,\n    LogfireTracer,\n    NoOpTracer,\n\n    # Usage\n    CostCalculator,\n\n    # Errors\n    FastroAIError,\n    PipelineValidationError,\n    CostBudgetExceededError,\n)\n</code></pre>"},{"location":"api/#error-hierarchy","title":"Error Hierarchy","text":"<p>All FastroAI exceptions inherit from <code>FastroAIError</code>, so you can catch all library errors with a single except clause:</p> <pre><code>FastroAIError                    # Base for all FastroAI errors\n\u251c\u2500\u2500 PipelineValidationError      # Invalid pipeline configuration\n\u251c\u2500\u2500 StepExecutionError           # Step failed during execution\n\u2514\u2500\u2500 CostBudgetExceededError      # Cost budget exceeded\n</code></pre> <pre><code>try:\n    result = await pipeline.execute(inputs, deps)\nexcept FastroAIError as e:\n    logger.error(f\"FastroAI error: {e}\")\n</code></pre> <p>\u2190 Recipes Agent \u2192</p>"},{"location":"api/agent/","title":"Agent","text":"<p>The agent module provides <code>FastroAgent</code>, a wrapper around PydanticAI's Agent with automatic cost calculation, distributed tracing, and a consistent response format.</p>"},{"location":"api/agent/#fastroagent","title":"FastroAgent","text":""},{"location":"api/agent/#fastroai.agent.FastroAgent","title":"<code>fastroai.agent.FastroAgent</code>","text":"<p>AI agent with usage tracking, cost calculation, and tracing.</p> <p>Wraps PydanticAI's Agent to provide: - Automatic cost calculation in microcents - Optional distributed tracing - Streaming and non-streaming modes - Consistent ChatResponse format - Structured output support via output_type</p> <p>The agent is STATELESS regarding conversation history. Callers load history from their storage and pass it to run().</p> <p>Examples:</p> <pre><code># Basic usage (returns string)\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are helpful.\",\n)\nresponse = await agent.run(\"Hello!\")\nprint(response.content)\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\n\n# With structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    value: int\n    explanation: str\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    output_type=Answer,\n)\nresponse = await agent.run(\"What is 2+2?\")\nprint(response.output.value)  # 4\n\n# With conversation history (you load it)\nhistory = await my_memory_service.load(user_id)\nresponse = await agent.run(\"Continue\", message_history=history)\nawait my_memory_service.save(user_id, \"Continue\", response.content)\n\n# With tracing\nfrom fastroai import SimpleTracer\ntracer = SimpleTracer()\nresponse = await agent.run(\"Hello\", tracer=tracer)\n\n# With custom deps for tools\nresponse = await agent.run(\"Search for news\", deps=MyDeps(api_key=\"...\"))\n\n# Streaming\nasync for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.agent","title":"<code>agent</code>  <code>property</code>","text":"<p>Access the underlying PydanticAI agent.</p> <p>Returns:</p> Type Description <code>Agent[Any, OutputT]</code> <p>The wrapped PydanticAI Agent instance.</p>"},{"location":"api/agent/#fastroai.agent.FastroAgent.__init__","title":"<code>__init__(config=None, agent=None, output_type=None, toolsets=None, cost_calculator=None, **kwargs)</code>","text":"<p>Initialize FastroAgent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AgentConfig | None</code> <p>Agent configuration. If None, creates from kwargs.</p> <code>None</code> <code>agent</code> <code>Agent[Any, OutputT] | None</code> <p>Pre-configured PydanticAI Agent (escape hatch).   If provided, config is only used for cost calculation.</p> <code>None</code> <code>output_type</code> <code>type[OutputT] | None</code> <p>Pydantic model for structured output. Defaults to str.</p> <code>None</code> <code>toolsets</code> <code>list[AbstractToolset] | None</code> <p>Tool sets available to the agent.</p> <code>None</code> <code>cost_calculator</code> <code>CostCalculator | None</code> <p>Cost calculator. Default uses standard pricing.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to AgentConfig if config is None.      Common: model, system_prompt, temperature, max_tokens.</p> <code>{}</code> <p>Examples:</p> <pre><code># Using config object\nconfig = AgentConfig(model=\"gpt-4o\", temperature=0.3)\nagent = FastroAgent(config=config)\n\n# Using kwargs (simpler)\nagent = FastroAgent(model=\"gpt-4o\", temperature=0.5)\n\n# With structured output\nagent = FastroAgent(model=\"gpt-4o\", output_type=MyResponseModel)\n\n# Custom pricing override (e.g., volume discount)\ncalc = CostCalculator(pricing_overrides={\n    \"gpt-4o\": {\"input_per_mtok\": 2.00, \"output_per_mtok\": 8.00}\n})\nagent = FastroAgent(cost_calculator=calc)\n\n# Escape hatch: your own PydanticAI agent\nfrom pydantic_ai import Agent\npydantic_agent = Agent(model=\"gpt-4o\", output_type=MyType)\nagent = FastroAgent(agent=pydantic_agent)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.run","title":"<code>run(message, deps=None, message_history=None, model_settings=None, tracer=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a single agent interaction.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>User message to send.</p> required <code>deps</code> <code>DepsT | None</code> <p>Dependencies passed to tools. Can be any type.</p> <code>None</code> <code>message_history</code> <code>list[ModelMessage] | None</code> <p>Previous messages (you load these from your storage).</p> <code>None</code> <code>model_settings</code> <code>ModelSettings | None</code> <p>Runtime model config overrides.</p> <code>None</code> <code>tracer</code> <code>Tracer | None</code> <p>Tracer for distributed tracing.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to PydanticAI Agent.run().</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatResponse[OutputT]</code> <p>ChatResponse with content, usage, cost, and trace_id.</p> <p>Examples:</p> <pre><code># Simple usage\nresponse = await agent.run(\"Hello!\")\nprint(response.content)\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\n\n# With conversation history\nhistory = await memory.load(user_id)\nresponse = await agent.run(\"Continue\", message_history=history)\nawait memory.save(user_id, \"Continue\", response.content)\n\n# With tracing\ntracer = SimpleTracer()\nresponse = await agent.run(\"Hello\", tracer=tracer)\nprint(f\"Trace ID: {response.trace_id}\")\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.run_stream","title":"<code>run_stream(message, deps=None, message_history=None, model_settings=None, tracer=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a streaming agent interaction.</p> <p>Yields StreamChunk objects as the response is generated. The final chunk has is_final=True and includes complete usage data.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>User message to send.</p> required <code>deps</code> <code>DepsT | None</code> <p>Dependencies passed to tools.</p> <code>None</code> <code>message_history</code> <code>list[ModelMessage] | None</code> <p>Previous messages.</p> <code>None</code> <code>model_settings</code> <code>ModelSettings | None</code> <p>Runtime model config overrides.</p> <code>None</code> <code>tracer</code> <code>Tracer | None</code> <p>Tracer for distributed tracing.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to PydanticAI Agent.run_stream().</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamChunk[OutputT], None]</code> <p>StreamChunk objects. Final chunk has usage_data.</p> <p>Examples:</p> <pre><code>async for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.as_step","title":"<code>as_step(prompt)</code>","text":"<p>Turn this agent into a pipeline step.</p> <p>Creates a BaseStep that runs this agent with the given prompt and returns the agent's output directly.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Callable[[StepContext[DepsT]], str] | str</code> <p>Either a static string or a function that builds    the prompt from the step context.</p> required <p>Returns:</p> Type Description <code>AgentStepWrapper[DepsT, OutputT]</code> <p>A BaseStep that can be used in a Pipeline.</p> <p>Examples:</p> <pre><code># Static prompt\nagent = FastroAgent(model=\"gpt-4o\", system_prompt=\"Summarize text.\")\nstep = agent.as_step(\"Summarize the document.\")\n\n# Dynamic prompt from context\nagent = FastroAgent(model=\"gpt-4o\", system_prompt=\"Summarize text.\")\nstep = agent.as_step(lambda ctx: f\"Summarize: {ctx.get_input('doc')}\")\n\n# With structured output\nagent = FastroAgent(model=\"gpt-4o\", output_type=Summary)\nstep = agent.as_step(lambda ctx: f\"Summarize: {ctx.get_input('doc')}\")\n# step returns Summary directly\n\n# Use in pipeline\npipeline = Pipeline(\n    name=\"summarizer\",\n    steps={\"summarize\": step},\n)\n</code></pre>"},{"location":"api/agent/#agentconfig","title":"AgentConfig","text":""},{"location":"api/agent/#fastroai.agent.AgentConfig","title":"<code>fastroai.agent.AgentConfig</code>","text":"<p>Configuration for FastroAgent instances.</p> <p>All parameters have sensible defaults. Override as needed.</p> <p>Examples:</p> <pre><code># Minimal - uses all defaults\nconfig = AgentConfig()\n\n# Custom configuration\nconfig = AgentConfig(\n    model=\"anthropic:claude-3-5-sonnet\",\n    system_prompt=\"You are a financial advisor.\",\n    temperature=0.3,\n)\n\n# Use with agent\nagent = FastroAgent(config=config)\n\n# Or pass kwargs directly to FastroAgent\nagent = FastroAgent(model=\"openai:gpt-4o-mini\", temperature=0.5)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.AgentConfig.max_retries","title":"<code>max_retries = Field(default=DEFAULT_MAX_RETRIES, ge=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum retry attempts on failure.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.max_tokens","title":"<code>max_tokens = DEFAULT_MAX_TOKENS</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum tokens in response.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.model","title":"<code>model = DEFAULT_MODEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'openai:gpt-4o', 'anthropic:claude-3-5-sonnet').</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.system_prompt","title":"<code>system_prompt = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>System prompt. If None, uses DEFAULT_SYSTEM_PROMPT.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.temperature","title":"<code>temperature = Field(default=DEFAULT_TEMPERATURE, ge=0.0, le=2.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sampling temperature (0.0 = deterministic, 2.0 = creative).</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.timeout_seconds","title":"<code>timeout_seconds = Field(default=DEFAULT_TIMEOUT_SECONDS, gt=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Request timeout in seconds.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.get_effective_system_prompt","title":"<code>get_effective_system_prompt()</code>","text":"<p>Get system prompt, using default if not set.</p> <p>Returns:</p> Type Description <code>str</code> <p>The configured system prompt or DEFAULT_SYSTEM_PROMPT.</p>"},{"location":"api/agent/#chatresponse","title":"ChatResponse","text":""},{"location":"api/agent/#fastroai.agent.ChatResponse","title":"<code>fastroai.agent.ChatResponse</code>","text":"<p>Response from an AI agent interaction.</p> <p>Contains the response content plus comprehensive usage metrics for billing, analytics, and debugging.</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>OutputT</code> <p>The typed output from the agent. For string agents, same as content.</p> <code>content</code> <code>str</code> <p>String representation of the output.</p> <code>model</code> <code>str</code> <p>Model that generated the response.</p> <code>input_tokens</code> <code>int</code> <p>Tokens consumed by input/prompt.</p> <code>output_tokens</code> <code>int</code> <p>Tokens in response/completion.</p> <code>total_tokens</code> <code>int</code> <p>input_tokens + output_tokens.</p> <code>tool_calls</code> <code>list[dict[str, Any]]</code> <p>Tools invoked during generation.</p> <code>cost_microcents</code> <code>int</code> <p>Cost in 1/10,000ths of a cent (integer).</p> <code>processing_time_ms</code> <code>int</code> <p>Wall-clock time in milliseconds.</p> <code>trace_id</code> <code>str | None</code> <p>Distributed tracing correlation ID.</p> <p>Examples:</p> <pre><code>response = await agent.run(\"What is 2+2?\")\n\nprint(f\"Answer: {response.content}\")\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\nprint(f\"Tokens: {response.total_tokens}\")\n\nif response.tool_calls:\n    for call in response.tool_calls:\n        print(f\"Used tool: {call['tool_name']}\")\n\n# With structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    value: int\n    explanation: str\n\nagent = FastroAgent(output_type=Answer)\nresponse = await agent.run(\"What is 2+2?\")\nprint(response.output.value)  # 4\nprint(response.output.explanation)  # \"2 plus 2 equals 4\"\n</code></pre> Note <p>Why microcents? Floating-point math has precision errors:</p> <p>0.1 + 0.2 0.30000000000000004</p> <p>With microcents (integers), precision is exact:</p> <p>100 + 200 300</p> <p>For billing systems, this matters.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.content","title":"<code>content</code>  <code>instance-attribute</code>","text":"<p>String representation of the output.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.cost_dollars","title":"<code>cost_dollars</code>  <code>property</code>","text":"<p>Cost in dollars for display purposes.</p> <p>Returns:</p> Type Description <code>float</code> <p>Cost as a float in dollars.</p> Note <p>Use cost_microcents for calculations to avoid floating-point errors.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.cost_microcents","title":"<code>cost_microcents</code>  <code>instance-attribute</code>","text":"<p>Cost in microcents (1/1,000,000 dollar).</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.input_tokens","title":"<code>input_tokens</code>  <code>instance-attribute</code>","text":"<p>Tokens consumed by input/prompt.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model that generated the response.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.output","title":"<code>output</code>  <code>instance-attribute</code>","text":"<p>The typed output from the agent.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.output_tokens","title":"<code>output_tokens</code>  <code>instance-attribute</code>","text":"<p>Tokens in response/completion.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.processing_time_ms","title":"<code>processing_time_ms</code>  <code>instance-attribute</code>","text":"<p>Wall-clock processing time in milliseconds.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.tool_calls","title":"<code>tool_calls = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tools invoked during generation.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.total_tokens","title":"<code>total_tokens</code>  <code>instance-attribute</code>","text":"<p>Total tokens (input + output).</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.trace_id","title":"<code>trace_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Distributed tracing correlation ID.</p>"},{"location":"api/agent/#streamchunk","title":"StreamChunk","text":""},{"location":"api/agent/#fastroai.agent.StreamChunk","title":"<code>fastroai.agent.StreamChunk</code>","text":"<p>A chunk in a streaming response.</p> <p>Most chunks have content with is_final=False. The last chunk has is_final=True with complete usage data.</p> <p>Examples:</p> <pre><code>async for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nTotal cost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.StreamChunk.content","title":"<code>content = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Text content of this chunk.</p>"},{"location":"api/agent/#fastroai.agent.StreamChunk.is_final","title":"<code>is_final = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>True if this is the final chunk with usage data.</p>"},{"location":"api/agent/#fastroai.agent.StreamChunk.usage_data","title":"<code>usage_data = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Complete usage data (only on final chunk).</p>"},{"location":"api/agent/#agentstepwrapper","title":"AgentStepWrapper","text":"<p>\u2190 API Overview Pipelines \u2192</p>"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper","title":"<code>fastroai.agent.AgentStepWrapper</code>","text":"<p>Pipeline step wrapper for FastroAgent.</p> <p>Created via FastroAgent.as_step(). Wraps an agent as a pipeline step.</p> <p>The wrapper uses ctx.run() for automatic tracer/deps forwarding and usage tracking, and returns the agent's typed output directly.</p> Note <p>Use FastroAgent.as_step() to create instances rather than instantiating directly.</p>"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper.agent","title":"<code>agent</code>  <code>property</code>","text":"<p>Access the underlying FastroAgent.</p> <p>Returns:</p> Type Description <code>FastroAgent[OutputT]</code> <p>The wrapped FastroAgent instance.</p>"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper.__init__","title":"<code>__init__(agent, prompt)</code>","text":"<p>Initialize the step wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>FastroAgent[OutputT]</code> <p>The FastroAgent to wrap.</p> required <code>prompt</code> <code>Callable[[StepContext[DepsT]], str] | str</code> <p>Static string or function that builds the prompt from context.</p> required"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper.execute","title":"<code>execute(context)</code>  <code>async</code>","text":"<p>Execute the agent with the configured prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StepContext[DepsT]</code> <p>Step execution context with inputs, deps, and config.</p> required <p>Returns:</p> Type Description <code>OutputT</code> <p>The agent's typed output.</p>"},{"location":"api/pipelines/","title":"Pipelines","text":"<p>The pipelines module provides DAG-based workflow orchestration with automatic parallelism, multi-turn conversation support, and usage tracking.</p>"},{"location":"api/pipelines/#pipeline","title":"Pipeline","text":""},{"location":"api/pipelines/#fastroai.pipelines.Pipeline","title":"<code>fastroai.pipelines.Pipeline</code>","text":"<p>Declarative DAG pipeline for multi-step AI workflows.</p> <p>Features: - Automatic parallelism from dependencies - Type-safe dependency access - Early termination on INCOMPLETE status - Aggregated usage tracking - Distributed tracing</p> <p>Examples:</p> <pre><code>pipeline = Pipeline(\n    name=\"document_processor\",\n    steps={\n        \"extract\": ExtractStep(),\n        \"classify\": ClassifyStep(),\n        \"summarize\": SummarizeStep(),\n    },\n    dependencies={\n        \"classify\": [\"extract\"],\n        \"summarize\": [\"classify\"],\n    },\n)\n\nresult = await pipeline.execute({\"document\": doc}, deps, tracer)\nsummary = result.output\n</code></pre> <p>Parallelism example - steps at the same level run in parallel:</p> <pre><code>dependencies = {\n    \"classify\": [\"extract\"],\n    \"fetch_market\": [\"classify\"],\n    \"fetch_user\": [\"classify\"],  # Same dep as above\n    \"calculate\": [\"fetch_market\", \"fetch_user\"],\n}\n# Execution:\n# Level 0: extract\n# Level 1: classify\n# Level 2: fetch_market, fetch_user (PARALLEL)\n# Level 3: calculate\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.Pipeline.__init__","title":"<code>__init__(name, steps, dependencies=None, output_step=None, config=None, step_configs=None)</code>","text":"<p>Initialize Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name (for tracing).</p> required <code>steps</code> <code>dict[str, BaseStep[DepsT, Any]]</code> <p>Dict of step_id -&gt; step instance.</p> required <code>dependencies</code> <code>dict[str, list[str]] | None</code> <p>Dict of step_id -&gt; [dependency_ids].</p> <code>None</code> <code>output_step</code> <code>str | None</code> <p>Which step's output is the pipeline output.         Defaults to last step in topological order.</p> <code>None</code> <code>config</code> <code>PipelineConfig | None</code> <p>Default configuration for all steps (timeout, retries, budget).</p> <code>None</code> <code>step_configs</code> <code>dict[str, StepConfig] | None</code> <p>Per-step configuration overrides.</p> <code>None</code> <p>Config Resolution (most specific wins):     1. Pipeline default config     2. Step class config (if step has .config attribute)     3. step_configs[step_id] override     4. Per-call overrides via ctx.run(timeout=..., retries=...)</p> <p>Raises:</p> Type Description <code>PipelineValidationError</code> <p>Invalid deps, unknown output_step, or cycles.</p> <p>Examples:</p> <pre><code>pipeline = Pipeline(\n    name=\"processor\",\n    steps={\"extract\": ExtractStep(), \"classify\": ClassifyStep()},\n    config=PipelineConfig(timeout=30.0, retries=1),\n    step_configs={\"classify\": StepConfig(timeout=60.0)},  # Override\n)\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.Pipeline.execute","title":"<code>execute(input_data, deps, tracer=None)</code>  <code>async</code>","text":"<p>Execute the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>InputT</code> <p>Input accessible via context.get_input().</p> required <code>deps</code> <code>DepsT</code> <p>Your deps accessible via context.deps.</p> required <code>tracer</code> <code>Tracer | None</code> <p>For distributed tracing.</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineResult[OutputT]</code> <p>PipelineResult with output and usage.</p> <p>Raises:</p> Type Description <code>StepExecutionError</code> <p>If any step fails.</p> <p>Examples:</p> <pre><code># Basic execution\nresult = await pipeline.execute(\n    {\"document\": \"Hello world\"},\n    deps=MyDeps(api_key=\"...\"),\n)\nprint(result.output)\n\n# With tracing\nfrom fastroai import SimpleTracer\n\ntracer = SimpleTracer()\nresult = await pipeline.execute(\n    {\"document\": doc},\n    deps=deps,\n    tracer=tracer,\n)\n\n# Handle early termination (multi-turn)\nif result.stopped_early:\n    missing = result.conversation_state.context[\"missing\"]\n    return {\"status\": \"incomplete\", \"missing\": missing}\n\n# Access usage metrics\nif result.usage:\n    print(f\"Cost: ${result.usage.total_cost_dollars:.6f}\")\n    print(f\"Tokens: {result.usage.total_input_tokens}\")\n</code></pre>"},{"location":"api/pipelines/#pipelineresult","title":"PipelineResult","text":""},{"location":"api/pipelines/#fastroai.pipelines.PipelineResult","title":"<code>fastroai.pipelines.PipelineResult</code>","text":"<p>Result from pipeline execution.</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>OutputT | None</code> <p>Final step's output, or None if stopped early.</p> <code>step_outputs</code> <code>dict[str, Any]</code> <p>All step outputs by ID.</p> <code>conversation_state</code> <code>ConversationState[Any] | None</code> <p>ConversationState if a step returned one.</p> <code>usage</code> <code>PipelineUsage | None</code> <p>Aggregated usage metrics.</p> <code>stopped_early</code> <code>bool</code> <p>True if stopped due to INCOMPLETE status.</p> <p>Examples:</p> <pre><code>result = await pipeline.execute(data, deps)\n\nif result.stopped_early:\n    missing = result.conversation_state.context[\"missing\"]\n    return {\"status\": \"incomplete\", \"missing\": missing}\n\nprint(f\"Cost: ${result.usage.total_cost_dollars:.6f}\")\nreturn {\"status\": \"complete\", \"output\": result.output}\n</code></pre>"},{"location":"api/pipelines/#basestep","title":"BaseStep","text":""},{"location":"api/pipelines/#fastroai.pipelines.BaseStep","title":"<code>fastroai.pipelines.BaseStep</code>","text":"<p>Abstract base class for pipeline steps.</p> <p>A step is one unit of work. It: - Receives context with inputs and dependencies - Does something (AI call, computation, API call) - Returns typed output</p> <p>Steps should be stateless. Any state goes in deps or inputs.</p> <p>Examples:</p> <pre><code>class ExtractStep(BaseStep[MyDeps, ExtractionResult]):\n    '''Extract entities from document.'''\n\n    def __init__(self):\n        self.agent = FastroAgent(system_prompt=\"Extract entities.\")\n\n    async def execute(self, context: StepContext[MyDeps]) -&gt; ExtractionResult:\n        document = context.get_input(\"document\")\n        response = await self.agent.run(f\"Extract: {document}\")\n        return ExtractionResult.model_validate_json(response.content)\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.BaseStep.execute","title":"<code>execute(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute step logic.</p> <p>Override this method to implement your step's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StepContext[DepsT]</code> <p>Execution context with inputs, deps, and step outputs.</p> required <p>Returns:</p> Type Description <code>OutputT</code> <p>The step's typed output.</p>"},{"location":"api/pipelines/#stepcontext","title":"StepContext","text":""},{"location":"api/pipelines/#fastroai.pipelines.StepContext","title":"<code>fastroai.pipelines.StepContext</code>","text":"<p>Execution context provided to pipeline steps.</p> <p>Provides access to: - Pipeline inputs (the data passed to execute()) - Outputs from dependency steps - Application dependencies (your db session, user, etc.) - Tracer for custom spans</p> <p>Examples:</p> <pre><code>class ProcessStep(BaseStep[MyDeps, Result]):\n    async def execute(self, context: StepContext[MyDeps]) -&gt; Result:\n        # Get pipeline input\n        document = context.get_input(\"document\")\n\n        # Get output from dependency step\n        classification = context.get_dependency(\"classify\", Classification)\n\n        # Access your deps\n        db = context.deps.session\n        user_id = context.deps.user_id\n\n        # Custom tracing\n        if context.tracer:\n            async with context.tracer.span(\"custom_operation\"):\n                result = await process(document)\n\n        return result\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.step_id","title":"<code>step_id</code>  <code>property</code>","text":"<p>Current step's ID.</p> <p>Returns:</p> Type Description <code>str</code> <p>The step identifier string.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.deps","title":"<code>deps</code>  <code>property</code>","text":"<p>Application dependencies (your session, user, etc.).</p> <p>Returns:</p> Type Description <code>DepsT</code> <p>The dependencies object passed to pipeline.execute().</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Tracer for custom spans.</p> <p>Returns:</p> Type Description <code>Tracer | None</code> <p>The tracer instance, or None if no tracing.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.usage","title":"<code>usage</code>  <code>property</code>","text":"<p>Accumulated usage from all ctx.run() calls in this step.</p> <p>Returns:</p> Type Description <code>StepUsage</code> <p>StepUsage with aggregated tokens and cost.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.config","title":"<code>config</code>  <code>property</code>","text":"<p>Configuration for this step (timeout, retries, budget).</p> <p>Returns:</p> Type Description <code>StepConfig</code> <p>The resolved StepConfig for this step.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.__init__","title":"<code>__init__(step_id, inputs, deps, step_outputs, tracer=None, config=None)</code>","text":"<p>Initialize step context.</p> <p>Parameters:</p> Name Type Description Default <code>step_id</code> <code>str</code> <p>Unique identifier for this step.</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Pipeline inputs passed to execute().</p> required <code>deps</code> <code>DepsT</code> <p>Application dependencies (db session, user, etc.).</p> required <code>step_outputs</code> <code>dict[str, Any]</code> <p>Outputs from completed dependency steps.</p> required <code>tracer</code> <code>Tracer | None</code> <p>Optional tracer for distributed tracing.</p> <code>None</code> <code>config</code> <code>StepConfig | None</code> <p>Step configuration (timeout, retries, budget).</p> <code>None</code>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.get_input","title":"<code>get_input(key, default=None)</code>","text":"<p>Get value from pipeline inputs.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The input key to retrieve.</p> required <code>default</code> <code>Any</code> <p>Value to return if key not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The input value, or default if not present.</p> <p>Examples:</p> <pre><code>class ProcessStep(BaseStep[MyDeps, str]):\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        # Get required input\n        document = ctx.get_input(\"document\")\n\n        # Get optional input with default\n        format_type = ctx.get_input(\"format\", \"json\")\n\n        return f\"Processing {document} as {format_type}\"\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.get_dependency","title":"<code>get_dependency(step_id, output_type=None)</code>","text":"<p>Get output from a dependency step.</p> <p>Parameters:</p> Name Type Description Default <code>step_id</code> <code>str</code> <p>ID of the dependency step.</p> required <code>output_type</code> <code>type[T] | None</code> <p>Expected type (for IDE/type checker, not enforced).</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>The output from the dependency step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If step_id not in dependencies or hasn't run.</p> <p>Examples:</p> <pre><code># With type hint (IDE knows extraction is ExtractionResult)\nextraction = context.get_dependency(\"extract\", ExtractionResult)\nextraction.entities  # Autocomplete works!\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.get_dependency_or_none","title":"<code>get_dependency_or_none(step_id, output_type=None)</code>","text":"<p>Get output from a dependency step, or None if not available.</p> <p>Use this for optional dependencies that may not have run.</p> <p>Parameters:</p> Name Type Description Default <code>step_id</code> <code>str</code> <p>ID of the dependency step.</p> required <code>output_type</code> <code>type[T] | None</code> <p>Expected type (for IDE/type checker, not enforced).</p> <code>None</code> <p>Returns:</p> Type Description <code>T | None</code> <p>The output from the dependency step, or None if not available.</p> <p>Examples:</p> <pre><code>class EnhanceStep(BaseStep[MyDeps, str]):\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        # Required dependency\n        base_content = ctx.get_dependency(\"extract\")\n\n        # Optional dependency - might not exist\n        metadata = ctx.get_dependency_or_none(\"fetch_metadata\", dict)\n\n        if metadata:\n            return f\"{base_content} (with metadata)\"\n        return base_content\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.run","title":"<code>run(agent, message, *, timeout=None, retries=None)</code>  <code>async</code>","text":"<p>Run an agent with automatic tracer, usage tracking, and config.</p> <p>This is THE way to call agents from within a step. It: - Passes deps and tracer automatically - Accumulates usage in ctx.usage - Enforces cost budget (raises CostBudgetExceededError if exceeded) - Supports timeout and retries (from config or per-call override)</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>FastroAgent[OutputT]</code> <p>The FastroAgent to run.</p> required <code>message</code> <code>str</code> <p>The message/prompt to send.</p> required <code>timeout</code> <code>float | None</code> <p>Per-call timeout override (seconds). Uses config if None.</p> <code>None</code> <code>retries</code> <code>int | None</code> <p>Per-call retries override. Uses config if None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatResponse[OutputT]</code> <p>ChatResponse with output, content, usage data, etc.</p> <p>Raises:</p> Type Description <code>CostBudgetExceededError</code> <p>If cost_budget is set and exceeded.</p> <code>TimeoutError</code> <p>If timeout exceeded after all retries.</p> <p>Examples:</p> <pre><code>class MyStep(BaseStep[MyDeps, str]):\n    classifier = FastroAgent(model=\"gpt-4o-mini\", output_type=Category)\n    writer = FastroAgent(model=\"gpt-4o\")\n\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        # Both calls tracked in ctx.usage\n        category = await ctx.run(self.classifier, \"Classify this\")\n        result = await ctx.run(self.writer, f\"Write about {category.output}\")\n        return result.content\n\n# With per-call overrides:\nresponse = await ctx.run(agent, \"msg\", timeout=30.0, retries=2)\n</code></pre>"},{"location":"api/pipelines/#step","title":"step","text":""},{"location":"api/pipelines/#fastroai.pipelines.step","title":"<code>fastroai.pipelines.step(func=None, *, timeout=None, retries=0, retry_delay=1.0, cost_budget=None)</code>","text":"<pre><code>step(func: Callable[..., OutputT]) -&gt; _FunctionStep\n</code></pre><pre><code>step(\n    func: None = None,\n    *,\n    timeout: float | None = None,\n    retries: int = 0,\n    retry_delay: float = 1.0,\n    cost_budget: int | None = None,\n) -&gt; Callable[[Callable[..., OutputT]], _FunctionStep]\n</code></pre> <p>Decorator to create a pipeline step from a function.</p> <p>Can be used with or without arguments:</p> <pre><code>@step\nasync def my_step(ctx): ...\n\n@step(timeout=30.0, retries=2)\nasync def my_step(ctx): ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any] | None</code> <p>The function to wrap (when used without parentheses).</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Maximum execution time in seconds.</p> <code>None</code> <code>retries</code> <code>int</code> <p>Number of retry attempts on failure.</p> <code>0</code> <code>retry_delay</code> <code>float</code> <p>Base delay between retries (exponential backoff).</p> <code>1.0</code> <code>cost_budget</code> <code>int | None</code> <p>Maximum cost in microcents for this step.</p> <code>None</code> <p>Returns:</p> Type Description <code>_FunctionStep | Callable[[Callable[..., Any]], _FunctionStep]</code> <p>A BaseStep instance that can be used in a Pipeline.</p>"},{"location":"api/pipelines/#configuration","title":"Configuration","text":""},{"location":"api/pipelines/#stepconfig","title":"StepConfig","text":""},{"location":"api/pipelines/#fastroai.pipelines.StepConfig","title":"<code>fastroai.pipelines.StepConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a pipeline step.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <code>float | None</code> <p>Maximum time in seconds for step execution. None = no timeout.</p> <code>retries</code> <code>int</code> <p>Number of retry attempts on failure. 0 = no retries.</p> <code>retry_delay</code> <code>float</code> <p>Delay in seconds between retry attempts.</p> <code>cost_budget</code> <code>int | None</code> <p>Maximum cost in microcents. None = no budget limit.</p> <p>Examples:</p> <pre><code># Step with 30s timeout and 2 retries\nconfig = StepConfig(timeout=30.0, retries=2)\n\n# Step with cost budget of $0.10 (10 cents = 100_000 microcents)\nconfig = StepConfig(cost_budget=100_000)\n</code></pre>"},{"location":"api/pipelines/#pipelineconfig","title":"PipelineConfig","text":""},{"location":"api/pipelines/#fastroai.pipelines.PipelineConfig","title":"<code>fastroai.pipelines.PipelineConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a pipeline with additional options.</p> <p>Inherits all StepConfig fields plus pipeline-specific options.</p> <p>Attributes:</p> Name Type Description <code>trace</code> <code>bool</code> <p>Whether to enable tracing for this pipeline.</p> <code>on_error</code> <code>Literal['fail', 'continue']</code> <p>Error handling strategy: - \"fail\": Stop pipeline on first error (default) - \"continue\": Continue executing other steps on error</p> <p>Examples:</p> <pre><code># Pipeline with tracing and 60s timeout\nconfig = PipelineConfig(trace=True, timeout=60.0)\n\n# Pipeline that continues on errors\nconfig = PipelineConfig(on_error=\"continue\")\n</code></pre>"},{"location":"api/pipelines/#conversation-state","title":"Conversation State","text":""},{"location":"api/pipelines/#conversationstatus","title":"ConversationStatus","text":""},{"location":"api/pipelines/#fastroai.pipelines.ConversationStatus","title":"<code>fastroai.pipelines.ConversationStatus</code>","text":"<p>Status of multi-turn conversation gathering.</p> <p>Attributes:</p> Name Type Description <code>COMPLETE</code> <p>All required information has been gathered. The pipeline proceeds to subsequent steps.</p> <code>INCOMPLETE</code> <p>More information is needed from the user. The pipeline pauses and returns partial state.</p>"},{"location":"api/pipelines/#conversationstate","title":"ConversationState","text":""},{"location":"api/pipelines/#fastroai.pipelines.ConversationState","title":"<code>fastroai.pipelines.ConversationState</code>","text":"<p>Signal for multi-turn conversation steps.</p> <p>When a step returns ConversationState with INCOMPLETE status, the pipeline stops early. Partial data and context are preserved.</p> <p>Examples:</p> <pre><code>class GatherInfoStep(BaseStep[MyDeps, ConversationState[UserInfo]]):\n    async def execute(self, context) -&gt; ConversationState[UserInfo]:\n        info = await self._extract(context.get_input(\"message\"))\n\n        if info.is_complete():\n            return ConversationState(\n                status=ConversationStatus.COMPLETE,\n                data=info,\n            )\n\n        return ConversationState(\n            status=ConversationStatus.INCOMPLETE,\n            data=info,  # Partial data\n            context={\"missing\": info.missing_fields()},\n        )\n</code></pre>"},{"location":"api/pipelines/#usage-tracking","title":"Usage Tracking","text":""},{"location":"api/pipelines/#stepusage","title":"StepUsage","text":""},{"location":"api/pipelines/#fastroai.pipelines.StepUsage","title":"<code>fastroai.pipelines.StepUsage</code>","text":"<p>Usage metrics for a single pipeline step.</p> <p>Automatically extracted from ChatResponse when using AgentStep.</p> <p>Examples:</p> <pre><code># From ChatResponse\nusage = StepUsage.from_chat_response(response)\n\n# Manual creation\nusage = StepUsage(\n    input_tokens=100,\n    output_tokens=50,\n    cost_microcents=175,\n    processing_time_ms=500,\n    model=\"gpt-4o\",\n)\n\n# Combine usages\ntotal = usage1 + usage2\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepUsage.__add__","title":"<code>__add__(other)</code>","text":"<p>Combine two StepUsage instances.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StepUsage</code> <p>Another StepUsage to add.</p> required <p>Returns:</p> Type Description <code>StepUsage</code> <p>New StepUsage with summed metrics.</p> <p>Examples:</p> <pre><code>usage1 = StepUsage(input_tokens=100, cost_microcents=50)\nusage2 = StepUsage(input_tokens=200, cost_microcents=100)\n\ntotal = usage1 + usage2\nprint(total.input_tokens)  # 300\nprint(total.cost_microcents)  # 150\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepUsage.from_chat_response","title":"<code>from_chat_response(response)</code>  <code>classmethod</code>","text":"<p>Create StepUsage from a ChatResponse.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatResponse[Any]</code> <p>ChatResponse from an agent run.</p> required <p>Returns:</p> Type Description <code>StepUsage</code> <p>StepUsage with metrics extracted from the response.</p> <p>Examples:</p> <pre><code>response = await agent.run(\"Hello\")\nusage = StepUsage.from_chat_response(response)\n\nprint(f\"Tokens: {usage.input_tokens} in, {usage.output_tokens} out\")\nprint(f\"Cost: {usage.cost_microcents} microcents\")\n</code></pre>"},{"location":"api/pipelines/#pipelineusage","title":"PipelineUsage","text":""},{"location":"api/pipelines/#fastroai.pipelines.PipelineUsage","title":"<code>fastroai.pipelines.PipelineUsage</code>","text":"<p>Aggregated usage across all pipeline steps.</p> <p>Examples:</p> <pre><code># From step usages\nusage = PipelineUsage.from_step_usages({\n    \"extract\": StepUsage(cost_microcents=100, ...),\n    \"classify\": StepUsage(cost_microcents=200, ...),\n})\n\nprint(f\"Total cost: ${usage.total_cost_dollars:.6f}\")\nprint(f\"Steps: {list(usage.steps.keys())}\")\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.PipelineUsage.total_cost_dollars","title":"<code>total_cost_dollars</code>  <code>property</code>","text":"<p>Total cost in dollars for display purposes.</p> <p>Returns:</p> Type Description <code>float</code> <p>Total cost as a float in dollars.</p> Note <p>Use total_cost_microcents for calculations to avoid floating-point errors.</p>"},{"location":"api/pipelines/#fastroai.pipelines.PipelineUsage.from_step_usages","title":"<code>from_step_usages(step_usages)</code>  <code>classmethod</code>","text":"<p>Aggregate metrics from individual step usages.</p> <p>Parameters:</p> Name Type Description Default <code>step_usages</code> <code>dict[str, StepUsage]</code> <p>Dict mapping step IDs to their usage metrics.</p> required <p>Returns:</p> Type Description <code>PipelineUsage</code> <p>PipelineUsage with summed totals and per-step breakdown.</p> <p>Examples:</p> <pre><code>step_usages = {\n    \"extract\": StepUsage(input_tokens=100, cost_microcents=50),\n    \"classify\": StepUsage(input_tokens=200, cost_microcents=100),\n}\n\nusage = PipelineUsage.from_step_usages(step_usages)\nprint(f\"Total tokens: {usage.total_input_tokens}\")  # 300\nprint(f\"Total cost: ${usage.total_cost_dollars:.6f}\")\n\n# Access per-step breakdown\nfor step_id, step_usage in usage.steps.items():\n    print(f\"  {step_id}: {step_usage.cost_microcents} microcents\")\n</code></pre>"},{"location":"api/pipelines/#errors","title":"Errors","text":""},{"location":"api/pipelines/#stepexecutionerror","title":"StepExecutionError","text":"<p>\u2190 Agent Tools \u2192</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepExecutionError","title":"<code>fastroai.pipelines.StepExecutionError</code>","text":"<p>Raised when a pipeline step fails during execution.</p> <p>Attributes:</p> Name Type Description <code>step_id</code> <p>The ID of the step that failed.</p> <code>original_error</code> <p>The underlying exception that caused the failure.</p> <p>Examples:</p> <pre><code>try:\n    result = await pipeline.execute(inputs, deps)\nexcept StepExecutionError as e:\n    print(f\"Step '{e.step_id}' failed: {e.original_error}\")\n</code></pre>"},{"location":"api/tools/","title":"Tools","text":"<p>The tools module provides production-safe tool decorators and toolset base classes for organizing AI agent capabilities.</p>"},{"location":"api/tools/#safe_tool","title":"safe_tool","text":""},{"location":"api/tools/#fastroai.tools.safe_tool","title":"<code>fastroai.tools.safe_tool(timeout=DEFAULT_TOOL_TIMEOUT, max_retries=DEFAULT_TOOL_MAX_RETRIES, on_timeout=None, on_error=None)</code>","text":"<p>Decorator that adds timeout, retry, and error handling to AI tools.</p> <p>When a tool times out or raises an exception, instead of crashing the conversation, this decorator returns an error message that the AI can use to respond gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum seconds per attempt. Default: 30.</p> <code>DEFAULT_TOOL_TIMEOUT</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts. Default: 3.</p> <code>DEFAULT_TOOL_MAX_RETRIES</code> <code>on_timeout</code> <code>str | None</code> <p>Custom message returned on timeout.        Default: \"Tool timed out after {max_retries} attempts\"</p> <code>None</code> <code>on_error</code> <code>str | None</code> <p>Custom message returned on error.      Use {error} placeholder for error details.      Default: \"Tool failed: {error}\"</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Callable[P, Awaitable[R]]], Callable[P, Awaitable[R | str]]]</code> <p>Decorated async function with safety features.</p> <p>Examples:</p> <pre><code>@safe_tool(timeout=10, max_retries=2)\nasync def web_search(query: str) -&gt; str:\n    '''Search the web for information.'''\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.example.com?q={query}\")\n        return response.text\n\n# If the API is slow or down:\n# - Waits max 10 seconds per attempt\n# - Retries up to 2 times with exponential backoff\n# - Returns error message on final failure\n# - AI sees: \"Tool timed out after 2 attempts\"\n\n# With custom messages:\n@safe_tool(\n    timeout=30,\n    on_timeout=\"Search is taking too long. Try a simpler query.\",\n    on_error=\"Search unavailable: {error}\",\n)\nasync def search(query: str) -&gt; str:\n    ...\n</code></pre>"},{"location":"api/tools/#safetoolset","title":"SafeToolset","text":""},{"location":"api/tools/#fastroai.tools.SafeToolset","title":"<code>fastroai.tools.SafeToolset</code>","text":"<p>Base class for toolsets containing only safe tools.</p> <p>Safe tools are those that: - Don't access external networks (or have timeout protection) - Don't modify system state - Have bounded execution time - Return graceful error messages instead of raising exceptions</p> <p>Use this as a base class to mark toolsets as production-safe.</p> <p>Examples:</p> <pre><code>@safe_tool(timeout=5)\nasync def calculator(expression: str) -&gt; str:\n    '''Evaluate a math expression.'''\n    try:\n        # Safe: no network, no state modification\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return str(result)\n    except Exception as e:\n        return f\"Error: {e}\"\n\n@safe_tool(timeout=1)\nasync def get_time() -&gt; str:\n    '''Get current time.'''\n    from datetime import datetime\n    return datetime.now().isoformat()\n\nclass UtilityToolset(SafeToolset):\n    def __init__(self):\n        super().__init__(\n            tools=[calculator, get_time],\n            name=\"utilities\",\n        )\n</code></pre>"},{"location":"api/tools/#functiontoolsetbase","title":"FunctionToolsetBase","text":"<p>\u2190 Pipelines Usage \u2192</p>"},{"location":"api/tools/#fastroai.tools.FunctionToolsetBase","title":"<code>fastroai.tools.FunctionToolsetBase</code>","text":"<p>Base class for organized tool sets.</p> <p>Extends PydanticAI's FunctionToolset with a name for identification and organization purposes.</p> <p>Examples:</p> <pre><code>from fastroai.tools import safe_tool, FunctionToolsetBase\n\n@safe_tool(timeout=30)\nasync def web_search(query: str) -&gt; str:\n    '''Search the web.'''\n    ...\n\n@safe_tool(timeout=10)\nasync def get_weather(location: str) -&gt; str:\n    '''Get weather for location.'''\n    ...\n\nclass WebToolset(FunctionToolsetBase):\n    def __init__(self):\n        super().__init__(\n            tools=[web_search, get_weather],\n            name=\"web\",\n        )\n\n# Use with FastroAgent\nagent = FastroAgent(toolsets=[WebToolset()])\n</code></pre>"},{"location":"api/tools/#fastroai.tools.FunctionToolsetBase.__init__","title":"<code>__init__(tools, name=None)</code>","text":"<p>Initialize toolset with tools and optional name.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Callable[..., Any]]</code> <p>List of tool functions to include.</p> required <code>name</code> <code>str | None</code> <p>Name for this toolset. Defaults to class name.</p> <code>None</code>"},{"location":"api/tracing/","title":"Tracing","text":"<p>The tracing module provides a protocol-based interface for distributed tracing integration. Implement the <code>Tracer</code> protocol to connect FastroAI with your observability platform, or use one of the built-in tracers.</p>"},{"location":"api/tracing/#tracer","title":"Tracer","text":""},{"location":"api/tracing/#fastroai.tracing.Tracer","title":"<code>fastroai.tracing.Tracer</code>","text":"<p>Protocol for distributed tracing implementations.</p> <p>Implement this protocol to integrate FastroAI with your preferred observability platform (OpenTelemetry, Datadog, etc.).</p> <p>FastroAI provides built-in implementations: - LogfireTracer: For Pydantic's Logfire platform - SimpleTracer: For logging-based tracing - NoOpTracer: For disabled tracing</p> <p>Examples:</p> <p>Using the built-in LogfireTracer: <pre><code>from fastroai import FastroAgent, LogfireTracer\n\ntracer = LogfireTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\", tracer=tracer)\n</code></pre></p> <p>Custom implementation for OpenTelemetry: <pre><code>from opentelemetry import trace as otel_trace\n\nclass OTelTracer:\n    def __init__(self):\n        self.tracer = otel_trace.get_tracer(\"fastroai\")\n\n    @asynccontextmanager\n    async def span(self, name: str, **attrs):\n        trace_id = str(uuid.uuid4())\n        with self.tracer.start_as_current_span(name) as span:\n            for key, value in attrs.items():\n                span.set_attribute(key, value)\n            yield trace_id\n\n    def log_metric(self, trace_id: str, name: str, value):\n        span = otel_trace.get_current_span()\n        span.set_attribute(f\"metric.{name}\", value)\n\n    def log_error(self, trace_id: str, error: Exception, context=None):\n        span = otel_trace.get_current_span()\n        span.record_exception(error)\n</code></pre></p>"},{"location":"api/tracing/#fastroai.tracing.Tracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>Log an error associated with a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID to associate the error with.</p> required <code>error</code> <code>Exception</code> <p>The exception that occurred.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional error context.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.Tracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>Log a metric associated with a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID to associate the metric with.</p> required <code>name</code> <code>str</code> <p>Metric name.</p> required <code>value</code> <code>Any</code> <p>Metric value.</p> required"},{"location":"api/tracing/#fastroai.tracing.Tracer.span","title":"<code>span(name, **attributes)</code>","text":"<p>Create a traced span for an operation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the operation being traced.</p> required <code>**attributes</code> <code>Any</code> <p>Additional context to attach to the span.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AbstractAsyncContextManager[str]</code> <p>Async context manager that yields a unique trace ID.</p>"},{"location":"api/tracing/#simpletracer","title":"SimpleTracer","text":""},{"location":"api/tracing/#fastroai.tracing.SimpleTracer","title":"<code>fastroai.tracing.SimpleTracer</code>","text":"<p>Basic tracer implementation using Python's logging module.</p> <p>Provides simple tracing functionality for development and debugging. For production use, consider implementing a Tracer for your observability platform.</p> <p>Examples:</p> <pre><code>tracer = SimpleTracer()\n\nasync with tracer.span(\"my_operation\", user_id=\"123\") as trace_id:\n    # Your operation here\n    result = await do_something()\n    tracer.log_metric(trace_id, \"result_size\", len(result))\n\n# Logs:\n# INFO [abc12345] Starting my_operation\n# INFO [abc12345] Metric result_size=42\n# INFO [abc12345] Completed my_operation in 0.123s\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize SimpleTracer.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger | None</code> <p>Logger to use. Defaults to 'fastroai.tracing'.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>Log an error with trace correlation.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>error</code> <code>Exception</code> <p>The exception that occurred.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional error context.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>Log a metric with trace correlation.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>name</code> <code>str</code> <p>Metric name.</p> required <code>value</code> <code>Any</code> <p>Metric value.</p> required"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.span","title":"<code>span(name, **attributes)</code>  <code>async</code>","text":"<p>Create a traced span with timing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the operation.</p> required <code>**attributes</code> <code>Any</code> <p>Additional context logged with the span.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Unique trace ID (first 8 chars shown in logs for readability).</p>"},{"location":"api/tracing/#logfiretracer","title":"LogfireTracer","text":""},{"location":"api/tracing/#fastroai.tracing.LogfireTracer","title":"<code>fastroai.tracing.LogfireTracer</code>","text":"<p>Tracer implementation for Pydantic's Logfire observability platform.</p> <p>Integrates FastroAI with Logfire for production-grade observability, including distributed tracing, metrics, and error tracking. Requires the <code>logfire</code> package to be installed.</p> Note <p>Install logfire with: <code>pip install logfire</code> Configure logfire before use: <code>logfire.configure()</code></p> <p>Examples:</p> <pre><code>import logfire\nfrom fastroai import FastroAgent, LogfireTracer\n\n# Configure logfire (typically done once at startup)\nlogfire.configure()\n\ntracer = LogfireTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\", tracer=tracer)\n\n# View traces in Logfire dashboard at https://logfire.pydantic.dev\n</code></pre> <p>With pipelines: <pre><code>from fastroai import Pipeline, LogfireTracer\n\ntracer = LogfireTracer()\nresult = await pipeline.execute(\n    {\"document\": doc},\n    deps=my_deps,\n    tracer=tracer,\n)\n</code></pre></p>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize LogfireTracer.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the logfire package is not installed.</p>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>Log an error to Logfire with trace correlation.</p> <p>Records the error with full exception information for debugging in the Logfire dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>error</code> <code>Exception</code> <p>The exception that occurred.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional error context (e.g., step_id, operation).</p> <code>None</code> <p>Examples:</p> <pre><code>try:\n    result = await risky_operation()\nexcept Exception as e:\n    tracer.log_error(trace_id, e, {\"step\": \"data_processing\"})\n    raise\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>Log a metric to Logfire with trace correlation.</p> <p>Metrics are logged as info-level spans with the metric name and value as attributes, allowing them to be queried and visualized in Logfire.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>name</code> <code>str</code> <p>Metric name (e.g., \"input_tokens\", \"cost_microcents\").</p> required <code>value</code> <code>Any</code> <p>Metric value.</p> required <p>Examples:</p> <pre><code>tracer.log_metric(trace_id, \"input_tokens\", 150)\ntracer.log_metric(trace_id, \"cost_microcents\", 2500)\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.span","title":"<code>span(name, **attributes)</code>  <code>async</code>","text":"<p>Create a traced span using Logfire.</p> <p>Wraps Logfire's span context manager and generates a unique trace ID for correlation across FastroAI operations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the operation being traced.</p> required <code>**attributes</code> <code>Any</code> <p>Additional context to attach to the span. These appear as attributes in the Logfire dashboard.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Unique trace ID for correlating metrics and errors.</p> <p>Examples:</p> <pre><code>async with tracer.span(\"my_operation\", user_id=\"123\") as trace_id:\n    result = await do_something()\n    tracer.log_metric(trace_id, \"result_size\", len(result))\n</code></pre>"},{"location":"api/tracing/#nooptracer","title":"NoOpTracer","text":"<p>\u2190 Usage API Overview \u2192</p>"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer","title":"<code>fastroai.tracing.NoOpTracer</code>","text":"<p>Tracer that does nothing. Use when tracing is disabled.</p> <p>This tracer satisfies the Tracer protocol but performs no operations, making it suitable for testing or when tracing overhead is undesirable.</p> <p>Examples:</p> <pre><code>tracer = NoOpTracer()\n\nasync with tracer.span(\"operation\") as trace_id:\n    # trace_id is still generated for compatibility\n    result = await do_something()\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>No-op error logging.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Ignored.</p> required <code>error</code> <code>Exception</code> <p>Ignored.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Ignored.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>No-op metric logging.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Ignored.</p> required <code>name</code> <code>str</code> <p>Ignored.</p> required <code>value</code> <code>Any</code> <p>Ignored.</p> required"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer.span","title":"<code>span(name, **attributes)</code>  <code>async</code>","text":"<p>Create a no-op span that just yields a trace ID.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Ignored.</p> required <code>**attributes</code> <code>Any</code> <p>Ignored.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Unique trace ID (still generated for compatibility).</p>"},{"location":"api/usage/","title":"Usage","text":"<p>The usage module provides precise cost calculation using integer microcents to avoid floating-point precision errors in billing systems.</p>"},{"location":"api/usage/#costcalculator","title":"CostCalculator","text":"<p>\u2190 Tools Tracing \u2192</p>"},{"location":"api/usage/#fastroai.usage.CostCalculator","title":"<code>fastroai.usage.CostCalculator</code>","text":"<p>Token cost calculator with microcents precision.</p> <p>Uses genai-prices for model pricing data, with support for custom pricing overrides. All costs are returned as integer microcents.</p> <p>1 microcent = 1/10,000 cent = 1/1,000,000 dollar</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\n\n# Calculate cost for a request\ncost = calc.calculate_cost(\"gpt-4o\", input_tokens=1000, output_tokens=500)\nprint(f\"Cost: {cost} microcents\")\nprint(f\"Cost: ${calc.microcents_to_dollars(cost):.6f}\")\n\n# With custom pricing override (e.g., volume discount)\ncalc = CostCalculator(pricing_overrides={\n    \"gpt-4o\": {\"input_per_mtok\": 2.00, \"output_per_mtok\": 8.00},\n})\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.__init__","title":"<code>__init__(pricing_overrides=None)</code>","text":"<p>Initialize calculator.</p> <p>Parameters:</p> Name Type Description Default <code>pricing_overrides</code> <code>dict[str, dict[str, float]] | None</code> <p>Custom pricing for specific models. Keys are model names, values are dicts with 'input_per_mtok' and 'output_per_mtok' (dollars per million tokens). Use for volume discounts or custom models.</p> <code>None</code>"},{"location":"api/usage/#fastroai.usage.CostCalculator.calculate_cost","title":"<code>calculate_cost(model, input_tokens, output_tokens)</code>","text":"<p>Calculate cost in microcents.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o\" or \"openai:gpt-4o\").</p> required <code>input_tokens</code> <code>int</code> <p>Number of input/prompt tokens.</p> required <code>output_tokens</code> <code>int</code> <p>Number of output/completion tokens.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Cost in microcents (integer). Returns 0 for unknown models.</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\ncost = calc.calculate_cost(\"gpt-4o\", 1000, 500)\nprint(f\"${calc.microcents_to_dollars(cost):.6f}\")\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.microcents_to_dollars","title":"<code>microcents_to_dollars(microcents)</code>","text":"<p>Convert microcents to dollars for display.</p> <p>Use this only for display purposes. For calculations, always use integer microcents.</p> <p>Parameters:</p> Name Type Description Default <code>microcents</code> <code>int</code> <p>Cost in microcents.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Cost in dollars (float).</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\ndollars = calc.microcents_to_dollars(1_500_000)\nprint(f\"${dollars:.2f}\")  # $1.50\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.dollars_to_microcents","title":"<code>dollars_to_microcents(dollars)</code>","text":"<p>Convert dollars to microcents.</p> <p>Parameters:</p> Name Type Description Default <code>dollars</code> <code>float</code> <p>Cost in dollars.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Cost in microcents (integer).</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\n\n# Set a budget of $0.10\nbudget = calc.dollars_to_microcents(0.10)\nprint(budget)  # 100000\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.format_cost","title":"<code>format_cost(microcents)</code>","text":"<p>Format cost in multiple representations.</p> <p>Parameters:</p> Name Type Description Default <code>microcents</code> <code>int</code> <p>Cost in microcents.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with microcents, cents, and dollars representations.</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\nformatted = calc.format_cost(1_500_000)\n\nprint(formatted)\n# {\n#     \"microcents\": 1500000,\n#     \"cents\": 150,\n#     \"dollars\": 1.5\n# }\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.add_pricing_override","title":"<code>add_pricing_override(model, input_per_mtok, output_per_mtok)</code>","text":"<p>Add or update pricing override for a model.</p> <p>Use this for custom pricing (volume discounts) or models not in genai-prices.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (will be normalized).</p> required <code>input_per_mtok</code> <code>float</code> <p>Input cost in dollars per million tokens.</p> required <code>output_per_mtok</code> <code>float</code> <p>Output cost in dollars per million tokens.</p> required <p>Examples:</p> <pre><code>calc = CostCalculator()\n\n# Add volume discount pricing (20% off standard)\ncalc.add_pricing_override(\n    model=\"gpt-4o\",\n    input_per_mtok=2.00,   # Standard is $2.50\n    output_per_mtok=8.00,  # Standard is $10.00\n)\n\n# Add custom/local model\ncalc.add_pricing_override(\n    model=\"my-local-llama\",\n    input_per_mtok=0.10,\n    output_per_mtok=0.20,\n)\n</code></pre>"},{"location":"guides/","title":"Guides","text":"<p>Deep dives into FastroAI's core components.</p> <p>Each guide explains what a component does, when you'd use it, and how to get it working. Start with whatever problem you're trying to solve.</p> <ul> <li> <p> FastroAgent</p> <p>Wrap PydanticAI agents with automatic cost tracking. Stateless, production-ready, with consistent response formats.</p> <p>FastroAgent \u2192</p> </li> <li> <p> Cost Calculator</p> <p>Track token costs in microcents for exact billing. Override pricing for volume discounts or custom models.</p> <p>Cost Calculator \u2192</p> </li> <li> <p> Pipelines</p> <p>Chain multiple AI steps with automatic parallelization. Track costs across entire workflows.</p> <p>Pipelines \u2192</p> </li> <li> <p> Safe Tools</p> <p>Timeout, retry, and graceful error handling for AI tools. Keep requests alive when external services fail.</p> <p>Safe Tools \u2192</p> </li> <li> <p> Tracing</p> <p>Correlate AI calls with the rest of your request flow. Integrate with any observability platform.</p> <p>Tracing \u2192</p> </li> </ul>"},{"location":"guides/#where-to-start","title":"Where to Start","text":"<p>Not sure which guide to read?</p> <p>Building an AI feature? Start with FastroAgent. It gives you cost tracking on every AI call.</p> <p>Multiple AI steps? Read Pipelines for execution order, parallelization, and aggregated costs.</p> <p>Tools calling external APIs? Check Safe Tools for graceful degradation when services fail.</p> <p>Going to production? Set up Tracing to debug slow or expensive requests.</p> <p>Most projects start with FastroAgent - it gives you cost tracking on every AI call, which you'll want in production. As your application grows, add pipelines for multi-step workflows and safe tools for external service calls.</p> <p>\u2190 Home FastroAgent \u2192</p>"},{"location":"guides/cost-calculator/","title":"Cost Calculator","text":"<p>LLM APIs charge per token. When you're processing thousands of requests, small precision errors in cost tracking add up. Floating-point math is the classic culprit:</p> <pre><code>&gt;&gt;&gt; 0.1 + 0.2\n0.30000000000000004\n</code></pre> <p>Your billing system shows $0.30000000000000004 and someone opens a support ticket. Or worse, you round incorrectly and underbill by a few cents per request, which becomes real money at scale.</p> <p>FastroAI tracks costs in microcents - integer math that doesn't drift. One microcent equals 1/1,000,000 of a dollar, so $0.01 is 10,000 microcents. All calculations use integers. When you display the cost to users, convert to dollars.</p>"},{"location":"guides/cost-calculator/#how-it-works","title":"How It Works","text":"<p>When you use <code>FastroAgent</code>, cost calculation happens automatically:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\")\n\nprint(response.cost_microcents)  # 2500 (exact integer)\nprint(response.cost_dollars)     # 0.0025 (for display)\n</code></pre> <p>The calculator looks up the model's per-token pricing, multiplies by your token counts, and returns an integer. No floating-point operations in the calculation path.</p> <p>The price data comes from genai-prices, which tracks pricing for OpenAI, Anthropic, Google, Groq, and other providers. The package is updated regularly as providers change their prices.</p>"},{"location":"guides/cost-calculator/#aggregating-costs","title":"Aggregating Costs","text":"<p>For a single request, precision doesn't matter much. But when you're aggregating across a conversation, a user's session, or your entire platform:</p> <pre><code># Track a conversation\ntotal_cost = 0\n\nfor message in user_messages:\n    response = await agent.run(message)\n    total_cost += response.cost_microcents  # Integer addition, no drift\n\n# Convert for display at the end\nprint(f\"Session cost: ${calc.microcents_to_dollars(total_cost):.4f}\")\n</code></pre> <p>After 10,000 additions, you have an exact count. No cumulative rounding errors.</p>"},{"location":"guides/cost-calculator/#direct-calculator-usage","title":"Direct Calculator Usage","text":"<p>Sometimes you need cost calculation without running an agent - estimating costs upfront, building dashboards, or custom tracking:</p> <pre><code>from fastroai import CostCalculator\n\ncalc = CostCalculator()\n\ncost = calc.calculate_cost(\n    model=\"gpt-4o\",\n    input_tokens=1000,\n    output_tokens=500,\n)\n\nprint(f\"Cost: {cost} microcents\")  # 7500 microcents\nprint(f\"Cost: ${calc.microcents_to_dollars(cost):.6f}\")  # $0.007500\n</code></pre> <p>Model names get normalized automatically. Both <code>\"gpt-4o\"</code> and <code>\"openai:gpt-4o\"</code> work.</p>"},{"location":"guides/cost-calculator/#conversion-methods","title":"Conversion Methods","text":"<pre><code>calc = CostCalculator()\n\n# Microcents to dollars (for display)\ndollars = calc.microcents_to_dollars(7500)  # 0.0075\n\n# Dollars to microcents (for storage/budgets)\nmicrocents = calc.dollars_to_microcents(0.10)  # 100000\n\n# Formatted output for debugging\nformatted = calc.format_cost(7500)\n# {\"microcents\": 7500, \"cents\": 0, \"dollars\": 0.0075}\n</code></pre>"},{"location":"guides/cost-calculator/#custom-pricing","title":"Custom Pricing","text":"<p>genai-prices covers most models, but you might need custom pricing. Maybe you've negotiated volume discounts with your provider. Or you're running self-hosted or fine-tuned models that aren't in any public pricing list. Or a new model came out and genai-prices hasn't updated yet.</p> <p>Override pricing at initialization:</p> <pre><code>from fastroai import CostCalculator\n\ncalc = CostCalculator(pricing_overrides={\n    \"gpt-4o\": {\n        \"input_per_mtok\": 2.00,   # $2.00 per million input tokens\n        \"output_per_mtok\": 8.00,  # $8.00 per million output tokens\n    },\n})\n</code></pre> <p>Or add overrides later:</p> <pre><code>calc.add_pricing_override(\n    model=\"my-local-model\",\n    input_per_mtok=0.10,\n    output_per_mtok=0.20,\n)\n</code></pre> <p>Overrides take precedence over genai-prices. Prices are in dollars per million tokens (the standard unit providers use).</p>"},{"location":"guides/cost-calculator/#using-custom-pricing-with-agents","title":"Using Custom Pricing with Agents","text":"<p>Pass your configured calculator to the agent:</p> <pre><code>from fastroai import FastroAgent, CostCalculator\n\ncalc = CostCalculator()\ncalc.add_pricing_override(\"gpt-4o\", input_per_mtok=2.00, output_per_mtok=8.00)\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    cost_calculator=calc,\n)\n\n# Now response.cost_microcents uses your pricing\nresponse = await agent.run(\"Hello!\")\n</code></pre>"},{"location":"guides/cost-calculator/#unknown-models","title":"Unknown Models","text":"<p>If you use a model that isn't in genai-prices and doesn't have a custom override, the calculator returns 0 cost and logs a debug message. This way your code doesn't crash - you just get missing cost data.</p> <p>Check your logs for \"Unknown model\" warnings. Either add a pricing override or open a PR on genai-prices.</p>"},{"location":"guides/cost-calculator/#cost-budgets","title":"Cost Budgets","text":"<p>Track cumulative costs and stop when you hit a limit:</p> <pre><code>from fastroai import CostCalculator\n\ncalc = CostCalculator()\ntotal_cost = 0\nbudget = calc.dollars_to_microcents(1.00)  # $1.00 budget\n\nfor query in queries:\n    response = await agent.run(query)\n    total_cost += response.cost_microcents\n\n    if total_cost &gt;= budget:\n        print(\"Budget exhausted\")\n        break\n\nprint(f\"Total spent: ${calc.microcents_to_dollars(total_cost):.4f}\")\n</code></pre> <p>This works fine for simple cases. For multi-step workflows where you want automatic budget enforcement, pipelines have built-in cost budgets that raise <code>CostBudgetExceededError</code> when exceeded. See the Pipelines guide.</p>"},{"location":"guides/cost-calculator/#pricing-reference","title":"Pricing Reference","text":"<p>Common model pricing as of January 2025:</p> Model Input ($/1M tokens) Output ($/1M tokens) gpt-4o $2.50 $10.00 gpt-4o-mini $0.15 $0.60 claude-3-5-sonnet $3.00 $15.00 claude-3-haiku $0.25 $1.25 gemini-1.5-pro $1.25 $5.00 gemini-1.5-flash $0.075 $0.30 <p>Prices change. Check your provider's current pricing, and consider using pricing overrides if you're on a negotiated rate.</p>"},{"location":"guides/cost-calculator/#key-files","title":"Key Files","text":"Component Location CostCalculator <code>fastroai/usage/calculator.py</code> <p>\u2190 FastroAgent Pipelines \u2192</p>"},{"location":"guides/fastro-agent/","title":"FastroAgent","text":"<p>FastroAgent wraps PydanticAI's <code>Agent</code> with automatic cost calculation, optional distributed tracing, and a consistent response format. Every response includes token counts and cost in microcents (exact integer math, no floating-point drift) so you can track usage, set budgets, and debug cost issues in production.</p>"},{"location":"guides/fastro-agent/#how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant You\n    participant FastroAgent\n    participant PydanticAI\n    participant LLM API\n\n    You-&gt;&gt;FastroAgent: run(\"Hello!\")\n    FastroAgent-&gt;&gt;PydanticAI: agent.run()\n    PydanticAI-&gt;&gt;LLM API: HTTP request\n    LLM API--&gt;&gt;PydanticAI: response + token counts\n    PydanticAI--&gt;&gt;FastroAgent: result\n    FastroAgent-&gt;&gt;FastroAgent: calculate cost\n    FastroAgent--&gt;&gt;You: ChatResponse</code></pre> <p>When you call <code>agent.run()</code>, FastroAgent passes your message to the underlying PydanticAI agent, which handles the actual LLM call. When the response comes back, FastroAgent extracts token counts, calculates cost using genai-prices (or your custom pricing), and packages everything into a <code>ChatResponse</code> with usage data.</p> <p>The agent itself is stateless. Conversation history, user context, and retry logic are your responsibility. This keeps FastroAgent simple and predictable.</p>"},{"location":"guides/fastro-agent/#creating-an-agent","title":"Creating an Agent","text":"<p>The simplest agent uses defaults for everything:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(model=\"openai:gpt-4o\")\n</code></pre> <p>Add a system prompt and tweak parameters as needed:</p> <pre><code>agent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a helpful financial advisor.\",\n    temperature=0.3,  # More consistent responses\n    max_tokens=4096,\n)\n</code></pre> <p>If you're passing configuration around (from environment variables, user settings, or a config file), use <code>AgentConfig</code>:</p> <pre><code>from fastroai import FastroAgent, AgentConfig\n\nconfig = AgentConfig(\n    model=\"anthropic:claude-3-5-sonnet\",\n    system_prompt=\"You are a code reviewer.\",\n    temperature=0.2,\n)\n\nagent = FastroAgent(config=config)\n</code></pre>"},{"location":"guides/fastro-agent/#configuration-reference","title":"Configuration Reference","text":"Parameter Default Description <code>model</code> <code>openai:gpt-4o</code> Model identifier with provider prefix <code>system_prompt</code> <code>\"You are a helpful AI assistant.\"</code> Instructions for the model <code>temperature</code> <code>0.7</code> Sampling temperature (0.0-2.0) <code>max_tokens</code> <code>4096</code> Maximum response tokens <code>timeout_seconds</code> <code>120</code> Request timeout <code>max_retries</code> <code>3</code> Retry attempts on failure <p>Model names use PydanticAI's provider prefix format: <code>openai:gpt-4o</code>, <code>anthropic:claude-3-5-sonnet</code>, <code>google:gemini-1.5-pro</code>. The prefix tells PydanticAI which API client to use.</p>"},{"location":"guides/fastro-agent/#running-queries","title":"Running Queries","text":"<p>Use <code>run()</code> to send a message and get a response:</p> <pre><code>response = await agent.run(\"What is the capital of France?\")\n\nprint(response.content)  # \"The capital of France is Paris.\"\nprint(response.cost_dollars)  # 0.000234\n</code></pre> <p>The response includes everything you need for billing and debugging:</p> <pre><code>print(f\"Model: {response.model}\")\nprint(f\"Input tokens: {response.input_tokens}\")\nprint(f\"Output tokens: {response.output_tokens}\")\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\nprint(f\"Time: {response.processing_time_ms}ms\")\n</code></pre>"},{"location":"guides/fastro-agent/#whats-in-a-response","title":"What's in a Response","text":"Field Type What It's For <code>content</code> <code>str</code> The response text <code>output</code> <code>OutputT</code> Typed output (same as content for string agents) <code>model</code> <code>str</code> Model that generated the response <code>input_tokens</code> <code>int</code> Prompt tokens consumed <code>output_tokens</code> <code>int</code> Completion tokens generated <code>cost_microcents</code> <code>int</code> Cost in 1/1,000,000 of a dollar (for calculations) <code>cost_dollars</code> <code>float</code> Cost in dollars (for display) <code>processing_time_ms</code> <code>int</code> Wall-clock time <code>tool_calls</code> <code>list</code> Tools invoked during generation <code>trace_id</code> <code>str</code> Tracing correlation ID <p>Use <code>cost_microcents</code> when you need to aggregate costs across many calls - integer math won't drift. Use <code>cost_dollars</code> when displaying to users.</p>"},{"location":"guides/fastro-agent/#structured-output","title":"Structured Output","text":"<p>Getting strings back means you have to parse them. For anything structured - extracting entities, classifying content, generating schemas - use Pydantic models instead:</p> <pre><code>from pydantic import BaseModel\nfrom fastroai import FastroAgent\n\nclass MovieReview(BaseModel):\n    title: str\n    rating: int\n    summary: str\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    output_type=MovieReview,\n)\n\nresponse = await agent.run(\"Review the movie Inception\")\n\n# response.output is a MovieReview instance, not a string\nprint(response.output.title)   # \"Inception\"\nprint(response.output.rating)  # 9\nprint(response.output.summary) # \"A mind-bending thriller...\"\n</code></pre> <p>PydanticAI handles the structured output extraction and validation. If the LLM output doesn't match your schema, it retries automatically. You pay for those retry tokens, so keep your schemas reasonable - don't ask for 50 required fields in one call.</p>"},{"location":"guides/fastro-agent/#conversation-history","title":"Conversation History","text":"<p>FastroAgent doesn't store conversation history. This sounds annoying until you realize how many ways there are to mess up conversation storage, and how much your storage requirements differ from everyone else's.</p> <p>You load history from your storage, pass it in, and save new messages yourself:</p> <pre><code>from pydantic_ai.messages import ModelMessage\n\n# Load from your storage (database, Redis, files, whatever)\nhistory: list[ModelMessage] = await my_storage.load(user_id)\n\n# Pass to the agent\nresponse = await agent.run(\n    \"Continue our conversation\",\n    message_history=history,\n)\n\n# Save the new user message and response\n# You manage how messages are stored - FastroAgent doesn't dictate the format\nawait my_storage.append(user_id, \"Continue our conversation\", response.content)\n</code></pre> <p>This gives you full control. Store history in a database table, Redis with a TTL, or flat files - whatever fits your architecture. Keep conversations forever or clear them after each session. Store complete messages or compress them into summaries. FastroAI doesn't care; it just takes whatever history you pass in.</p>"},{"location":"guides/fastro-agent/#streaming","title":"Streaming","text":"<p>For long responses, stream chunks as they arrive so users don't stare at a loading spinner:</p> <pre><code>async for chunk in agent.run_stream(\"Write a short story\"):\n    if chunk.is_final:\n        # Last chunk has complete usage data\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        # Print content as it arrives\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Cost tracking still works - you just get it at the end instead of upfront. The final chunk has <code>is_final=True</code> and includes the complete <code>usage_data</code> with token counts and cost.</p> <p>One gotcha: if your connection drops mid-stream, you lose both the partial response and the usage data. Consider logging partial streams if you need that level of tracking.</p>"},{"location":"guides/fastro-agent/#adding-tools","title":"Adding Tools","text":"<p>Give agents capabilities by passing toolsets. Here's a simple weather tool:</p> <pre><code>from pydantic_ai.toolsets import FunctionToolset\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    # Your weather API call here\n    return f\"Sunny, 72\u00b0F in {city}\"\n\ntoolset = FunctionToolset(tools=[get_weather])\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You can check the weather.\",\n    toolsets=[toolset],\n)\n\nresponse = await agent.run(\"What's the weather in Paris?\")\n# Agent calls get_weather(\"Paris\") and uses the result\n</code></pre> <p>Tool calls are tracked in the response:</p> <pre><code>for call in response.tool_calls:\n    print(f\"Called: {call['tool_name']}\")\n    print(f\"Args: {call['args']}\")\n</code></pre> <p>In production, tools fail. APIs time out. External services go down. Regular tools crash the whole request when that happens. The Safe Tools guide covers <code>@safe_tool</code>, which wraps tools with timeout, retry, and graceful error handling so failures become messages the LLM can work with instead of exceptions that kill your request.</p>"},{"location":"guides/fastro-agent/#dependencies","title":"Dependencies","text":"<p>Pass runtime dependencies to your tools - database connections, API clients, user context:</p> <pre><code>class MyDeps:\n    def __init__(self, api_key: str, user_id: str):\n        self.api_key = api_key\n        self.user_id = user_id\n\nresponse = await agent.run(\n    \"Search for news about AI\",\n    deps=MyDeps(api_key=\"sk-...\", user_id=\"user_123\"),\n)\n</code></pre> <p>Your tools receive these deps as their first argument. This keeps tools testable (inject mock deps) and avoids global state.</p>"},{"location":"guides/fastro-agent/#tracing","title":"Tracing","text":"<p>In production, you need to correlate AI calls with the rest of your request flow. Pass a tracer:</p> <pre><code>from fastroai import SimpleTracer\n\ntracer = SimpleTracer()\nresponse = await agent.run(\"Hello\", tracer=tracer)\n\nprint(response.trace_id)  # \"trace_abc123...\"\n</code></pre> <p><code>SimpleTracer</code> logs to Python's logging module - good for development. For production, implement the <code>Tracer</code> protocol to send spans to your observability platform (Datadog, Honeycomb, Logfire, etc.). The Tracing guide has examples for common platforms.</p>"},{"location":"guides/fastro-agent/#using-existing-pydanticai-agents","title":"Using Existing PydanticAI Agents","text":"<p>Already have a configured PydanticAI agent with custom output types, validators, or complex tooling? Wrap it with FastroAgent to add cost tracking without changing anything else:</p> <pre><code>from pydantic_ai import Agent\nfrom fastroai import FastroAgent\n\n# Your existing PydanticAI agent\npydantic_agent = Agent(\n    model=\"openai:gpt-4o\",\n    output_type=MyCustomType,\n    # ... your configuration\n)\n\n# Wrap it with FastroAgent\nfastro_agent = FastroAgent(agent=pydantic_agent)\n\n# Now you get cost tracking on your existing agent\nresponse = await fastro_agent.run(\"Hello\")\nprint(response.cost_dollars)\n</code></pre> <p>The underlying agent is always accessible if you need it:</p> <pre><code>pydantic_agent = fastro_agent.agent\n</code></pre>"},{"location":"guides/fastro-agent/#key-files","title":"Key Files","text":"Component Location FastroAgent <code>fastroai/agent/agent.py</code> AgentConfig <code>fastroai/agent/schemas.py</code> ChatResponse <code>fastroai/agent/schemas.py</code> StreamChunk <code>fastroai/agent/schemas.py</code> <p>\u2190 Guides Overview Cost Calculator \u2192</p>"},{"location":"guides/pipelines/","title":"Pipelines","text":"<p>A single agent call is one thing. Real applications chain multiple calls: extract entities, classify them, look up related data, generate a response. Each step might need different models, different prompts, different retry behavior. And you want to track costs across the whole flow, not just individual calls.</p> <p>Pipelines orchestrate multi-step AI workflows. You declare your steps and their dependencies. FastroAI figures out what can run in parallel, executes everything in the right order, and aggregates costs and timing across the whole pipeline.</p>"},{"location":"guides/pipelines/#when-to-use-pipelines","title":"When to Use Pipelines","text":"<p>Pipelines make sense when you're chaining multiple AI steps - extract entities, then classify them, then summarize. Or when you have independent steps that should run in parallel, like fetching data from multiple sources at once. They're also useful when you need cost tracking across an entire workflow rather than individual calls, or when you want to enforce cost budgets that stop execution before you blow through money.</p> <p>For a single agent call, use <code>FastroAgent</code> directly. The pipeline machinery isn't worth it for one step.</p>"},{"location":"guides/pipelines/#how-pipelines-work","title":"How Pipelines Work","text":"<p>You define steps and declare which steps depend on which other steps. FastroAI builds a dependency graph, topologically sorts it, and runs steps level by level. Steps at the same level run concurrently.</p> <pre><code>flowchart TB\n    extract --&gt; classify\n    classify --&gt; fetch_market\n    classify --&gt; fetch_user\n    fetch_market --&gt; calculate\n    fetch_user --&gt; calculate\n\n    subgraph parallel[\" \"]\n        fetch_market\n        fetch_user\n    end\n\n    style parallel fill:none,stroke:#666,stroke-dasharray: 5 5</code></pre> <p>In this example, <code>fetch_market</code> and <code>fetch_user</code> both depend on <code>classify</code>, so they run in parallel once <code>classify</code> completes. The <code>calculate</code> step waits for both to finish.</p> <p>Each step gets a <code>StepContext</code> with access to pipeline inputs, outputs from dependency steps, your application deps, and a usage tracker. When you call agents through <code>ctx.run()</code>, costs accumulate automatically.</p>"},{"location":"guides/pipelines/#a-basic-pipeline","title":"A Basic Pipeline","text":"<p>Here's a two-step pipeline that extracts entities and then classifies them:</p> <pre><code>from fastroai import Pipeline, BaseStep, StepContext, FastroAgent\n\nclass ExtractStep(BaseStep[None, str]):\n    def __init__(self):\n        self.agent = FastroAgent(\n            model=\"openai:gpt-4o-mini\",\n            system_prompt=\"Extract key entities from text.\",\n        )\n\n    async def execute(self, ctx: StepContext[None]) -&gt; str:\n        document = ctx.get_input(\"document\")\n        response = await ctx.run(self.agent, f\"Extract entities: {document}\")\n        return response.content\n\nclass ClassifyStep(BaseStep[None, str]):\n    def __init__(self):\n        self.agent = FastroAgent(\n            model=\"openai:gpt-4o-mini\",\n            system_prompt=\"Classify documents.\",\n        )\n\n    async def execute(self, ctx: StepContext[None]) -&gt; str:\n        entities = ctx.get_dependency(\"extract\")\n        response = await ctx.run(self.agent, f\"Classify based on: {entities}\")\n        return response.content\n\npipeline = Pipeline(\n    name=\"document_processor\",\n    steps={\n        \"extract\": ExtractStep(),\n        \"classify\": ClassifyStep(),\n    },\n    dependencies={\n        \"classify\": [\"extract\"],  # classify waits for extract\n    },\n)\n\nresult = await pipeline.execute({\"document\": \"Apple announced...\"}, deps=None)\nprint(result.output)  # Classification result\nprint(f\"Total cost: ${result.usage.total_cost_dollars:.6f}\")\n</code></pre> <p>The <code>dependencies</code> dict says \"classify depends on extract\". FastroAI runs extract first, then classify. If extract fails, classify never runs.</p>"},{"location":"guides/pipelines/#three-ways-to-define-steps","title":"Three Ways to Define Steps","text":"<p>FastroAI offers three approaches, from simplest to most flexible. Use whichever fits your situation.</p>"},{"location":"guides/pipelines/#1-the-step-decorator","title":"1. The <code>@step</code> Decorator","text":"<p>For steps that don't need initialization or complex state:</p> <pre><code>from fastroai import step, StepContext\n\n@step\nasync def transform(ctx: StepContext[None]) -&gt; str:\n    text = ctx.get_input(\"text\")\n    return text.upper()\n\n@step(timeout=30.0, retries=2)\nasync def classify(ctx: StepContext[None]) -&gt; str:\n    text = ctx.get_dependency(\"transform\")\n    response = await ctx.run(classifier_agent, f\"Classify: {text}\")\n    return response.output\n\npipeline = Pipeline(\n    name=\"processor\",\n    steps={\"transform\": transform, \"classify\": classify},\n    dependencies={\"classify\": [\"transform\"]},\n)\n</code></pre> <p>The decorator accepts <code>timeout</code>, <code>retries</code>, <code>retry_delay</code>, and <code>cost_budget</code> parameters.</p>"},{"location":"guides/pipelines/#2-agentas_step","title":"2. <code>agent.as_step()</code>","text":"<p>When a step is just a single agent call with no other logic:</p> <pre><code>from fastroai import FastroAgent, Pipeline\n\nsummarizer = FastroAgent(\n    model=\"openai:gpt-4o-mini\",\n    system_prompt=\"Summarize text concisely.\",\n)\n\npipeline = Pipeline(\n    name=\"summarizer\",\n    steps={\n        \"summarize\": summarizer.as_step(\n            lambda ctx: f\"Summarize: {ctx.get_input('text')}\"\n        ),\n    },\n)\n\nresult = await pipeline.execute({\"text\": \"Long article...\"}, deps=None)\n</code></pre> <p>The prompt can be a static string or a function that builds it from context.</p>"},{"location":"guides/pipelines/#3-basestep-class","title":"3. <code>BaseStep</code> Class","text":"<p>For complex steps with multiple agents, conditional logic, or state:</p> <pre><code>from fastroai import BaseStep, StepContext, FastroAgent\n\nclass ResearchStep(BaseStep[MyDeps, dict]):\n    def __init__(self):\n        self.classifier = FastroAgent(model=\"gpt-4o-mini\", system_prompt=\"Classify.\")\n        self.writer = FastroAgent(model=\"gpt-4o\", system_prompt=\"Write reports.\")\n\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; dict:\n        topic = ctx.get_input(\"topic\")\n\n        # Multiple agent calls with branching logic\n        category = await ctx.run(self.classifier, f\"Classify: {topic}\")\n\n        if \"technical\" in category.output.lower():\n            report = await ctx.run(self.writer, f\"Technical report on: {topic}\")\n        else:\n            report = await ctx.run(self.writer, f\"General summary of: {topic}\")\n\n        return {\"category\": category.output, \"report\": report.content}\n</code></pre>"},{"location":"guides/pipelines/#the-ctxrun-method","title":"The <code>ctx.run()</code> Method","text":"<p>Always call agents through <code>ctx.run()</code> rather than <code>agent.run()</code> directly. This is how FastroAI tracks costs and enforces budgets.</p> <pre><code>response = await ctx.run(agent, \"Your message\")\n</code></pre> <p><code>ctx.run()</code> handles the integration work for you. It passes your deps to the agent, forwards the tracer for distributed tracing, and accumulates usage in <code>ctx.usage</code> so pipeline-level cost tracking works. It also enforces timeout and retry settings from your config, and checks cost budgets - raising <code>CostBudgetExceededError</code> if you've exceeded your limit.</p> <p>You can override config per-call when one step needs different behavior:</p> <pre><code>response = await ctx.run(agent, \"message\", timeout=60.0, retries=5)\n</code></pre>"},{"location":"guides/pipelines/#parallel-execution","title":"Parallel Execution","text":"<p>Steps with different dependencies run in parallel automatically. You don't need to manage asyncio.gather or thread pools.</p> <pre><code>dependencies = {\n    \"classify\": [\"extract\"],\n    \"fetch_market\": [\"classify\"],\n    \"fetch_user\": [\"classify\"],     # Same dependency as fetch_market\n    \"calculate\": [\"fetch_market\", \"fetch_user\"],\n}\n</code></pre> <p>Here's how that executes over time:</p> <pre><code>sequenceDiagram\n    participant Pipeline\n    participant extract\n    participant classify\n    participant fetch_market\n    participant fetch_user\n    participant calculate\n\n    Pipeline-&gt;&gt;extract: execute\n    extract--&gt;&gt;Pipeline: done\n    Pipeline-&gt;&gt;classify: execute\n    classify--&gt;&gt;Pipeline: done\n    par parallel execution\n        Pipeline-&gt;&gt;fetch_market: execute\n        Pipeline-&gt;&gt;fetch_user: execute\n        fetch_market--&gt;&gt;Pipeline: done\n        fetch_user--&gt;&gt;Pipeline: done\n    end\n    Pipeline-&gt;&gt;calculate: execute\n    calculate--&gt;&gt;Pipeline: done</code></pre> <p>FastroAI groups steps by their maximum dependency depth and runs each level concurrently. If a step fails, downstream steps that depend on it won't run, but unrelated steps continue normally.</p>"},{"location":"guides/pipelines/#configuration","title":"Configuration","text":""},{"location":"guides/pipelines/#pipeline-level-defaults","title":"Pipeline-Level Defaults","text":"<p>Set defaults that apply to all steps:</p> <pre><code>from fastroai import Pipeline, PipelineConfig\n\npipeline = Pipeline(\n    name=\"processor\",\n    steps={...},\n    config=PipelineConfig(\n        timeout=30.0,         # Default timeout for all steps\n        retries=2,            # Default retry count\n        cost_budget=100_000,  # $0.10 total budget (in microcents)\n    ),\n)\n</code></pre>"},{"location":"guides/pipelines/#per-step-overrides","title":"Per-Step Overrides","text":"<p>Override configuration for specific steps:</p> <pre><code>from fastroai import Pipeline, PipelineConfig, StepConfig\n\npipeline = Pipeline(\n    name=\"processor\",\n    steps={...},\n    config=PipelineConfig(timeout=30.0),\n    step_configs={\n        \"slow_step\": StepConfig(timeout=120.0),\n        \"expensive_step\": StepConfig(cost_budget=50_000),\n    },\n)\n</code></pre>"},{"location":"guides/pipelines/#config-resolution-order","title":"Config Resolution Order","text":"<p>Most specific wins:</p> <ol> <li><code>PipelineConfig</code> defaults (lowest priority)</li> <li>Step class <code>.config</code> attribute</li> <li><code>step_configs[step_id]</code> override</li> <li>Per-call <code>ctx.run(timeout=..., retries=...)</code> override (highest priority)</li> </ol>"},{"location":"guides/pipelines/#accessing-data","title":"Accessing Data","text":""},{"location":"guides/pipelines/#pipeline-inputs","title":"Pipeline Inputs","text":"<p>Get data passed to <code>pipeline.execute()</code>:</p> <pre><code>async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n    # Required input - raises KeyError if missing\n    document = ctx.get_input(\"document\")\n\n    # Optional input with default\n    format_type = ctx.get_input(\"format\", \"json\")\n</code></pre>"},{"location":"guides/pipelines/#dependency-outputs","title":"Dependency Outputs","text":"<p>Get outputs from steps this step depends on:</p> <pre><code>async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n    # Get output from extract step\n    entities = ctx.get_dependency(\"extract\")\n\n    # With type hint for IDE support\n    entities = ctx.get_dependency(\"extract\", ExtractionResult)\n\n    # Optional dependency (might not exist in the graph)\n    metadata = ctx.get_dependency_or_none(\"fetch_metadata\", dict)\n</code></pre>"},{"location":"guides/pipelines/#application-dependencies","title":"Application Dependencies","text":"<p>Access your deps object (database connections, user context, API clients):</p> <pre><code>async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n    db = ctx.deps.session\n    user_id = ctx.deps.user_id\n    api_client = ctx.deps.external_api\n</code></pre>"},{"location":"guides/pipelines/#error-handling","title":"Error Handling","text":"<p>When a step fails, the pipeline raises <code>StepExecutionError</code>. You can catch this and handle it:</p> <pre><code>from fastroai import StepExecutionError\n\ntry:\n    result = await pipeline.execute(inputs, deps)\nexcept StepExecutionError as e:\n    print(f\"Step '{e.step_id}' failed: {e.original_error}\")\n</code></pre> <p>For cost budget violations specifically, the step raises <code>CostBudgetExceededError</code>:</p> <pre><code>from fastroai import CostBudgetExceededError\n\ntry:\n    result = await pipeline.execute(inputs, deps)\nexcept CostBudgetExceededError as e:\n    print(f\"Budget exceeded: spent {e.actual_microcents} of {e.budget_microcents}\")\n</code></pre>"},{"location":"guides/pipelines/#early-termination","title":"Early Termination","text":"<p>For multi-turn conversations or workflows that need user input, steps can signal that more information is needed:</p> <pre><code>from fastroai import BaseStep, ConversationState, ConversationStatus\n\nclass GatherInfoStep(BaseStep[None, ConversationState[dict]]):\n    async def execute(self, ctx) -&gt; ConversationState[dict]:\n        message = ctx.get_input(\"message\")\n        current_data = ctx.get_input(\"current_data\") or {}\n\n        # Extract info from message\n        if \"email\" in message.lower():\n            current_data[\"email\"] = extract_email(message)\n\n        # Check if we have everything we need\n        required = {\"name\", \"email\"}\n        missing = required - set(current_data.keys())\n\n        if not missing:\n            return ConversationState(\n                status=ConversationStatus.COMPLETE,\n                data=current_data,\n            )\n\n        return ConversationState(\n            status=ConversationStatus.INCOMPLETE,\n            data=current_data,\n            context={\"missing\": list(missing)},\n        )\n</code></pre> <p>When a step returns <code>INCOMPLETE</code>, the pipeline stops early:</p> <pre><code>result = await pipeline.execute(inputs, deps)\n\nif result.stopped_early:\n    missing = result.conversation_state.context[\"missing\"]\n    return {\"status\": \"incomplete\", \"need\": missing}\n\nreturn {\"status\": \"complete\", \"output\": result.output}\n</code></pre> <p>This lets you build conversational flows where the pipeline pauses to ask the user for more information, then resumes when they respond.</p>"},{"location":"guides/pipelines/#usage-tracking","title":"Usage Tracking","text":"<p>The result includes aggregated usage across all steps:</p> <pre><code>result = await pipeline.execute(inputs, deps)\n\nif result.usage:\n    print(f\"Input tokens: {result.usage.total_input_tokens}\")\n    print(f\"Output tokens: {result.usage.total_output_tokens}\")\n    print(f\"Total cost: ${result.usage.total_cost_dollars:.6f}\")\n</code></pre> <p>This is the sum of all agent calls across all steps. If you need per-step breakdowns, log them from within your step implementations.</p>"},{"location":"guides/pipelines/#key-files","title":"Key Files","text":"Component Location Pipeline <code>fastroai/pipelines/pipeline.py</code> BaseStep <code>fastroai/pipelines/base.py</code> StepContext <code>fastroai/pipelines/base.py</code> @step decorator <code>fastroai/pipelines/decorators.py</code> StepConfig <code>fastroai/pipelines/config.py</code> PipelineConfig <code>fastroai/pipelines/config.py</code> ConversationState <code>fastroai/pipelines/base.py</code> <p>\u2190 Cost Calculator Safe Tools \u2192</p>"},{"location":"guides/safe-tools/","title":"Safe Tools","text":"<p>Tools that call external services can fail. APIs time out, return errors, or hang indefinitely. When a regular tool fails, the whole agent request crashes and the user sees an error. The AI never gets a chance to respond gracefully.</p> <p><code>@safe_tool</code> wraps tools with timeout, retry, and graceful error handling. When something goes wrong, the AI receives an error message instead of an exception. It can then respond appropriately - apologize, suggest alternatives, or try a different approach.</p>"},{"location":"guides/safe-tools/#how-it-works","title":"How It Works","text":"<pre><code>from fastroai import safe_tool\n\n@safe_tool(timeout=10, max_retries=2)\nasync def fetch_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{location}\")\n        return response.text\n</code></pre> <p>When the API is slow or fails:</p> <ol> <li>The wrapper waits up to 10 seconds for the first attempt</li> <li>If it times out or errors, it retries (up to 2 more attempts)</li> <li>Retries use exponential backoff (0.1s, 0.2s, 0.4s...)</li> <li>If all attempts fail, it returns an error message string</li> </ol> <p>The AI sees \"Tool timed out after 2 attempts\" and can respond: \"I'm having trouble getting weather data right now. Would you like me to try again, or is there something else I can help with?\"</p> <p>Your request doesn't crash. You don't lose the prompt tokens. The user gets a response.</p>"},{"location":"guides/safe-tools/#decorator-options","title":"Decorator Options","text":"<pre><code>@safe_tool(\n    timeout=30,          # Seconds per attempt (default: 30)\n    max_retries=3,       # Total attempts (default: 3)\n    on_timeout=\"...\",    # Custom timeout message\n    on_error=\"...\",      # Custom error message\n)\n</code></pre> Parameter Default Description <code>timeout</code> <code>30</code> Maximum seconds per attempt <code>max_retries</code> <code>3</code> Total attempts before giving up <code>on_timeout</code> <code>\"Tool timed out after {max_retries} attempts\"</code> Message on timeout <code>on_error</code> <code>\"Tool failed: {error}\"</code> Message on error (use <code>{error}</code> placeholder)"},{"location":"guides/safe-tools/#custom-error-messages","title":"Custom Error Messages","text":"<p>Generic error messages are fine for development, but in production you want messages that help the AI respond well:</p> <pre><code>@safe_tool(\n    timeout=30,\n    on_timeout=\"Web search is taking too long. Suggest the user try a simpler query.\",\n    on_error=\"Web search is currently unavailable: {error}. Apologize and offer alternatives.\",\n)\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    ...\n</code></pre> <p>The AI receives these messages as tool output and incorporates them into its response. Clearer error messages lead to better AI responses.</p>"},{"location":"guides/safe-tools/#retry-behavior","title":"Retry Behavior","text":"<p>Retries use exponential backoff to avoid hammering a struggling service:</p> Attempt Delay Before 1 immediate 2 0.1s 3 0.2s 4 0.4s 5 0.8s <p>The delays are short because you're holding a user request. If a service needs multiple seconds to recover, three quick retries aren't going to help anyway.</p> <p>For services that need longer recovery time, reduce <code>max_retries</code> and increase <code>timeout</code> instead.</p>"},{"location":"guides/safe-tools/#organizing-tools-with-toolsets","title":"Organizing Tools with Toolsets","text":"<p>When you have multiple tools, group them:</p> <pre><code>from fastroai import safe_tool, SafeToolset\n\n@safe_tool(timeout=10)\nasync def get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather.\"\"\"\n    ...\n\n@safe_tool(timeout=30)\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    ...\n\nclass WebToolset(SafeToolset):\n    def __init__(self):\n        super().__init__(\n            tools=[get_weather, web_search],\n            name=\"web\",\n        )\n</code></pre> <p>Then pass the toolset to your agent:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You can check weather and search the web.\",\n    toolsets=[WebToolset()],\n)\n</code></pre> <p><code>SafeToolset</code> is a marker class that indicates all tools in the set have proper timeout and error handling. It extends PydanticAI's <code>FunctionToolset</code> with a name for logging and debugging.</p>"},{"location":"guides/safe-tools/#when-to-use-safe-tools","title":"When to Use Safe Tools","text":"<p>Use <code>@safe_tool</code> for anything that touches the outside world. Network requests to APIs that might be slow or down. Database queries that could timeout. File system operations on paths that might not exist. Third-party services with rate limits or auth that expires. Basically, if something outside your process can make the tool fail or hang, wrap it.</p> <p>You can skip it for pure computation - math, string manipulation, data transformation. These either work or they don't, and if they don't, something's fundamentally broken anyway. Same for simple lookups in local data structures or cached values.</p>"},{"location":"guides/safe-tools/#what-happens-when-tools-fail","title":"What Happens When Tools Fail","text":"<p>With regular tools, a timeout or error crashes the whole request:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    participant API\n\n    User-&gt;&gt;Agent: \"What's the weather?\"\n    Agent-&gt;&gt;Tool: get_weather(\"Paris\")\n    Tool-&gt;&gt;API: HTTP request\n    API--xTool: Timeout\n    Tool--xAgent: Exception\n    Agent--xUser: 500 Error</code></pre> <p>With safe tools, failures become messages the AI can work with:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    participant API\n\n    User-&gt;&gt;Agent: \"What's the weather?\"\n    Agent-&gt;&gt;Tool: get_weather(\"Paris\")\n    Tool-&gt;&gt;API: HTTP request\n    API--xTool: Timeout\n    Tool--&gt;&gt;Agent: \"Tool timed out\"\n    Agent-&gt;&gt;User: \"I'm having trouble checking weather right now...\"</code></pre> <p>The AI incorporates the failure into its response and the user gets a helpful answer instead of an error page.</p>"},{"location":"guides/safe-tools/#testing-safe-tools","title":"Testing Safe Tools","text":"<p>In tests, you probably want errors to propagate so you can verify your error handling. Mock the underlying service rather than the safe_tool wrapper:</p> <pre><code>@pytest.fixture\ndef mock_weather_api(monkeypatch):\n    async def mock_get(*args, **kwargs):\n        raise httpx.TimeoutException(\"test timeout\")\n    monkeypatch.setattr(httpx.AsyncClient, \"get\", mock_get)\n\nasync def test_weather_timeout(mock_weather_api):\n    result = await fetch_weather(\"London\")\n    assert \"timed out\" in result.lower()\n</code></pre> <p>The safe_tool wrapper returns the error message, so your test can verify the AI receives appropriate feedback.</p>"},{"location":"guides/safe-tools/#complete-example","title":"Complete Example","text":"<pre><code>from fastroai import FastroAgent, safe_tool, SafeToolset\n\n@safe_tool(timeout=10, max_retries=2)\nasync def get_stock_price(symbol: str) -&gt; str:\n    \"\"\"Get current stock price.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(f\"https://api.stocks.com/{symbol}\")\n        data = resp.json()\n        return f\"${data['price']}\"\n\n@safe_tool(\n    timeout=5,\n    on_error=\"Could not get exchange rate: {error}. Suggest using approximate rates.\",\n)\nasync def get_exchange_rate(from_currency: str, to_currency: str) -&gt; str:\n    \"\"\"Get exchange rate between currencies.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(\n            f\"https://api.exchange.com/rate?from={from_currency}&amp;to={to_currency}\"\n        )\n        return resp.json()[\"rate\"]\n\nclass FinanceToolset(SafeToolset):\n    def __init__(self):\n        super().__init__(\n            tools=[get_stock_price, get_exchange_rate],\n            name=\"finance\",\n        )\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a financial assistant with access to stock prices and exchange rates.\",\n    toolsets=[FinanceToolset()],\n)\n\nresponse = await agent.run(\"What's the current price of AAPL in euros?\")\n</code></pre> <p>If the stock API is down, the agent might respond: \"I'm having trouble getting stock prices right now. Based on recent data, AAPL was trading around $180. Would you like me to try again in a moment?\"</p> <p>That's a much better experience than a 500 error.</p>"},{"location":"guides/safe-tools/#key-files","title":"Key Files","text":"Component Location @safe_tool <code>fastroai/tools/decorators.py</code> SafeToolset <code>fastroai/tools/toolsets.py</code> FunctionToolsetBase <code>fastroai/tools/toolsets.py</code> <p>\u2190 Pipelines Tracing \u2192</p>"},{"location":"guides/tracing/","title":"Tracing","text":"<p>AI calls are part of larger request flows. Without tracing, you know a request was slow or expensive, but not which AI call caused it.</p> <p>FastroAI provides a protocol-based tracing interface that integrates with any observability backend. Pass a tracer to correlate AI calls with the rest of your application - each response includes a trace ID you can search for in your observability platform.</p>"},{"location":"guides/tracing/#how-tracing-works","title":"How Tracing Works","text":"<pre><code>sequenceDiagram\n    participant App\n    participant FastroAgent\n    participant Tracer\n    participant LLM\n\n    App-&gt;&gt;FastroAgent: run(message, tracer)\n    FastroAgent-&gt;&gt;Tracer: span(\"agent.run\")\n    Tracer--&gt;&gt;FastroAgent: trace_id\n    FastroAgent-&gt;&gt;LLM: API call\n    LLM--&gt;&gt;FastroAgent: response + tokens\n    FastroAgent-&gt;&gt;Tracer: log_metric(tokens, cost)\n    FastroAgent-&gt;&gt;Tracer: close span\n    FastroAgent--&gt;&gt;App: ChatResponse with trace_id</code></pre> <p>When you pass a tracer to <code>agent.run()</code>, FastroAgent creates a span with a unique trace ID, makes the LLM call, records token usage and cost as metrics, then closes the span. The trace ID comes back in the response so you can correlate it with your logs.</p>"},{"location":"guides/tracing/#quick-start","title":"Quick Start","text":"<p><code>SimpleTracer</code> logs to Python's logging module. Good for development and debugging:</p> <pre><code>from fastroai import FastroAgent, SimpleTracer\n\ntracer = SimpleTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\n\nresponse = await agent.run(\"Hello!\", tracer=tracer)\nprint(response.trace_id)  # \"abc12345-...\"\n</code></pre> <p>Logs output:</p> <pre><code>INFO [abc12345] Starting fastroai.agent.run\nINFO [abc12345] Metric input_tokens=12\nINFO [abc12345] Metric output_tokens=8\nINFO [abc12345] Metric cost_microcents=250\nINFO [abc12345] Completed fastroai.agent.run in 0.847s\n</code></pre> <p>When something fails:</p> <pre><code>ERROR [abc12345] FAILED fastroai.agent.run after 0.456s: Connection timeout\n</code></pre>"},{"location":"guides/tracing/#built-in-tracers","title":"Built-in Tracers","text":""},{"location":"guides/tracing/#simpletracer","title":"SimpleTracer","text":"<p>Logs to Python's logging module. Use for development, debugging, or when you just need basic visibility:</p> <pre><code>from fastroai import SimpleTracer\nimport logging\n\n# Use default logger\ntracer = SimpleTracer()\n\n# Or use your own logger\nlogger = logging.getLogger(\"my_app.ai\")\ntracer = SimpleTracer(logger=logger)\n</code></pre> <p>The trace ID appears in every log line, so you can grep for it:</p> <pre><code>grep \"abc12345\" app.log\n</code></pre>"},{"location":"guides/tracing/#logfiretracer","title":"LogfireTracer","text":"<p>Integrates with Pydantic Logfire, a modern observability platform built by the Pydantic team. Install with:</p> <pre><code>pip install fastroai[logfire]\n</code></pre> <p>Usage:</p> <pre><code>import logfire\nfrom fastroai import FastroAgent, LogfireTracer\n\n# Configure logfire once at startup\nlogfire.configure()\n\ntracer = LogfireTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\", tracer=tracer)\n</code></pre> <p>View your traces in the Logfire dashboard at logfire.pydantic.dev.</p>"},{"location":"guides/tracing/#nooptracer","title":"NoOpTracer","text":"<p>Does nothing. Use when tracing is disabled, in tests, or when you need trace IDs for compatibility but don't want actual tracing:</p> <pre><code>from fastroai import NoOpTracer\n\ntracer = NoOpTracer()\n\n# Still generates trace IDs for compatibility\nasync with tracer.span(\"operation\") as trace_id:\n    result = await do_something()\n    # trace_id exists, but no logging happens\n</code></pre> <p>FastroAI uses <code>NoOpTracer</code> internally when you don't pass a tracer. Your code doesn't crash, you just don't get observability.</p>"},{"location":"guides/tracing/#the-tracer-protocol","title":"The Tracer Protocol","text":"<p>To integrate with your observability platform, implement this protocol:</p> <pre><code>from typing import Protocol, Any\nfrom contextlib import AbstractAsyncContextManager\n\nclass Tracer(Protocol):\n    def span(self, name: str, **attributes: Any) -&gt; AbstractAsyncContextManager[str]:\n        \"\"\"Create a traced span. Yields a unique trace ID.\"\"\"\n        ...\n\n    def log_metric(self, trace_id: str, name: str, value: Any) -&gt; None:\n        \"\"\"Log a metric associated with a trace.\"\"\"\n        ...\n\n    def log_error(self, trace_id: str, error: Exception, context: dict | None = None) -&gt; None:\n        \"\"\"Log an error associated with a trace.\"\"\"\n        ...\n</code></pre> <p>Three methods. That's it. <code>span()</code> creates a context manager that yields a trace ID. <code>log_metric()</code> records values during the span. <code>log_error()</code> records failures.</p>"},{"location":"guides/tracing/#platform-integrations","title":"Platform Integrations","text":""},{"location":"guides/tracing/#logfire","title":"Logfire","text":"<p>FastroAI includes a built-in <code>LogfireTracer</code>. See the LogfireTracer section above for usage.</p> <p>Logfire gives you a nice UI to explore traces, and the Pydantic team maintains it, so it plays well with PydanticAI.</p>"},{"location":"guides/tracing/#opentelemetry","title":"OpenTelemetry","text":"<p>OpenTelemetry is the standard for distributed tracing. Most observability platforms (Datadog, Honeycomb, Jaeger, etc.) support OTLP export:</p> <pre><code>import uuid\nfrom contextlib import asynccontextmanager\nfrom opentelemetry import trace as otel_trace\n\nclass OTelTracer:\n    def __init__(self):\n        self.tracer = otel_trace.get_tracer(\"fastroai\")\n\n    @asynccontextmanager\n    async def span(self, name: str, **attrs):\n        trace_id = str(uuid.uuid4())\n        with self.tracer.start_as_current_span(name) as span:\n            span.set_attribute(\"trace_id\", trace_id)\n            for key, value in attrs.items():\n                span.set_attribute(key, value)\n            yield trace_id\n\n    def log_metric(self, trace_id: str, name: str, value):\n        span = otel_trace.get_current_span()\n        span.set_attribute(f\"metric.{name}\", value)\n\n    def log_error(self, trace_id: str, error: Exception, context=None):\n        span = otel_trace.get_current_span()\n        span.record_exception(error)\n        span.set_status(otel_trace.Status(otel_trace.StatusCode.ERROR))\n</code></pre> <p>Configure your OTLP exporter separately, then spans show up in your platform.</p>"},{"location":"guides/tracing/#using-tracers","title":"Using Tracers","text":""},{"location":"guides/tracing/#with-agents","title":"With Agents","text":"<pre><code>response = await agent.run(\"Hello!\", tracer=tracer)\nprint(response.trace_id)\n</code></pre>"},{"location":"guides/tracing/#with-pipelines","title":"With Pipelines","text":"<pre><code>result = await pipeline.execute(\n    {\"document\": doc},\n    deps=my_deps,\n    tracer=tracer,\n)\n</code></pre> <p>The tracer flows through to all steps. Each step's agent calls share the same trace context.</p>"},{"location":"guides/tracing/#custom-spans","title":"Custom Spans","text":"<p>Create your own spans within step execution for operations you want to trace:</p> <pre><code>class MyStep(BaseStep[MyDeps, str]):\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        if ctx.tracer:\n            async with ctx.tracer.span(\"custom_operation\", user_id=ctx.deps.user_id) as trace_id:\n                result = await self.do_something()\n                ctx.tracer.log_metric(trace_id, \"result_size\", len(result))\n                return result\n\n        # Fallback when no tracer\n        return await self.do_something()\n</code></pre>"},{"location":"guides/tracing/#what-gets-traced","title":"What Gets Traced","text":"<p>FastroAI automatically creates spans for agent runs (<code>fastroai.agent.run</code>, <code>fastroai.agent.run_stream</code>) and pipeline execution (<code>pipeline.{name}</code>). Within those spans, it logs token usage metrics: <code>input_tokens</code>, <code>output_tokens</code>, and <code>cost_microcents</code>.</p> <p>The trace ID is included in every <code>ChatResponse</code>:</p> <pre><code>response = await agent.run(\"Hello!\")\nprint(response.trace_id)  # Use this to correlate with your logs\n</code></pre> <p>Log the trace ID in your application code, and you can trace from \"user clicked button\" all the way to \"AI returned 347 tokens\".</p>"},{"location":"guides/tracing/#trace-correlation","title":"Trace Correlation","text":"<p>The real value of tracing is correlation. Here's how to connect AI calls with your request handling:</p> <pre><code>from fastroai import LogfireTracer\n\nasync def handle_request(request):\n    tracer = LogfireTracer()\n\n    # Your business logic span\n    async with tracer.span(\"handle_request\", user_id=request.user_id) as parent_trace:\n        # AI call is nested under your span\n        response = await agent.run(request.message, tracer=tracer)\n\n        # Log business metrics\n        tracer.log_metric(response.trace_id, \"response_length\", len(response.content))\n\n        return response.content\n</code></pre> <p>When something goes wrong, search your observability platform by trace ID to see:</p> <ol> <li>The HTTP request came in</li> <li>You authenticated the user</li> <li>The AI call started</li> <li>Token usage and cost</li> <li>Where it failed or how long it took</li> <li>The response went out</li> </ol> <p>Without tracing, you're debugging blind.</p>"},{"location":"guides/tracing/#production-considerations","title":"Production Considerations","text":"<p>Sampling: In high-volume production, you might not want to trace every request. Most observability platforms support sampling - trace 10% of requests, or always trace errors.</p> <p>Costs: Tracing adds some overhead. The overhead is small (microseconds), but the data you send to your observability platform costs money. Consider what you actually need.</p> <p>Sensitive data: Don't log prompts or responses in production traces. They might contain PII or sensitive business data. Log token counts and costs, not content.</p>"},{"location":"guides/tracing/#key-files","title":"Key Files","text":"Component Location Tracer protocol <code>fastroai/tracing/tracer.py</code> SimpleTracer <code>fastroai/tracing/tracer.py</code> LogfireTracer <code>fastroai/tracing/tracer.py</code> NoOpTracer <code>fastroai/tracing/tracer.py</code> <p>\u2190 Safe Tools Recipes \u2192</p>"},{"location":"learn/","title":"Learn FastroAI","text":"<p>Build AI applications step by step.</p> <p>This isn't a reference manual. Each section builds on the previous, and by the end you'll know how to use FastroAI's cost tracking, pipelines, and tools in production.</p> <p>Choose Your Starting Point</p> <p>New to AI development? Start from the beginning - we cover everything from basic LLM concepts.</p> <p>Already know PydanticAI? Jump to Cost Tracking (section 3) to see what FastroAI adds.</p> <p>Just want code? Check the Quick Start for the 2-minute setup.</p>"},{"location":"learn/#what-youll-build","title":"What You'll Build","text":"<p>Instead of toy examples, you'll work through progressively complex scenarios that mirror real production applications:</p> FundamentalsProduction FeaturesOrchestration <p>Understand the building blocks</p> <ul> <li>What LLMs are and how to call them</li> <li>Creating agents with system prompts</li> <li>Understanding tokens and why costs matter</li> <li>Structured output with Pydantic models</li> </ul> <p>Add real-world capabilities</p> <ul> <li>Tools that call external services safely</li> <li>Conversation history and stateless patterns</li> <li>Streaming responses for better UX</li> <li>Tracing for debugging and observability</li> </ul> <p>Build complex workflows</p> <ul> <li>Multi-step pipelines with dependencies</li> <li>Parallel execution for performance</li> <li>Cost budgets and early termination</li> <li>Error handling that doesn't crash requests</li> </ul>"},{"location":"learn/#the-learning-path","title":"The Learning Path","text":"<ul> <li> <p> 1. LLM Basics</p> <p>\"What are these things anyway?\"</p> <p>What language models are, how to call them, understanding tokens and why costs matter for production applications.</p> <p>(Coming soon)</p> </li> <li> <p> 2. Your First Agent</p> <p>\"Hello, AI world\"</p> <p>Creating a FastroAgent, crafting system prompts, running queries, and understanding the response format.</p> <p>(Coming soon)</p> </li> <li> <p> 3. Cost Tracking</p> <p>\"How much did that cost?\"</p> <p>Why microcents matter for billing, tracking usage across calls, and setting cost budgets.</p> <p>(Coming soon)</p> </li> <li> <p> 4. Structured Output</p> <p>\"Give me data, not strings\"</p> <p>Getting Pydantic models back instead of raw text. Type-safe responses for real applications.</p> <p>(Coming soon)</p> </li> <li> <p> 5. Tools</p> <p>\"Let the agent do things\"</p> <p>Giving agents capabilities with <code>@safe_tool</code>. Timeout, retry, and graceful error handling.</p> <p>(Coming soon)</p> </li> <li> <p> 6. Conversations</p> <p>\"Remember what we talked about\"</p> <p>Message history patterns, stateless design, and when to use each approach.</p> <p>(Coming soon)</p> </li> <li> <p> 7. Streaming</p> <p>\"Show me as you think\"</p> <p>Real-time responses, handling chunks, and cost tracking with streaming.</p> <p>(Coming soon)</p> </li> <li> <p> 8. Pipelines</p> <p>\"Chain steps together\"</p> <p>Multi-step workflows with automatic dependency resolution and cost aggregation.</p> <p>(Coming soon)</p> </li> <li> <p> 9. Parallel Execution</p> <p>\"Run independent steps concurrently\"</p> <p>DAG-based execution, automatic parallelization, and performance optimization.</p> <p>(Coming soon)</p> </li> <li> <p> 10. Production</p> <p>\"Ship it with confidence\"</p> <p>Tracing integration, error handling patterns, and observability for production systems.</p> <p>(Coming soon)</p> </li> </ul>"},{"location":"learn/#alternative-learning-paths","title":"Alternative Learning Paths","text":"By Time AvailableBy Experience LevelBy Goal <ul> <li>30 minutes: Sections 1-2 \u2192 Understand agents and basic usage</li> <li>2 hours: Sections 1-5 \u2192 Build a functional AI application</li> <li>Half day: Sections 1-8 \u2192 Production-ready with pipelines</li> <li>Full day: All sections \u2192 Complete mastery</li> </ul> <ul> <li>New to AI: Start from section 1, don't skip fundamentals</li> <li>Know LLMs, new to PydanticAI: Start at section 2</li> <li>Know PydanticAI: Jump to section 3 (Cost Tracking) for FastroAI specifics</li> <li>Production developer: Focus on sections 5, 8-10</li> </ul> <ul> <li>\"I want to understand AI agents\" \u2192 Sections 1-4</li> <li>\"I want to build a chatbot\" \u2192 Sections 1-6</li> <li>\"I want to build a data pipeline\" \u2192 Sections 1-4, then 8-9</li> <li>\"I want to go to production\" \u2192 All sections, focus on 5 and 10</li> </ul>"},{"location":"learn/#prerequisites","title":"Prerequisites","text":"<p>You should be comfortable with:</p> <ul> <li>Python async/await syntax</li> <li>Basic Pydantic models</li> <li>Environment variables and API keys</li> </ul> <p>You don't need prior experience with:</p> <ul> <li>PydanticAI (we'll cover what you need)</li> <li>AI/LLM concepts (we start from basics)</li> <li>Production infrastructure (we'll build up to it)</li> </ul> <p>Ready to learn?</p> <p>The learning path is coming soon. In the meantime, check the Quick Start or explore the Guides for deep dives into specific features.</p> <p>Quick Start \u2192 Browse Guides \u2192</p>"},{"location":"recipes/","title":"Recipes","text":"<p>Copy-paste solutions for common patterns.</p> <p>Recipes are complete, working examples you can adapt for your use case. Unlike Guides (which explain how things work), Recipes give you code you can drop into your project immediately.</p> <p>Coming Soon</p> <p>Recipes are being written. In the meantime, check out the Guides for detailed explanations of each feature, or see the examples in the GitHub repo.</p> <ul> <li> <p> More Recipes Coming Soon</p> <p>We're working on recipes for common patterns like document processing, parallel research, conversation bots, and cost budget enforcement.</p> </li> </ul>"},{"location":"recipes/#what-makes-a-good-recipe","title":"What Makes a Good Recipe","text":"<p>Each recipe includes:</p> Section What It Contains Problem What you're trying to solve Solution Complete, runnable code How It Works Brief explanation of key parts Variations Common modifications <p>Recipes assume you've read the Quick Start and understand FastroAgent basics.</p>"},{"location":"recipes/#contributing","title":"Contributing","text":"<p>Have a pattern that would help others? We welcome recipe contributions. Open a PR on GitHub with your recipe following the format above.</p> <p>\u2190 Guides API Reference \u2192</p>"}]}