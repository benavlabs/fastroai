{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastroAI","text":"<p> Lightweight AI orchestration built on PydanticAI. </p> <p> </p> <p> FastroAI wraps PydanticAI with production essentials: cost tracking in microcents, multi-step pipelines, and tools that handle failures gracefully. You get everything PydanticAI offers, plus the infrastructure you'd build yourself anyway. </p> <p>Note: FastroAI is experimental, it was extracted into a package from code that we had in production in different contexts. We built it for ourselves but you're free to use and contribute. The API may change between versions and you'll probably find bugs, we're here to fix them. Use in production at your own risk (we do).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Cost Tracking: Automatic cost calculation in microcents. Integer math, no floating-point drift.</li> <li>Pipelines: DAG-based workflows with automatic parallelization and dependency resolution.</li> <li>Safe Tools: Timeout, retry, and graceful error handling for AI tools.</li> <li>Tracing: Protocol-based integration with any observability platform.</li> <li>Structured Output: Type-safe responses with Pydantic models.</li> <li>Streaming: Real-time responses with cost tracking on the final chunk.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+: Modern async/await and type hints.</li> <li>AI API Key: OpenAI, Anthropic, Google, or other provider.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#1-install-fastroai","title":"1. Install FastroAI","text":"pipuvpoetry <pre><code>pip install fastroai\n</code></pre> <pre><code>uv add fastroai\n</code></pre> <pre><code>poetry add fastroai\n</code></pre>"},{"location":"#2-set-your-api-key","title":"2. Set Your API Key","text":"OpenAIAnthropicGoogle <pre><code>export OPENAI_API_KEY=\"sk-your-key-here\"\n</code></pre> <pre><code>export ANTHROPIC_API_KEY=\"sk-ant-your-key-here\"\n</code></pre> <pre><code>export GOOGLE_API_KEY=\"your-key-here\"\n</code></pre>"},{"location":"#3-run-your-first-agent","title":"3. Run Your First Agent","text":"<pre><code>import asyncio\nfrom fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\nasync def main():\n    response = await agent.run(\"What is 2 + 2?\")\n\n    print(response.output)\n    print(f\"Tokens: {response.input_tokens} in, {response.output_tokens} out\")\n    print(f\"Cost: ${response.cost_dollars:.6f}\")\n\nasyncio.run(main())\n</code></pre> <p>Output:</p> <pre><code>2 + 2 equals 4.\nTokens: 24 in, 8 out\nCost: $0.000120\n</code></pre> <p>That's it. You have an AI agent with automatic cost tracking.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#single-agent-calls","title":"Single Agent Calls","text":"<p><code>FastroAgent</code> is a thin wrapper around PydanticAI's Agent. It adds automatic cost tracking and a consistent response format, but otherwise stays out of your way. All PydanticAI features work exactly as documented:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a helpful assistant.\",\n)\n\nresponse = await agent.run(\"What is the capital of France?\")\nprint(response.output)\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\n</code></pre>"},{"location":"#structured-output","title":"Structured Output","text":"<p>Get Pydantic models back instead of strings:</p> <pre><code>from pydantic import BaseModel\nfrom fastroai import FastroAgent\n\nclass MovieReview(BaseModel):\n    title: str\n    rating: int\n    summary: str\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    output_type=MovieReview,\n)\n\nresponse = await agent.run(\"Review the movie Inception\")\nprint(response.output.title)   # \"Inception\"\nprint(response.output.rating)  # 9\n</code></pre>"},{"location":"#multi-step-pipelines","title":"Multi-Step Pipelines","text":"<p>Real applications often need multiple AI calls: extract entities, then classify them, then generate a response. You could chain these manually with <code>await</code>, but then you're writing boilerplate for dependency ordering, parallel execution, and cost aggregation.</p> <p>Pipelines handle this. Declare your steps and dependencies, and FastroAI runs them in the right order, parallelizes independent steps, and tracks costs across the whole workflow. All FastroAI features flow through: you get cost tracking per step and per pipeline, plus distributed tracing across the entire flow.</p> <p>For simple DAG workflows, this is less verbose than pydantic-graph and far simpler than durable execution frameworks like Temporal. It's enough for most AI orchestration needs:</p> <pre><code>from fastroai import Pipeline, step, StepContext, FastroAgent\n\nextractor = FastroAgent(model=\"openai:gpt-4o-mini\", system_prompt=\"Extract entities.\")\nclassifier = FastroAgent(model=\"openai:gpt-4o-mini\", system_prompt=\"Classify documents.\")\n\n@step\nasync def extract(ctx: StepContext[None]) -&gt; str:\n    document = ctx.get_input(\"document\")\n    response = await ctx.run(extractor, f\"Extract entities: {document}\")\n    return response.output\n\n@step(timeout=30.0, retries=2)\nasync def classify(ctx: StepContext[None]) -&gt; str:\n    entities = ctx.get_dependency(\"extract\")\n    response = await ctx.run(classifier, f\"Classify based on: {entities}\")\n    return response.output\n\npipeline = Pipeline(\n    name=\"document_processor\",\n    steps={\"extract\": extract, \"classify\": classify},\n    dependencies={\"classify\": [\"extract\"]},\n)\n\nresult = await pipeline.execute({\"document\": \"Apple announced...\"}, deps=None)\nprint(f\"Total cost: ${result.usage.total_cost_dollars:.6f}\")\n</code></pre>"},{"location":"#safe-tools","title":"Safe Tools","text":"<p>When you give an AI agent tools that call external APIs, those APIs will eventually fail. They'll time out, return errors, or hang indefinitely. With regular tools, this crashes your entire request and the user sees an error page.</p> <p><code>@safe_tool</code> wraps tools with timeout, retry, and graceful error handling. When something fails, instead of raising an exception, the AI receives an error message it can work with:</p> <pre><code>from fastroai import safe_tool\n\n@safe_tool(timeout=10, max_retries=2)\nasync def fetch_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(f\"https://api.weather.com/{location}\")\n        return resp.text\n</code></pre> <p>If the API times out, the AI sees \"Tool timed out after 2 attempts\" and can respond: \"I'm having trouble checking the weather right now. Would you like me to try again?\" Your request doesn't crash, you don't lose the prompt tokens, and the user gets a response.</p>"},{"location":"#response-fields","title":"Response Fields","text":"<p>Every <code>ChatResponse</code> includes:</p> Field Type Description <code>content</code> <code>str</code> The response text <code>output</code> <code>OutputT</code> Typed output (same as content for string agents) <code>input_tokens</code> <code>int</code> Prompt tokens consumed <code>output_tokens</code> <code>int</code> Completion tokens generated <code>cost_microcents</code> <code>int</code> Cost in 1/1,000,000 of a dollar <code>cost_dollars</code> <code>float</code> Cost in dollars (for display) <code>processing_time_ms</code> <code>int</code> Wall-clock time <code>trace_id</code> <code>str</code> Tracing correlation ID <p>Use <code>cost_microcents</code> when aggregating costs across many calls. Use <code>cost_dollars</code> for display.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"#fastroai-template","title":"FastroAI Template","text":"<p>Looking for a complete AI SaaS starter? FastroAI Template includes authentication, payments, background tasks, and more built on top of this library.</p>          Start Learning               Browse Guides"},{"location":"changelog/","title":"FastroAI Changelog","text":""},{"location":"changelog/#introduction","title":"Introduction","text":"<p>The Changelog documents all notable changes made to FastroAI. This includes new features, bug fixes, and improvements. It's organized by version and date, providing a clear history of the library's development.</p>"},{"location":"changelog/#030-dec-17-2025","title":"[0.3.0] - Dec 17, 2025","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>LogfireTracer by @igorbenav</li> <li>Built-in tracer implementation for Pydantic's Logfire observability platform</li> <li>Implements the <code>Tracer</code> protocol - drop-in replacement for <code>SimpleTracer</code></li> <li>Automatic span creation with <code>_tags=[\"fastroai\"]</code> for easy filtering in Logfire dashboard</li> <li>Metric logging via <code>logfire.info()</code> with trace correlation</li> <li>Error logging with full exception info via <code>logfire.error()</code></li> <li> <p>Clear <code>ImportError</code> when logfire package is not installed</p> </li> <li> <p>Optional Dependency by @igorbenav</p> </li> <li>Install with <code>pip install fastroai[logfire]</code> to enable Logfire support</li> <li>Logfire remains optional - core functionality works without it</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Added LogfireTracer to built-in tracers section in tracing guide</li> <li>Added LogfireTracer to API reference</li> <li>Updated README with Logfire installation instructions</li> </ul>"},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>LogfireTracer implementation by @igorbenav</li> </ul> <p>Full Changelog: https://github.com/benavlabs/fastroai/compare/v0.2.0...v0.3.0</p>"},{"location":"changelog/#020-dec-16-2025","title":"[0.2.0] - Dec 16, 2025","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>FastroAgent - PydanticAI wrapper with automatic cost calculation and tracing</li> <li>Pipeline - DAG-based workflow orchestration with automatic parallelization</li> <li>@step decorator - Concise function-based pipeline step definitions</li> <li>@safe_tool - Production-safe tool decorator with timeout and retry</li> <li>CostCalculator - Precise cost tracking using microcents (integer arithmetic)</li> <li>Tracer Protocol - Protocol-based tracing interface for observability integration</li> <li>SimpleTracer - Logging-based tracer for development</li> <li>NoOpTracer - No-op tracer for testing or disabled tracing</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Complete documentation site with guides, API reference, and recipes</li> <li>MkDocs Material theme with dark/light mode support</li> </ul> <p>Full Changelog: https://github.com/benavlabs/fastroai/compare/v0.1.0...v0.2.0</p>"},{"location":"changelog/#010-dec-15-2025","title":"[0.1.0] - Dec 15, 2025","text":"<p>Initial release.</p> <ul> <li>Core FastroAgent functionality</li> <li>Basic pipeline support</li> <li>Cost calculation primitives</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for FastroAI's public API.</p> <p>All classes, functions, and protocols documented here are considered stable and follow semantic versioning.</p> <p>Looking for explanations?</p> <p>This is a reference, not a tutorial. For explanations and examples, see the Guides.</p>"},{"location":"api/#core-components","title":"Core Components","text":"<ul> <li> <p> Agent</p> <p>FastroAgent, AgentConfig, ChatResponse, StreamChunk</p> <p> Learn more</p> </li> <li> <p> Pipelines</p> <p>Pipeline, BaseStep, StepContext, configurations</p> <p> Learn more</p> </li> <li> <p> Tools</p> <p>@safe_tool decorator, SafeToolset, FunctionToolsetBase</p> <p> Learn more</p> </li> <li> <p> Usage</p> <p>CostCalculator with microcents precision</p> <p> Learn more</p> </li> <li> <p> Tracing</p> <p>Tracer protocol, SimpleTracer, LogfireTracer, NoOpTracer</p> <p> Learn more</p> </li> </ul>"},{"location":"api/#quick-import-reference","title":"Quick Import Reference","text":"<pre><code>from fastroai import (\n    # Agent\n    FastroAgent,\n    AgentConfig,\n    ChatResponse,\n    StreamChunk,\n\n    # Pipelines\n    Pipeline,\n    PipelineResult,\n    PipelineConfig,\n    BaseStep,\n    StepContext,\n    StepConfig,\n    step,\n    ConversationState,\n    ConversationStatus,\n\n    # Tools\n    safe_tool,\n    SafeToolset,\n    FunctionToolsetBase,\n\n    # Tracing\n    Tracer,\n    SimpleTracer,\n    LogfireTracer,\n    NoOpTracer,\n\n    # Usage\n    CostCalculator,\n\n    # Errors\n    FastroAIError,\n    PipelineValidationError,\n    CostBudgetExceededError,\n)\n</code></pre>"},{"location":"api/#error-hierarchy","title":"Error Hierarchy","text":"<p>All FastroAI exceptions inherit from <code>FastroAIError</code>, so you can catch all library errors with a single except clause:</p> <pre><code>FastroAIError                    # Base for all FastroAI errors\n\u251c\u2500\u2500 PipelineValidationError      # Invalid pipeline configuration\n\u251c\u2500\u2500 StepExecutionError           # Step failed during execution\n\u2514\u2500\u2500 CostBudgetExceededError      # Cost budget exceeded\n</code></pre> <pre><code>try:\n    result = await pipeline.execute(inputs, deps)\nexcept FastroAIError as e:\n    logger.error(f\"FastroAI error: {e}\")\n</code></pre> <p>\u2190 Recipes Agent \u2192</p>"},{"location":"api/agent/","title":"Agent","text":"<p>The agent module provides <code>FastroAgent</code>, a wrapper around PydanticAI's Agent with automatic cost calculation, distributed tracing, and a consistent response format.</p>"},{"location":"api/agent/#fastroagent","title":"FastroAgent","text":""},{"location":"api/agent/#fastroai.agent.FastroAgent","title":"<code>fastroai.agent.FastroAgent</code>","text":"<p>AI agent with usage tracking, cost calculation, and tracing.</p> <p>Wraps PydanticAI's Agent to provide: - Automatic cost calculation in microcents - Optional distributed tracing - Streaming and non-streaming modes - Consistent ChatResponse format - Structured output support via output_type</p> <p>The agent is STATELESS regarding conversation history. Callers load history from their storage and pass it to run().</p> <p>Examples:</p> <pre><code># Basic usage (returns string)\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are helpful.\",\n)\nresponse = await agent.run(\"Hello!\")\nprint(response.content)\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\n\n# With structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    value: int\n    explanation: str\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    output_type=Answer,\n)\nresponse = await agent.run(\"What is 2+2?\")\nprint(response.output.value)  # 4\n\n# With conversation history (you load it)\nhistory = await my_memory_service.load(user_id)\nresponse = await agent.run(\"Continue\", message_history=history)\nawait my_memory_service.save(user_id, \"Continue\", response.content)\n\n# With tracing\nfrom fastroai import SimpleTracer\ntracer = SimpleTracer()\nresponse = await agent.run(\"Hello\", tracer=tracer)\n\n# With custom deps for tools\nresponse = await agent.run(\"Search for news\", deps=MyDeps(api_key=\"...\"))\n\n# Streaming\nasync for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.agent","title":"<code>agent</code>  <code>property</code>","text":"<p>Access the underlying PydanticAI agent.</p> <p>Returns:</p> Type Description <code>Agent[Any, OutputT]</code> <p>The wrapped PydanticAI Agent instance.</p>"},{"location":"api/agent/#fastroai.agent.FastroAgent.__init__","title":"<code>__init__(config=None, agent=None, output_type=None, toolsets=None, cost_calculator=None, **kwargs)</code>","text":"<p>Initialize FastroAgent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AgentConfig | None</code> <p>Agent configuration. If None, creates from kwargs.</p> <code>None</code> <code>agent</code> <code>Agent[Any, OutputT] | None</code> <p>Pre-configured PydanticAI Agent (escape hatch).   If provided, config is only used for cost calculation.</p> <code>None</code> <code>output_type</code> <code>type[OutputT] | None</code> <p>Pydantic model for structured output. Defaults to str.</p> <code>None</code> <code>toolsets</code> <code>list[AbstractToolset] | None</code> <p>Tool sets available to the agent.</p> <code>None</code> <code>cost_calculator</code> <code>CostCalculator | None</code> <p>Cost calculator. Default uses standard pricing.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to AgentConfig if config is None.      Common: model, system_prompt, temperature, max_tokens.</p> <code>{}</code> <p>Examples:</p> <pre><code># Using config object\nconfig = AgentConfig(model=\"gpt-4o\", temperature=0.3)\nagent = FastroAgent(config=config)\n\n# Using kwargs (simpler)\nagent = FastroAgent(model=\"gpt-4o\", temperature=0.5)\n\n# With structured output\nagent = FastroAgent(model=\"gpt-4o\", output_type=MyResponseModel)\n\n# Custom pricing override (e.g., volume discount)\ncalc = CostCalculator(pricing_overrides={\n    \"gpt-4o\": {\"input_per_mtok\": 2.00, \"output_per_mtok\": 8.00}\n})\nagent = FastroAgent(cost_calculator=calc)\n\n# Escape hatch: your own PydanticAI agent\nfrom pydantic_ai import Agent\npydantic_agent = Agent(model=\"gpt-4o\", output_type=MyType)\nagent = FastroAgent(agent=pydantic_agent)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.run","title":"<code>run(message, deps=None, message_history=None, model_settings=None, tracer=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a single agent interaction.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>User message to send.</p> required <code>deps</code> <code>DepsT | None</code> <p>Dependencies passed to tools. Can be any type.</p> <code>None</code> <code>message_history</code> <code>list[ModelMessage] | None</code> <p>Previous messages (you load these from your storage).</p> <code>None</code> <code>model_settings</code> <code>ModelSettings | None</code> <p>Runtime model config overrides.</p> <code>None</code> <code>tracer</code> <code>Tracer | None</code> <p>Tracer for distributed tracing.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to PydanticAI Agent.run().</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatResponse[OutputT]</code> <p>ChatResponse with content, usage, cost, and trace_id.</p> <p>Examples:</p> <pre><code># Simple usage\nresponse = await agent.run(\"Hello!\")\nprint(response.content)\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\n\n# With conversation history\nhistory = await memory.load(user_id)\nresponse = await agent.run(\"Continue\", message_history=history)\nawait memory.save(user_id, \"Continue\", response.content)\n\n# With tracing\ntracer = SimpleTracer()\nresponse = await agent.run(\"Hello\", tracer=tracer)\nprint(f\"Trace ID: {response.trace_id}\")\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.run_stream","title":"<code>run_stream(message, deps=None, message_history=None, model_settings=None, tracer=None, **kwargs)</code>  <code>async</code>","text":"<p>Execute a streaming agent interaction.</p> <p>Yields StreamChunk objects as the response is generated. The final chunk has is_final=True and includes complete usage data.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>User message to send.</p> required <code>deps</code> <code>DepsT | None</code> <p>Dependencies passed to tools.</p> <code>None</code> <code>message_history</code> <code>list[ModelMessage] | None</code> <p>Previous messages.</p> <code>None</code> <code>model_settings</code> <code>ModelSettings | None</code> <p>Runtime model config overrides.</p> <code>None</code> <code>tracer</code> <code>Tracer | None</code> <p>Tracer for distributed tracing.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Passed to PydanticAI Agent.run_stream().</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[StreamChunk[OutputT], None]</code> <p>StreamChunk objects. Final chunk has usage_data.</p> <p>Examples:</p> <pre><code>async for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.FastroAgent.as_step","title":"<code>as_step(prompt)</code>","text":"<p>Turn this agent into a pipeline step.</p> <p>Creates a BaseStep that runs this agent with the given prompt and returns the agent's output directly.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Callable[[StepContext[DepsT]], str] | str</code> <p>Either a static string or a function that builds    the prompt from the step context.</p> required <p>Returns:</p> Type Description <code>AgentStepWrapper[DepsT, OutputT]</code> <p>A BaseStep that can be used in a Pipeline.</p> <p>Examples:</p> <pre><code># Static prompt\nagent = FastroAgent(model=\"gpt-4o\", system_prompt=\"Summarize text.\")\nstep = agent.as_step(\"Summarize the document.\")\n\n# Dynamic prompt from context\nagent = FastroAgent(model=\"gpt-4o\", system_prompt=\"Summarize text.\")\nstep = agent.as_step(lambda ctx: f\"Summarize: {ctx.get_input('doc')}\")\n\n# With structured output\nagent = FastroAgent(model=\"gpt-4o\", output_type=Summary)\nstep = agent.as_step(lambda ctx: f\"Summarize: {ctx.get_input('doc')}\")\n# step returns Summary directly\n\n# Use in pipeline\npipeline = Pipeline(\n    name=\"summarizer\",\n    steps={\"summarize\": step},\n)\n</code></pre>"},{"location":"api/agent/#agentconfig","title":"AgentConfig","text":""},{"location":"api/agent/#fastroai.agent.AgentConfig","title":"<code>fastroai.agent.AgentConfig</code>","text":"<p>Configuration for FastroAgent instances.</p> <p>All parameters have sensible defaults. Override as needed.</p> <p>Examples:</p> <pre><code># Minimal - uses all defaults\nconfig = AgentConfig()\n\n# Custom configuration\nconfig = AgentConfig(\n    model=\"anthropic:claude-3-5-sonnet\",\n    system_prompt=\"You are a financial advisor.\",\n    temperature=0.3,\n)\n\n# Use with agent\nagent = FastroAgent(config=config)\n\n# Or pass kwargs directly to FastroAgent\nagent = FastroAgent(model=\"openai:gpt-4o-mini\", temperature=0.5)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.AgentConfig.max_retries","title":"<code>max_retries = Field(default=DEFAULT_MAX_RETRIES, ge=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum retry attempts on failure.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.max_tokens","title":"<code>max_tokens = DEFAULT_MAX_TOKENS</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum tokens in response.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.model","title":"<code>model = DEFAULT_MODEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'openai:gpt-4o', 'anthropic:claude-3-5-sonnet').</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.system_prompt","title":"<code>system_prompt = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>System prompt. If None, uses DEFAULT_SYSTEM_PROMPT.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.temperature","title":"<code>temperature = Field(default=DEFAULT_TEMPERATURE, ge=0.0, le=2.0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Sampling temperature (0.0 = deterministic, 2.0 = creative).</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.timeout_seconds","title":"<code>timeout_seconds = Field(default=DEFAULT_TIMEOUT_SECONDS, gt=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Request timeout in seconds.</p>"},{"location":"api/agent/#fastroai.agent.AgentConfig.get_effective_system_prompt","title":"<code>get_effective_system_prompt()</code>","text":"<p>Get system prompt, using default if not set.</p> <p>Returns:</p> Type Description <code>str</code> <p>The configured system prompt or DEFAULT_SYSTEM_PROMPT.</p>"},{"location":"api/agent/#chatresponse","title":"ChatResponse","text":""},{"location":"api/agent/#fastroai.agent.ChatResponse","title":"<code>fastroai.agent.ChatResponse</code>","text":"<p>Response from an AI agent interaction.</p> <p>Contains the response content plus comprehensive usage metrics for billing, analytics, and debugging.</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>OutputT</code> <p>The typed output from the agent. For string agents, same as content.</p> <code>content</code> <code>str</code> <p>String representation of the output.</p> <code>model</code> <code>str</code> <p>Model that generated the response.</p> <code>input_tokens</code> <code>int</code> <p>Tokens consumed by input/prompt.</p> <code>output_tokens</code> <code>int</code> <p>Tokens in response/completion.</p> <code>total_tokens</code> <code>int</code> <p>input_tokens + output_tokens.</p> <code>tool_calls</code> <code>list[dict[str, Any]]</code> <p>Tools invoked during generation.</p> <code>cost_microcents</code> <code>int</code> <p>Cost in 1/10,000ths of a cent (integer).</p> <code>processing_time_ms</code> <code>int</code> <p>Wall-clock time in milliseconds.</p> <code>trace_id</code> <code>str | None</code> <p>Distributed tracing correlation ID.</p> <p>Examples:</p> <pre><code>response = await agent.run(\"What is 2+2?\")\n\nprint(f\"Answer: {response.content}\")\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\nprint(f\"Tokens: {response.total_tokens}\")\n\nif response.tool_calls:\n    for call in response.tool_calls:\n        print(f\"Used tool: {call['tool_name']}\")\n\n# With structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    value: int\n    explanation: str\n\nagent = FastroAgent(output_type=Answer)\nresponse = await agent.run(\"What is 2+2?\")\nprint(response.output.value)  # 4\nprint(response.output.explanation)  # \"2 plus 2 equals 4\"\n</code></pre> Note <p>Why microcents? Floating-point math has precision errors:</p> <p>0.1 + 0.2 0.30000000000000004</p> <p>With microcents (integers), precision is exact:</p> <p>100 + 200 300</p> <p>For billing systems, this matters.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.content","title":"<code>content</code>  <code>instance-attribute</code>","text":"<p>String representation of the output.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.cost_dollars","title":"<code>cost_dollars</code>  <code>property</code>","text":"<p>Cost in dollars for display purposes.</p> <p>Returns:</p> Type Description <code>float</code> <p>Cost as a float in dollars.</p> Note <p>Use cost_microcents for calculations to avoid floating-point errors.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.cost_microcents","title":"<code>cost_microcents</code>  <code>instance-attribute</code>","text":"<p>Cost in microcents (1/1,000,000 dollar).</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.input_tokens","title":"<code>input_tokens</code>  <code>instance-attribute</code>","text":"<p>Tokens consumed by input/prompt.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model that generated the response.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.output","title":"<code>output</code>  <code>instance-attribute</code>","text":"<p>The typed output from the agent.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.output_tokens","title":"<code>output_tokens</code>  <code>instance-attribute</code>","text":"<p>Tokens in response/completion.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.processing_time_ms","title":"<code>processing_time_ms</code>  <code>instance-attribute</code>","text":"<p>Wall-clock processing time in milliseconds.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.tool_calls","title":"<code>tool_calls = []</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tools invoked during generation.</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.total_tokens","title":"<code>total_tokens</code>  <code>instance-attribute</code>","text":"<p>Total tokens (input + output).</p>"},{"location":"api/agent/#fastroai.agent.ChatResponse.trace_id","title":"<code>trace_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Distributed tracing correlation ID.</p>"},{"location":"api/agent/#streamchunk","title":"StreamChunk","text":""},{"location":"api/agent/#fastroai.agent.StreamChunk","title":"<code>fastroai.agent.StreamChunk</code>","text":"<p>A chunk in a streaming response.</p> <p>Most chunks have content with is_final=False. The last chunk has is_final=True with complete usage data.</p> <p>Examples:</p> <pre><code>async for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nTotal cost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre>"},{"location":"api/agent/#fastroai.agent.StreamChunk.content","title":"<code>content = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Text content of this chunk.</p>"},{"location":"api/agent/#fastroai.agent.StreamChunk.is_final","title":"<code>is_final = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>True if this is the final chunk with usage data.</p>"},{"location":"api/agent/#fastroai.agent.StreamChunk.usage_data","title":"<code>usage_data = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Complete usage data (only on final chunk).</p>"},{"location":"api/agent/#agentstepwrapper","title":"AgentStepWrapper","text":"<p>\u2190 API Overview Pipelines \u2192</p>"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper","title":"<code>fastroai.agent.AgentStepWrapper</code>","text":"<p>Pipeline step wrapper for FastroAgent.</p> <p>Created via FastroAgent.as_step(). Wraps an agent as a pipeline step.</p> <p>The wrapper uses ctx.run() for automatic tracer/deps forwarding and usage tracking, and returns the agent's typed output directly.</p> Note <p>Use FastroAgent.as_step() to create instances rather than instantiating directly.</p>"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper.agent","title":"<code>agent</code>  <code>property</code>","text":"<p>Access the underlying FastroAgent.</p> <p>Returns:</p> Type Description <code>FastroAgent[OutputT]</code> <p>The wrapped FastroAgent instance.</p>"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper.__init__","title":"<code>__init__(agent, prompt)</code>","text":"<p>Initialize the step wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>FastroAgent[OutputT]</code> <p>The FastroAgent to wrap.</p> required <code>prompt</code> <code>Callable[[StepContext[DepsT]], str] | str</code> <p>Static string or function that builds the prompt from context.</p> required"},{"location":"api/agent/#fastroai.agent.AgentStepWrapper.execute","title":"<code>execute(context)</code>  <code>async</code>","text":"<p>Execute the agent with the configured prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StepContext[DepsT]</code> <p>Step execution context with inputs, deps, and config.</p> required <p>Returns:</p> Type Description <code>OutputT</code> <p>The agent's typed output.</p>"},{"location":"api/pipelines/","title":"Pipelines","text":"<p>The pipelines module provides DAG-based workflow orchestration with automatic parallelism, multi-turn conversation support, and usage tracking.</p>"},{"location":"api/pipelines/#pipeline","title":"Pipeline","text":""},{"location":"api/pipelines/#fastroai.pipelines.Pipeline","title":"<code>fastroai.pipelines.Pipeline</code>","text":"<p>Declarative DAG pipeline for multi-step AI workflows.</p> <p>Features: - Automatic parallelism from dependencies - Type-safe dependency access - Early termination on INCOMPLETE status - Aggregated usage tracking - Distributed tracing</p> <p>Examples:</p> <pre><code>pipeline = Pipeline(\n    name=\"document_processor\",\n    steps={\n        \"extract\": ExtractStep(),\n        \"classify\": ClassifyStep(),\n        \"summarize\": SummarizeStep(),\n    },\n    dependencies={\n        \"classify\": [\"extract\"],\n        \"summarize\": [\"classify\"],\n    },\n)\n\nresult = await pipeline.execute({\"document\": doc}, deps, tracer)\nsummary = result.output\n</code></pre> <p>Parallelism example - steps at the same level run in parallel:</p> <pre><code>dependencies = {\n    \"classify\": [\"extract\"],\n    \"fetch_market\": [\"classify\"],\n    \"fetch_user\": [\"classify\"],  # Same dep as above\n    \"calculate\": [\"fetch_market\", \"fetch_user\"],\n}\n# Execution:\n# Level 0: extract\n# Level 1: classify\n# Level 2: fetch_market, fetch_user (PARALLEL)\n# Level 3: calculate\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.Pipeline.__init__","title":"<code>__init__(name, steps, dependencies=None, output_step=None, config=None, step_configs=None)</code>","text":"<p>Initialize Pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name (for tracing).</p> required <code>steps</code> <code>dict[str, BaseStep[DepsT, Any]]</code> <p>Dict of step_id -&gt; step instance.</p> required <code>dependencies</code> <code>dict[str, list[str]] | None</code> <p>Dict of step_id -&gt; [dependency_ids].</p> <code>None</code> <code>output_step</code> <code>str | None</code> <p>Which step's output is the pipeline output.         Defaults to last step in topological order.</p> <code>None</code> <code>config</code> <code>PipelineConfig | None</code> <p>Default configuration for all steps (timeout, retries, budget).</p> <code>None</code> <code>step_configs</code> <code>dict[str, StepConfig] | None</code> <p>Per-step configuration overrides.</p> <code>None</code> <p>Config Resolution (most specific wins):     1. Pipeline default config     2. Step class config (if step has .config attribute)     3. step_configs[step_id] override     4. Per-call overrides via ctx.run(timeout=..., retries=...)</p> <p>Raises:</p> Type Description <code>PipelineValidationError</code> <p>Invalid deps, unknown output_step, or cycles.</p> <p>Examples:</p> <pre><code>pipeline = Pipeline(\n    name=\"processor\",\n    steps={\"extract\": ExtractStep(), \"classify\": ClassifyStep()},\n    config=PipelineConfig(timeout=30.0, retries=1),\n    step_configs={\"classify\": StepConfig(timeout=60.0)},  # Override\n)\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.Pipeline.execute","title":"<code>execute(input_data, deps, tracer=None)</code>  <code>async</code>","text":"<p>Execute the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>InputT</code> <p>Input accessible via context.get_input().</p> required <code>deps</code> <code>DepsT</code> <p>Your deps accessible via context.deps.</p> required <code>tracer</code> <code>Tracer | None</code> <p>For distributed tracing.</p> <code>None</code> <p>Returns:</p> Type Description <code>PipelineResult[OutputT]</code> <p>PipelineResult with output and usage.</p> <p>Raises:</p> Type Description <code>StepExecutionError</code> <p>If any step fails.</p> <p>Examples:</p> <pre><code># Basic execution\nresult = await pipeline.execute(\n    {\"document\": \"Hello world\"},\n    deps=MyDeps(api_key=\"...\"),\n)\nprint(result.output)\n\n# With tracing\nfrom fastroai import SimpleTracer\n\ntracer = SimpleTracer()\nresult = await pipeline.execute(\n    {\"document\": doc},\n    deps=deps,\n    tracer=tracer,\n)\n\n# Handle early termination (multi-turn)\nif result.stopped_early:\n    missing = result.conversation_state.context[\"missing\"]\n    return {\"status\": \"incomplete\", \"missing\": missing}\n\n# Access usage metrics\nif result.usage:\n    print(f\"Cost: ${result.usage.total_cost_dollars:.6f}\")\n    print(f\"Tokens: {result.usage.total_input_tokens}\")\n</code></pre>"},{"location":"api/pipelines/#pipelineresult","title":"PipelineResult","text":""},{"location":"api/pipelines/#fastroai.pipelines.PipelineResult","title":"<code>fastroai.pipelines.PipelineResult</code>","text":"<p>Result from pipeline execution.</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>OutputT | None</code> <p>Final step's output, or None if stopped early.</p> <code>step_outputs</code> <code>dict[str, Any]</code> <p>All step outputs by ID.</p> <code>conversation_state</code> <code>ConversationState[Any] | None</code> <p>ConversationState if a step returned one.</p> <code>usage</code> <code>PipelineUsage | None</code> <p>Aggregated usage metrics.</p> <code>stopped_early</code> <code>bool</code> <p>True if stopped due to INCOMPLETE status.</p> <p>Examples:</p> <pre><code>result = await pipeline.execute(data, deps)\n\nif result.stopped_early:\n    missing = result.conversation_state.context[\"missing\"]\n    return {\"status\": \"incomplete\", \"missing\": missing}\n\nprint(f\"Cost: ${result.usage.total_cost_dollars:.6f}\")\nreturn {\"status\": \"complete\", \"output\": result.output}\n</code></pre>"},{"location":"api/pipelines/#basestep","title":"BaseStep","text":""},{"location":"api/pipelines/#fastroai.pipelines.BaseStep","title":"<code>fastroai.pipelines.BaseStep</code>","text":"<p>Abstract base class for pipeline steps.</p> <p>A step is one unit of work. It: - Receives context with inputs and dependencies - Does something (AI call, computation, API call) - Returns typed output</p> <p>Steps should be stateless. Any state goes in deps or inputs.</p> <p>Examples:</p> <pre><code>class ExtractStep(BaseStep[MyDeps, ExtractionResult]):\n    '''Extract entities from document.'''\n\n    def __init__(self):\n        self.agent = FastroAgent(system_prompt=\"Extract entities.\")\n\n    async def execute(self, context: StepContext[MyDeps]) -&gt; ExtractionResult:\n        document = context.get_input(\"document\")\n        response = await self.agent.run(f\"Extract: {document}\")\n        return ExtractionResult.model_validate_json(response.content)\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.BaseStep.execute","title":"<code>execute(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute step logic.</p> <p>Override this method to implement your step's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>StepContext[DepsT]</code> <p>Execution context with inputs, deps, and step outputs.</p> required <p>Returns:</p> Type Description <code>OutputT</code> <p>The step's typed output.</p>"},{"location":"api/pipelines/#stepcontext","title":"StepContext","text":""},{"location":"api/pipelines/#fastroai.pipelines.StepContext","title":"<code>fastroai.pipelines.StepContext</code>","text":"<p>Execution context provided to pipeline steps.</p> <p>Provides access to: - Pipeline inputs (the data passed to execute()) - Outputs from dependency steps - Application dependencies (your db session, user, etc.) - Tracer for custom spans</p> <p>Examples:</p> <pre><code>class ProcessStep(BaseStep[MyDeps, Result]):\n    async def execute(self, context: StepContext[MyDeps]) -&gt; Result:\n        # Get pipeline input\n        document = context.get_input(\"document\")\n\n        # Get output from dependency step\n        classification = context.get_dependency(\"classify\", Classification)\n\n        # Access your deps\n        db = context.deps.session\n        user_id = context.deps.user_id\n\n        # Custom tracing\n        if context.tracer:\n            async with context.tracer.span(\"custom_operation\"):\n                result = await process(document)\n\n        return result\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.step_id","title":"<code>step_id</code>  <code>property</code>","text":"<p>Current step's ID.</p> <p>Returns:</p> Type Description <code>str</code> <p>The step identifier string.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.deps","title":"<code>deps</code>  <code>property</code>","text":"<p>Application dependencies (your session, user, etc.).</p> <p>Returns:</p> Type Description <code>DepsT</code> <p>The dependencies object passed to pipeline.execute().</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.tracer","title":"<code>tracer</code>  <code>property</code>","text":"<p>Tracer for custom spans.</p> <p>Returns:</p> Type Description <code>Tracer | None</code> <p>The tracer instance, or None if no tracing.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.usage","title":"<code>usage</code>  <code>property</code>","text":"<p>Accumulated usage from all ctx.run() calls in this step.</p> <p>Returns:</p> Type Description <code>StepUsage</code> <p>StepUsage with aggregated tokens and cost.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.config","title":"<code>config</code>  <code>property</code>","text":"<p>Configuration for this step (timeout, retries, budget).</p> <p>Returns:</p> Type Description <code>StepConfig</code> <p>The resolved StepConfig for this step.</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.__init__","title":"<code>__init__(step_id, inputs, deps, step_outputs, tracer=None, config=None)</code>","text":"<p>Initialize step context.</p> <p>Parameters:</p> Name Type Description Default <code>step_id</code> <code>str</code> <p>Unique identifier for this step.</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Pipeline inputs passed to execute().</p> required <code>deps</code> <code>DepsT</code> <p>Application dependencies (db session, user, etc.).</p> required <code>step_outputs</code> <code>dict[str, Any]</code> <p>Outputs from completed dependency steps.</p> required <code>tracer</code> <code>Tracer | None</code> <p>Optional tracer for distributed tracing.</p> <code>None</code> <code>config</code> <code>StepConfig | None</code> <p>Step configuration (timeout, retries, budget).</p> <code>None</code>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.get_input","title":"<code>get_input(key, default=None)</code>","text":"<p>Get value from pipeline inputs.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The input key to retrieve.</p> required <code>default</code> <code>Any</code> <p>Value to return if key not found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The input value, or default if not present.</p> <p>Examples:</p> <pre><code>class ProcessStep(BaseStep[MyDeps, str]):\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        # Get required input\n        document = ctx.get_input(\"document\")\n\n        # Get optional input with default\n        format_type = ctx.get_input(\"format\", \"json\")\n\n        return f\"Processing {document} as {format_type}\"\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.get_dependency","title":"<code>get_dependency(step_id, output_type=None)</code>","text":"<p>Get output from a dependency step.</p> <p>Parameters:</p> Name Type Description Default <code>step_id</code> <code>str</code> <p>ID of the dependency step.</p> required <code>output_type</code> <code>type[T] | None</code> <p>Expected type (for IDE/type checker, not enforced).</p> <code>None</code> <p>Returns:</p> Type Description <code>T</code> <p>The output from the dependency step.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If step_id not in dependencies or hasn't run.</p> <p>Examples:</p> <pre><code># With type hint (IDE knows extraction is ExtractionResult)\nextraction = context.get_dependency(\"extract\", ExtractionResult)\nextraction.entities  # Autocomplete works!\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.get_dependency_or_none","title":"<code>get_dependency_or_none(step_id, output_type=None)</code>","text":"<p>Get output from a dependency step, or None if not available.</p> <p>Use this for optional dependencies that may not have run.</p> <p>Parameters:</p> Name Type Description Default <code>step_id</code> <code>str</code> <p>ID of the dependency step.</p> required <code>output_type</code> <code>type[T] | None</code> <p>Expected type (for IDE/type checker, not enforced).</p> <code>None</code> <p>Returns:</p> Type Description <code>T | None</code> <p>The output from the dependency step, or None if not available.</p> <p>Examples:</p> <pre><code>class EnhanceStep(BaseStep[MyDeps, str]):\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        # Required dependency\n        base_content = ctx.get_dependency(\"extract\")\n\n        # Optional dependency - might not exist\n        metadata = ctx.get_dependency_or_none(\"fetch_metadata\", dict)\n\n        if metadata:\n            return f\"{base_content} (with metadata)\"\n        return base_content\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepContext.run","title":"<code>run(agent, message, *, timeout=None, retries=None)</code>  <code>async</code>","text":"<p>Run an agent with automatic tracer, usage tracking, and config.</p> <p>This is THE way to call agents from within a step. It: - Passes deps and tracer automatically - Accumulates usage in ctx.usage - Enforces cost budget (raises CostBudgetExceededError if exceeded) - Supports timeout and retries (from config or per-call override)</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>FastroAgent[OutputT]</code> <p>The FastroAgent to run.</p> required <code>message</code> <code>str</code> <p>The message/prompt to send.</p> required <code>timeout</code> <code>float | None</code> <p>Per-call timeout override (seconds). Uses config if None.</p> <code>None</code> <code>retries</code> <code>int | None</code> <p>Per-call retries override. Uses config if None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatResponse[OutputT]</code> <p>ChatResponse with output, content, usage data, etc.</p> <p>Raises:</p> Type Description <code>CostBudgetExceededError</code> <p>If cost_budget is set and exceeded.</p> <code>TimeoutError</code> <p>If timeout exceeded after all retries.</p> <p>Examples:</p> <pre><code>class MyStep(BaseStep[MyDeps, str]):\n    classifier = FastroAgent(model=\"gpt-4o-mini\", output_type=Category)\n    writer = FastroAgent(model=\"gpt-4o\")\n\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        # Both calls tracked in ctx.usage\n        category = await ctx.run(self.classifier, \"Classify this\")\n        result = await ctx.run(self.writer, f\"Write about {category.output}\")\n        return result.content\n\n# With per-call overrides:\nresponse = await ctx.run(agent, \"msg\", timeout=30.0, retries=2)\n</code></pre>"},{"location":"api/pipelines/#step","title":"step","text":""},{"location":"api/pipelines/#fastroai.pipelines.step","title":"<code>fastroai.pipelines.step(func=None, *, timeout=None, retries=0, retry_delay=1.0, cost_budget=None)</code>","text":"<pre><code>step(func: Callable[..., OutputT]) -&gt; _FunctionStep\n</code></pre><pre><code>step(\n    func: None = None,\n    *,\n    timeout: float | None = None,\n    retries: int = 0,\n    retry_delay: float = 1.0,\n    cost_budget: int | None = None,\n) -&gt; Callable[[Callable[..., OutputT]], _FunctionStep]\n</code></pre> <p>Decorator to create a pipeline step from a function.</p> <p>Can be used with or without arguments:</p> <pre><code>@step\nasync def my_step(ctx): ...\n\n@step(timeout=30.0, retries=2)\nasync def my_step(ctx): ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any] | None</code> <p>The function to wrap (when used without parentheses).</p> <code>None</code> <code>timeout</code> <code>float | None</code> <p>Maximum execution time in seconds.</p> <code>None</code> <code>retries</code> <code>int</code> <p>Number of retry attempts on failure.</p> <code>0</code> <code>retry_delay</code> <code>float</code> <p>Base delay between retries (exponential backoff).</p> <code>1.0</code> <code>cost_budget</code> <code>int | None</code> <p>Maximum cost in microcents for this step.</p> <code>None</code> <p>Returns:</p> Type Description <code>_FunctionStep | Callable[[Callable[..., Any]], _FunctionStep]</code> <p>A BaseStep instance that can be used in a Pipeline.</p>"},{"location":"api/pipelines/#configuration","title":"Configuration","text":""},{"location":"api/pipelines/#stepconfig","title":"StepConfig","text":""},{"location":"api/pipelines/#fastroai.pipelines.StepConfig","title":"<code>fastroai.pipelines.StepConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a pipeline step.</p> <p>Attributes:</p> Name Type Description <code>timeout</code> <code>float | None</code> <p>Maximum time in seconds for step execution. None = no timeout.</p> <code>retries</code> <code>int</code> <p>Number of retry attempts on failure. 0 = no retries.</p> <code>retry_delay</code> <code>float</code> <p>Delay in seconds between retry attempts.</p> <code>cost_budget</code> <code>int | None</code> <p>Maximum cost in microcents. None = no budget limit.</p> <p>Examples:</p> <pre><code># Step with 30s timeout and 2 retries\nconfig = StepConfig(timeout=30.0, retries=2)\n\n# Step with cost budget of $0.10 (10 cents = 100_000 microcents)\nconfig = StepConfig(cost_budget=100_000)\n</code></pre>"},{"location":"api/pipelines/#pipelineconfig","title":"PipelineConfig","text":""},{"location":"api/pipelines/#fastroai.pipelines.PipelineConfig","title":"<code>fastroai.pipelines.PipelineConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a pipeline with additional options.</p> <p>Inherits all StepConfig fields plus pipeline-specific options.</p> <p>Attributes:</p> Name Type Description <code>trace</code> <code>bool</code> <p>Whether to enable tracing for this pipeline.</p> <code>on_error</code> <code>Literal['fail', 'continue']</code> <p>Error handling strategy: - \"fail\": Stop pipeline on first error (default) - \"continue\": Continue executing other steps on error</p> <p>Examples:</p> <pre><code># Pipeline with tracing and 60s timeout\nconfig = PipelineConfig(trace=True, timeout=60.0)\n\n# Pipeline that continues on errors\nconfig = PipelineConfig(on_error=\"continue\")\n</code></pre>"},{"location":"api/pipelines/#conversation-state","title":"Conversation State","text":""},{"location":"api/pipelines/#conversationstatus","title":"ConversationStatus","text":""},{"location":"api/pipelines/#fastroai.pipelines.ConversationStatus","title":"<code>fastroai.pipelines.ConversationStatus</code>","text":"<p>Status of multi-turn conversation gathering.</p> <p>Attributes:</p> Name Type Description <code>COMPLETE</code> <p>All required information has been gathered. The pipeline proceeds to subsequent steps.</p> <code>INCOMPLETE</code> <p>More information is needed from the user. The pipeline pauses and returns partial state.</p>"},{"location":"api/pipelines/#conversationstate","title":"ConversationState","text":""},{"location":"api/pipelines/#fastroai.pipelines.ConversationState","title":"<code>fastroai.pipelines.ConversationState</code>","text":"<p>Signal for multi-turn conversation steps.</p> <p>When a step returns ConversationState with INCOMPLETE status, the pipeline stops early. Partial data and context are preserved.</p> <p>Examples:</p> <pre><code>class GatherInfoStep(BaseStep[MyDeps, ConversationState[UserInfo]]):\n    async def execute(self, context) -&gt; ConversationState[UserInfo]:\n        info = await self._extract(context.get_input(\"message\"))\n\n        if info.is_complete():\n            return ConversationState(\n                status=ConversationStatus.COMPLETE,\n                data=info,\n            )\n\n        return ConversationState(\n            status=ConversationStatus.INCOMPLETE,\n            data=info,  # Partial data\n            context={\"missing\": info.missing_fields()},\n        )\n</code></pre>"},{"location":"api/pipelines/#usage-tracking","title":"Usage Tracking","text":""},{"location":"api/pipelines/#stepusage","title":"StepUsage","text":""},{"location":"api/pipelines/#fastroai.pipelines.StepUsage","title":"<code>fastroai.pipelines.StepUsage</code>","text":"<p>Usage metrics for a single pipeline step.</p> <p>Automatically extracted from ChatResponse when using AgentStep.</p> <p>Examples:</p> <pre><code># From ChatResponse\nusage = StepUsage.from_chat_response(response)\n\n# Manual creation\nusage = StepUsage(\n    input_tokens=100,\n    output_tokens=50,\n    cost_microcents=175,\n    processing_time_ms=500,\n    model=\"gpt-4o\",\n)\n\n# Combine usages\ntotal = usage1 + usage2\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepUsage.__add__","title":"<code>__add__(other)</code>","text":"<p>Combine two StepUsage instances.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>StepUsage</code> <p>Another StepUsage to add.</p> required <p>Returns:</p> Type Description <code>StepUsage</code> <p>New StepUsage with summed metrics.</p> <p>Examples:</p> <pre><code>usage1 = StepUsage(input_tokens=100, cost_microcents=50)\nusage2 = StepUsage(input_tokens=200, cost_microcents=100)\n\ntotal = usage1 + usage2\nprint(total.input_tokens)  # 300\nprint(total.cost_microcents)  # 150\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.StepUsage.from_chat_response","title":"<code>from_chat_response(response)</code>  <code>classmethod</code>","text":"<p>Create StepUsage from a ChatResponse.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatResponse[Any]</code> <p>ChatResponse from an agent run.</p> required <p>Returns:</p> Type Description <code>StepUsage</code> <p>StepUsage with metrics extracted from the response.</p> <p>Examples:</p> <pre><code>response = await agent.run(\"Hello\")\nusage = StepUsage.from_chat_response(response)\n\nprint(f\"Tokens: {usage.input_tokens} in, {usage.output_tokens} out\")\nprint(f\"Cost: {usage.cost_microcents} microcents\")\n</code></pre>"},{"location":"api/pipelines/#pipelineusage","title":"PipelineUsage","text":""},{"location":"api/pipelines/#fastroai.pipelines.PipelineUsage","title":"<code>fastroai.pipelines.PipelineUsage</code>","text":"<p>Aggregated usage across all pipeline steps.</p> <p>Examples:</p> <pre><code># From step usages\nusage = PipelineUsage.from_step_usages({\n    \"extract\": StepUsage(cost_microcents=100, ...),\n    \"classify\": StepUsage(cost_microcents=200, ...),\n})\n\nprint(f\"Total cost: ${usage.total_cost_dollars:.6f}\")\nprint(f\"Steps: {list(usage.steps.keys())}\")\n</code></pre>"},{"location":"api/pipelines/#fastroai.pipelines.PipelineUsage.total_cost_dollars","title":"<code>total_cost_dollars</code>  <code>property</code>","text":"<p>Total cost in dollars for display purposes.</p> <p>Returns:</p> Type Description <code>float</code> <p>Total cost as a float in dollars.</p> Note <p>Use total_cost_microcents for calculations to avoid floating-point errors.</p>"},{"location":"api/pipelines/#fastroai.pipelines.PipelineUsage.from_step_usages","title":"<code>from_step_usages(step_usages)</code>  <code>classmethod</code>","text":"<p>Aggregate metrics from individual step usages.</p> <p>Parameters:</p> Name Type Description Default <code>step_usages</code> <code>dict[str, StepUsage]</code> <p>Dict mapping step IDs to their usage metrics.</p> required <p>Returns:</p> Type Description <code>PipelineUsage</code> <p>PipelineUsage with summed totals and per-step breakdown.</p> <p>Examples:</p> <pre><code>step_usages = {\n    \"extract\": StepUsage(input_tokens=100, cost_microcents=50),\n    \"classify\": StepUsage(input_tokens=200, cost_microcents=100),\n}\n\nusage = PipelineUsage.from_step_usages(step_usages)\nprint(f\"Total tokens: {usage.total_input_tokens}\")  # 300\nprint(f\"Total cost: ${usage.total_cost_dollars:.6f}\")\n\n# Access per-step breakdown\nfor step_id, step_usage in usage.steps.items():\n    print(f\"  {step_id}: {step_usage.cost_microcents} microcents\")\n</code></pre>"},{"location":"api/pipelines/#errors","title":"Errors","text":""},{"location":"api/pipelines/#stepexecutionerror","title":"StepExecutionError","text":"<p>\u2190 Agent Tools \u2192</p>"},{"location":"api/pipelines/#fastroai.pipelines.StepExecutionError","title":"<code>fastroai.pipelines.StepExecutionError</code>","text":"<p>Raised when a pipeline step fails during execution.</p> <p>Attributes:</p> Name Type Description <code>step_id</code> <p>The ID of the step that failed.</p> <code>original_error</code> <p>The underlying exception that caused the failure.</p> <p>Examples:</p> <pre><code>try:\n    result = await pipeline.execute(inputs, deps)\nexcept StepExecutionError as e:\n    print(f\"Step '{e.step_id}' failed: {e.original_error}\")\n</code></pre>"},{"location":"api/tools/","title":"Tools","text":"<p>The tools module provides production-safe tool decorators and toolset base classes for organizing AI agent capabilities.</p>"},{"location":"api/tools/#safe_tool","title":"safe_tool","text":""},{"location":"api/tools/#fastroai.tools.safe_tool","title":"<code>fastroai.tools.safe_tool(timeout=DEFAULT_TOOL_TIMEOUT, max_retries=DEFAULT_TOOL_MAX_RETRIES, on_timeout=None, on_error=None)</code>","text":"<p>Decorator that adds timeout, retry, and error handling to AI tools.</p> <p>When a tool times out or raises an exception, instead of crashing the conversation, this decorator returns an error message that the AI can use to respond gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Maximum seconds per attempt. Default: 30.</p> <code>DEFAULT_TOOL_TIMEOUT</code> <code>max_retries</code> <code>int</code> <p>Maximum retry attempts. Default: 3.</p> <code>DEFAULT_TOOL_MAX_RETRIES</code> <code>on_timeout</code> <code>str | None</code> <p>Custom message returned on timeout.        Default: \"Tool timed out after {max_retries} attempts\"</p> <code>None</code> <code>on_error</code> <code>str | None</code> <p>Custom message returned on error.      Use {error} placeholder for error details.      Default: \"Tool failed: {error}\"</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable[[Callable[P, Awaitable[R]]], Callable[P, Awaitable[R | str]]]</code> <p>Decorated async function with safety features.</p> <p>Examples:</p> <pre><code>@safe_tool(timeout=10, max_retries=2)\nasync def web_search(query: str) -&gt; str:\n    '''Search the web for information.'''\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.example.com?q={query}\")\n        return response.text\n\n# If the API is slow or down:\n# - Waits max 10 seconds per attempt\n# - Retries up to 2 times with exponential backoff\n# - Returns error message on final failure\n# - AI sees: \"Tool timed out after 2 attempts\"\n\n# With custom messages:\n@safe_tool(\n    timeout=30,\n    on_timeout=\"Search is taking too long. Try a simpler query.\",\n    on_error=\"Search unavailable: {error}\",\n)\nasync def search(query: str) -&gt; str:\n    ...\n</code></pre>"},{"location":"api/tools/#safetoolset","title":"SafeToolset","text":""},{"location":"api/tools/#fastroai.tools.SafeToolset","title":"<code>fastroai.tools.SafeToolset</code>","text":"<p>Base class for toolsets containing only safe tools.</p> <p>Safe tools are those that: - Don't access external networks (or have timeout protection) - Don't modify system state - Have bounded execution time - Return graceful error messages instead of raising exceptions</p> <p>Use this as a base class to mark toolsets as production-safe.</p> <p>Examples:</p> <pre><code>@safe_tool(timeout=5)\nasync def calculator(expression: str) -&gt; str:\n    '''Evaluate a math expression.'''\n    try:\n        # Safe: no network, no state modification\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return str(result)\n    except Exception as e:\n        return f\"Error: {e}\"\n\n@safe_tool(timeout=1)\nasync def get_time() -&gt; str:\n    '''Get current time.'''\n    from datetime import datetime\n    return datetime.now().isoformat()\n\nclass UtilityToolset(SafeToolset):\n    def __init__(self):\n        super().__init__(\n            tools=[calculator, get_time],\n            name=\"utilities\",\n        )\n</code></pre>"},{"location":"api/tools/#functiontoolsetbase","title":"FunctionToolsetBase","text":"<p>\u2190 Pipelines Usage \u2192</p>"},{"location":"api/tools/#fastroai.tools.FunctionToolsetBase","title":"<code>fastroai.tools.FunctionToolsetBase</code>","text":"<p>Base class for organized tool sets.</p> <p>Extends PydanticAI's FunctionToolset with a name for identification and organization purposes.</p> <p>Examples:</p> <pre><code>from fastroai.tools import safe_tool, FunctionToolsetBase\n\n@safe_tool(timeout=30)\nasync def web_search(query: str) -&gt; str:\n    '''Search the web.'''\n    ...\n\n@safe_tool(timeout=10)\nasync def get_weather(location: str) -&gt; str:\n    '''Get weather for location.'''\n    ...\n\nclass WebToolset(FunctionToolsetBase):\n    def __init__(self):\n        super().__init__(\n            tools=[web_search, get_weather],\n            name=\"web\",\n        )\n\n# Use with FastroAgent\nagent = FastroAgent(toolsets=[WebToolset()])\n</code></pre>"},{"location":"api/tools/#fastroai.tools.FunctionToolsetBase.__init__","title":"<code>__init__(tools, name=None)</code>","text":"<p>Initialize toolset with tools and optional name.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[Callable[..., Any]]</code> <p>List of tool functions to include.</p> required <code>name</code> <code>str | None</code> <p>Name for this toolset. Defaults to class name.</p> <code>None</code>"},{"location":"api/tracing/","title":"Tracing","text":"<p>The tracing module provides a protocol-based interface for distributed tracing integration. Implement the <code>Tracer</code> protocol to connect FastroAI with your observability platform, or use one of the built-in tracers.</p>"},{"location":"api/tracing/#tracer","title":"Tracer","text":""},{"location":"api/tracing/#fastroai.tracing.Tracer","title":"<code>fastroai.tracing.Tracer</code>","text":"<p>Protocol for distributed tracing implementations.</p> <p>Implement this protocol to integrate FastroAI with your preferred observability platform (OpenTelemetry, Datadog, etc.).</p> <p>FastroAI provides built-in implementations: - LogfireTracer: For Pydantic's Logfire platform - SimpleTracer: For logging-based tracing - NoOpTracer: For disabled tracing</p> <p>Examples:</p> <p>Using the built-in LogfireTracer: <pre><code>from fastroai import FastroAgent, LogfireTracer\n\ntracer = LogfireTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\", tracer=tracer)\n</code></pre></p> <p>Custom implementation for OpenTelemetry: <pre><code>from opentelemetry import trace as otel_trace\n\nclass OTelTracer:\n    def __init__(self):\n        self.tracer = otel_trace.get_tracer(\"fastroai\")\n\n    @asynccontextmanager\n    async def span(self, name: str, **attrs):\n        trace_id = str(uuid.uuid4())\n        with self.tracer.start_as_current_span(name) as span:\n            for key, value in attrs.items():\n                span.set_attribute(key, value)\n            yield trace_id\n\n    def log_metric(self, trace_id: str, name: str, value):\n        span = otel_trace.get_current_span()\n        span.set_attribute(f\"metric.{name}\", value)\n\n    def log_error(self, trace_id: str, error: Exception, context=None):\n        span = otel_trace.get_current_span()\n        span.record_exception(error)\n</code></pre></p>"},{"location":"api/tracing/#fastroai.tracing.Tracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>Log an error associated with a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID to associate the error with.</p> required <code>error</code> <code>Exception</code> <p>The exception that occurred.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional error context.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.Tracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>Log a metric associated with a trace.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID to associate the metric with.</p> required <code>name</code> <code>str</code> <p>Metric name.</p> required <code>value</code> <code>Any</code> <p>Metric value.</p> required"},{"location":"api/tracing/#fastroai.tracing.Tracer.span","title":"<code>span(name, **attributes)</code>","text":"<p>Create a traced span for an operation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the operation being traced.</p> required <code>**attributes</code> <code>Any</code> <p>Additional context to attach to the span.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AbstractAsyncContextManager[str]</code> <p>Async context manager that yields a unique trace ID.</p>"},{"location":"api/tracing/#simpletracer","title":"SimpleTracer","text":""},{"location":"api/tracing/#fastroai.tracing.SimpleTracer","title":"<code>fastroai.tracing.SimpleTracer</code>","text":"<p>Basic tracer implementation using Python's logging module.</p> <p>Provides simple tracing functionality for development and debugging. For production use, consider implementing a Tracer for your observability platform.</p> <p>Examples:</p> <pre><code>tracer = SimpleTracer()\n\nasync with tracer.span(\"my_operation\", user_id=\"123\") as trace_id:\n    # Your operation here\n    result = await do_something()\n    tracer.log_metric(trace_id, \"result_size\", len(result))\n\n# Logs:\n# INFO [abc12345] Starting my_operation\n# INFO [abc12345] Metric result_size=42\n# INFO [abc12345] Completed my_operation in 0.123s\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize SimpleTracer.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger | None</code> <p>Logger to use. Defaults to 'fastroai.tracing'.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>Log an error with trace correlation.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>error</code> <code>Exception</code> <p>The exception that occurred.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional error context.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>Log a metric with trace correlation.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>name</code> <code>str</code> <p>Metric name.</p> required <code>value</code> <code>Any</code> <p>Metric value.</p> required"},{"location":"api/tracing/#fastroai.tracing.SimpleTracer.span","title":"<code>span(name, **attributes)</code>  <code>async</code>","text":"<p>Create a traced span with timing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the operation.</p> required <code>**attributes</code> <code>Any</code> <p>Additional context logged with the span.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Unique trace ID (first 8 chars shown in logs for readability).</p>"},{"location":"api/tracing/#logfiretracer","title":"LogfireTracer","text":""},{"location":"api/tracing/#fastroai.tracing.LogfireTracer","title":"<code>fastroai.tracing.LogfireTracer</code>","text":"<p>Tracer implementation for Pydantic's Logfire observability platform.</p> <p>Integrates FastroAI with Logfire for production-grade observability, including distributed tracing, metrics, and error tracking. Requires the <code>logfire</code> package to be installed.</p> Note <p>Install logfire with: <code>pip install logfire</code> Configure logfire before use: <code>logfire.configure()</code></p> <p>Examples:</p> <pre><code>import logfire\nfrom fastroai import FastroAgent, LogfireTracer\n\n# Configure logfire (typically done once at startup)\nlogfire.configure()\n\ntracer = LogfireTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\", tracer=tracer)\n\n# View traces in Logfire dashboard at https://logfire.pydantic.dev\n</code></pre> <p>With pipelines: <pre><code>from fastroai import Pipeline, LogfireTracer\n\ntracer = LogfireTracer()\nresult = await pipeline.execute(\n    {\"document\": doc},\n    deps=my_deps,\n    tracer=tracer,\n)\n</code></pre></p>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize LogfireTracer.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the logfire package is not installed.</p>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>Log an error to Logfire with trace correlation.</p> <p>Records the error with full exception information for debugging in the Logfire dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>error</code> <code>Exception</code> <p>The exception that occurred.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional error context (e.g., step_id, operation).</p> <code>None</code> <p>Examples:</p> <pre><code>try:\n    result = await risky_operation()\nexcept Exception as e:\n    tracer.log_error(trace_id, e, {\"step\": \"data_processing\"})\n    raise\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>Log a metric to Logfire with trace correlation.</p> <p>Metrics are logged as info-level spans with the metric name and value as attributes, allowing them to be queried and visualized in Logfire.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Trace ID for correlation.</p> required <code>name</code> <code>str</code> <p>Metric name (e.g., \"input_tokens\", \"cost_microcents\").</p> required <code>value</code> <code>Any</code> <p>Metric value.</p> required <p>Examples:</p> <pre><code>tracer.log_metric(trace_id, \"input_tokens\", 150)\ntracer.log_metric(trace_id, \"cost_microcents\", 2500)\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.LogfireTracer.span","title":"<code>span(name, **attributes)</code>  <code>async</code>","text":"<p>Create a traced span using Logfire.</p> <p>Wraps Logfire's span context manager and generates a unique trace ID for correlation across FastroAI operations.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the operation being traced.</p> required <code>**attributes</code> <code>Any</code> <p>Additional context to attach to the span. These appear as attributes in the Logfire dashboard.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Unique trace ID for correlating metrics and errors.</p> <p>Examples:</p> <pre><code>async with tracer.span(\"my_operation\", user_id=\"123\") as trace_id:\n    result = await do_something()\n    tracer.log_metric(trace_id, \"result_size\", len(result))\n</code></pre>"},{"location":"api/tracing/#nooptracer","title":"NoOpTracer","text":"<p>\u2190 Usage API Overview \u2192</p>"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer","title":"<code>fastroai.tracing.NoOpTracer</code>","text":"<p>Tracer that does nothing. Use when tracing is disabled.</p> <p>This tracer satisfies the Tracer protocol but performs no operations, making it suitable for testing or when tracing overhead is undesirable.</p> <p>Examples:</p> <pre><code>tracer = NoOpTracer()\n\nasync with tracer.span(\"operation\") as trace_id:\n    # trace_id is still generated for compatibility\n    result = await do_something()\n</code></pre>"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer.log_error","title":"<code>log_error(trace_id, error, context=None)</code>","text":"<p>No-op error logging.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Ignored.</p> required <code>error</code> <code>Exception</code> <p>Ignored.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Ignored.</p> <code>None</code>"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer.log_metric","title":"<code>log_metric(trace_id, name, value)</code>","text":"<p>No-op metric logging.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>Ignored.</p> required <code>name</code> <code>str</code> <p>Ignored.</p> required <code>value</code> <code>Any</code> <p>Ignored.</p> required"},{"location":"api/tracing/#fastroai.tracing.NoOpTracer.span","title":"<code>span(name, **attributes)</code>  <code>async</code>","text":"<p>Create a no-op span that just yields a trace ID.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Ignored.</p> required <code>**attributes</code> <code>Any</code> <p>Ignored.</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncIterator[str]</code> <p>Unique trace ID (still generated for compatibility).</p>"},{"location":"api/usage/","title":"Usage","text":"<p>The usage module provides precise cost calculation using integer microcents to avoid floating-point precision errors in billing systems.</p>"},{"location":"api/usage/#costcalculator","title":"CostCalculator","text":"<p>\u2190 Tools Tracing \u2192</p>"},{"location":"api/usage/#fastroai.usage.CostCalculator","title":"<code>fastroai.usage.CostCalculator</code>","text":"<p>Token cost calculator with microcents precision.</p> <p>Uses genai-prices for model pricing data, with support for custom pricing overrides. All costs are returned as integer microcents.</p> <p>1 microcent = 1/10,000 cent = 1/1,000,000 dollar</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\n\n# Calculate cost for a request\ncost = calc.calculate_cost(\"gpt-4o\", input_tokens=1000, output_tokens=500)\nprint(f\"Cost: {cost} microcents\")\nprint(f\"Cost: ${calc.microcents_to_dollars(cost):.6f}\")\n\n# With custom pricing override (e.g., volume discount)\ncalc = CostCalculator(pricing_overrides={\n    \"gpt-4o\": {\"input_per_mtok\": 2.00, \"output_per_mtok\": 8.00},\n})\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.__init__","title":"<code>__init__(pricing_overrides=None)</code>","text":"<p>Initialize calculator.</p> <p>Parameters:</p> Name Type Description Default <code>pricing_overrides</code> <code>dict[str, dict[str, float]] | None</code> <p>Custom pricing for specific models. Keys are model names, values are dicts with 'input_per_mtok' and 'output_per_mtok' (dollars per million tokens). Use for volume discounts or custom models.</p> <code>None</code>"},{"location":"api/usage/#fastroai.usage.CostCalculator.calculate_cost","title":"<code>calculate_cost(model, input_tokens, output_tokens)</code>","text":"<p>Calculate cost in microcents.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4o\" or \"openai:gpt-4o\").</p> required <code>input_tokens</code> <code>int</code> <p>Number of input/prompt tokens.</p> required <code>output_tokens</code> <code>int</code> <p>Number of output/completion tokens.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Cost in microcents (integer). Returns 0 for unknown models.</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\ncost = calc.calculate_cost(\"gpt-4o\", 1000, 500)\nprint(f\"${calc.microcents_to_dollars(cost):.6f}\")\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.microcents_to_dollars","title":"<code>microcents_to_dollars(microcents)</code>","text":"<p>Convert microcents to dollars for display.</p> <p>Use this only for display purposes. For calculations, always use integer microcents.</p> <p>Parameters:</p> Name Type Description Default <code>microcents</code> <code>int</code> <p>Cost in microcents.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Cost in dollars (float).</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\ndollars = calc.microcents_to_dollars(1_500_000)\nprint(f\"${dollars:.2f}\")  # $1.50\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.dollars_to_microcents","title":"<code>dollars_to_microcents(dollars)</code>","text":"<p>Convert dollars to microcents.</p> <p>Parameters:</p> Name Type Description Default <code>dollars</code> <code>float</code> <p>Cost in dollars.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Cost in microcents (integer).</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\n\n# Set a budget of $0.10\nbudget = calc.dollars_to_microcents(0.10)\nprint(budget)  # 100000\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.format_cost","title":"<code>format_cost(microcents)</code>","text":"<p>Format cost in multiple representations.</p> <p>Parameters:</p> Name Type Description Default <code>microcents</code> <code>int</code> <p>Cost in microcents.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with microcents, cents, and dollars representations.</p> <p>Examples:</p> <pre><code>calc = CostCalculator()\nformatted = calc.format_cost(1_500_000)\n\nprint(formatted)\n# {\n#     \"microcents\": 1500000,\n#     \"cents\": 150,\n#     \"dollars\": 1.5\n# }\n</code></pre>"},{"location":"api/usage/#fastroai.usage.CostCalculator.add_pricing_override","title":"<code>add_pricing_override(model, input_per_mtok, output_per_mtok)</code>","text":"<p>Add or update pricing override for a model.</p> <p>Use this for custom pricing (volume discounts) or models not in genai-prices.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (will be normalized).</p> required <code>input_per_mtok</code> <code>float</code> <p>Input cost in dollars per million tokens.</p> required <code>output_per_mtok</code> <code>float</code> <p>Output cost in dollars per million tokens.</p> required <p>Examples:</p> <pre><code>calc = CostCalculator()\n\n# Add volume discount pricing (20% off standard)\ncalc.add_pricing_override(\n    model=\"gpt-4o\",\n    input_per_mtok=2.00,   # Standard is $2.50\n    output_per_mtok=8.00,  # Standard is $10.00\n)\n\n# Add custom/local model\ncalc.add_pricing_override(\n    model=\"my-local-llama\",\n    input_per_mtok=0.10,\n    output_per_mtok=0.20,\n)\n</code></pre>"},{"location":"guides/","title":"Guides","text":"<p>Deep dives into FastroAI's core components.</p> <p>Each guide explains what a component does, when you'd use it, and how to get it working. Start with whatever problem you're trying to solve.</p> <ul> <li> <p> FastroAgent</p> <p>Wrap PydanticAI agents with automatic cost tracking. Stateless, production-ready, with consistent response formats.</p> <p>FastroAgent \u2192</p> </li> <li> <p> Cost Calculator</p> <p>Track token costs in microcents for exact billing. Override pricing for volume discounts or custom models.</p> <p>Cost Calculator \u2192</p> </li> <li> <p> Pipelines</p> <p>Chain multiple AI steps with automatic parallelization. Track costs across entire workflows.</p> <p>Pipelines \u2192</p> </li> <li> <p> Safe Tools</p> <p>Timeout, retry, and graceful error handling for AI tools. Keep requests alive when external services fail.</p> <p>Safe Tools \u2192</p> </li> <li> <p> Tracing</p> <p>Correlate AI calls with the rest of your request flow. Integrate with any observability platform.</p> <p>Tracing \u2192</p> </li> </ul>"},{"location":"guides/#where-to-start","title":"Where to Start","text":"<p>Not sure which guide to read?</p> <p>Building an AI feature? Start with FastroAgent. It gives you cost tracking on every AI call.</p> <p>Multiple AI steps? Read Pipelines for execution order, parallelization, and aggregated costs.</p> <p>Tools calling external APIs? Check Safe Tools for graceful degradation when services fail.</p> <p>Going to production? Set up Tracing to debug slow or expensive requests.</p> <p>Most projects start with FastroAgent - it gives you cost tracking on every AI call, which you'll want in production. As your application grows, add pipelines for multi-step workflows and safe tools for external service calls.</p> <p>\u2190 Home FastroAgent \u2192</p>"},{"location":"guides/cost-calculator/","title":"Cost Calculator","text":"<p>LLM APIs charge per token. When you're processing thousands of requests, small precision errors in cost tracking add up. Floating-point math is the classic culprit:</p> <pre><code>&gt;&gt;&gt; 0.1 + 0.2\n0.30000000000000004\n</code></pre> <p>Your billing system shows $0.30000000000000004 and someone opens a support ticket. Or worse, you round incorrectly and underbill by a few cents per request, which becomes real money at scale.</p> <p>FastroAI tracks costs in microcents - integer math that doesn't drift. One microcent equals 1/1,000,000 of a dollar, so $0.01 is 10,000 microcents. All calculations use integers. When you display the cost to users, convert to dollars.</p>"},{"location":"guides/cost-calculator/#how-it-works","title":"How It Works","text":"<p>When you use <code>FastroAgent</code>, cost calculation happens automatically:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\")\n\nprint(response.cost_microcents)  # 2500 (exact integer)\nprint(response.cost_dollars)     # 0.0025 (for display)\n</code></pre> <p>The calculator looks up the model's per-token pricing, multiplies by your token counts, and returns an integer. No floating-point operations in the calculation path.</p> <p>The price data comes from genai-prices, which tracks pricing for OpenAI, Anthropic, Google, Groq, and other providers. The package is updated regularly as providers change their prices.</p>"},{"location":"guides/cost-calculator/#aggregating-costs","title":"Aggregating Costs","text":"<p>For a single request, precision doesn't matter much. But when you're aggregating across a conversation, a user's session, or your entire platform:</p> <pre><code># Track a conversation\ntotal_cost = 0\n\nfor message in user_messages:\n    response = await agent.run(message)\n    total_cost += response.cost_microcents  # Integer addition, no drift\n\n# Convert for display at the end\nprint(f\"Session cost: ${calc.microcents_to_dollars(total_cost):.4f}\")\n</code></pre> <p>After 10,000 additions, you have an exact count. No cumulative rounding errors.</p>"},{"location":"guides/cost-calculator/#direct-calculator-usage","title":"Direct Calculator Usage","text":"<p>Sometimes you need cost calculation without running an agent - estimating costs upfront, building dashboards, or custom tracking:</p> <pre><code>from fastroai import CostCalculator\n\ncalc = CostCalculator()\n\ncost = calc.calculate_cost(\n    model=\"gpt-4o\",\n    input_tokens=1000,\n    output_tokens=500,\n)\n\nprint(f\"Cost: {cost} microcents\")  # 7500 microcents\nprint(f\"Cost: ${calc.microcents_to_dollars(cost):.6f}\")  # $0.007500\n</code></pre> <p>Model names get normalized automatically. Both <code>\"gpt-4o\"</code> and <code>\"openai:gpt-4o\"</code> work.</p>"},{"location":"guides/cost-calculator/#conversion-methods","title":"Conversion Methods","text":"<pre><code>calc = CostCalculator()\n\n# Microcents to dollars (for display)\ndollars = calc.microcents_to_dollars(7500)  # 0.0075\n\n# Dollars to microcents (for storage/budgets)\nmicrocents = calc.dollars_to_microcents(0.10)  # 100000\n\n# Formatted output for debugging\nformatted = calc.format_cost(7500)\n# {\"microcents\": 7500, \"cents\": 0, \"dollars\": 0.0075}\n</code></pre>"},{"location":"guides/cost-calculator/#custom-pricing","title":"Custom Pricing","text":"<p>genai-prices covers most models, but you might need custom pricing. Maybe you've negotiated volume discounts with your provider. Or you're running self-hosted or fine-tuned models that aren't in any public pricing list. Or a new model came out and genai-prices hasn't updated yet.</p> <p>Override pricing at initialization:</p> <pre><code>from fastroai import CostCalculator\n\ncalc = CostCalculator(pricing_overrides={\n    \"gpt-4o\": {\n        \"input_per_mtok\": 2.00,   # $2.00 per million input tokens\n        \"output_per_mtok\": 8.00,  # $8.00 per million output tokens\n    },\n})\n</code></pre> <p>Or add overrides later:</p> <pre><code>calc.add_pricing_override(\n    model=\"my-local-model\",\n    input_per_mtok=0.10,\n    output_per_mtok=0.20,\n)\n</code></pre> <p>Overrides take precedence over genai-prices. Prices are in dollars per million tokens (the standard unit providers use).</p>"},{"location":"guides/cost-calculator/#using-custom-pricing-with-agents","title":"Using Custom Pricing with Agents","text":"<p>Pass your configured calculator to the agent:</p> <pre><code>from fastroai import FastroAgent, CostCalculator\n\ncalc = CostCalculator()\ncalc.add_pricing_override(\"gpt-4o\", input_per_mtok=2.00, output_per_mtok=8.00)\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    cost_calculator=calc,\n)\n\n# Now response.cost_microcents uses your pricing\nresponse = await agent.run(\"Hello!\")\n</code></pre>"},{"location":"guides/cost-calculator/#unknown-models","title":"Unknown Models","text":"<p>If you use a model that isn't in genai-prices and doesn't have a custom override, the calculator returns 0 cost and logs a debug message. This way your code doesn't crash - you just get missing cost data.</p> <p>Check your logs for \"Unknown model\" warnings. Either add a pricing override or open a PR on genai-prices.</p>"},{"location":"guides/cost-calculator/#cost-budgets","title":"Cost Budgets","text":"<p>Track cumulative costs and stop when you hit a limit:</p> <pre><code>from fastroai import CostCalculator\n\ncalc = CostCalculator()\ntotal_cost = 0\nbudget = calc.dollars_to_microcents(1.00)  # $1.00 budget\n\nfor query in queries:\n    response = await agent.run(query)\n    total_cost += response.cost_microcents\n\n    if total_cost &gt;= budget:\n        print(\"Budget exhausted\")\n        break\n\nprint(f\"Total spent: ${calc.microcents_to_dollars(total_cost):.4f}\")\n</code></pre> <p>This works fine for simple cases. For multi-step workflows where you want automatic budget enforcement, pipelines have built-in cost budgets that raise <code>CostBudgetExceededError</code> when exceeded. See the Pipelines guide.</p>"},{"location":"guides/cost-calculator/#pricing-reference","title":"Pricing Reference","text":"<p>Common model pricing as of January 2025:</p> Model Input ($/1M tokens) Output ($/1M tokens) gpt-4o $2.50 $10.00 gpt-4o-mini $0.15 $0.60 claude-3-5-sonnet $3.00 $15.00 claude-3-haiku $0.25 $1.25 gemini-1.5-pro $1.25 $5.00 gemini-1.5-flash $0.075 $0.30 <p>Prices change. Check your provider's current pricing, and consider using pricing overrides if you're on a negotiated rate.</p>"},{"location":"guides/cost-calculator/#key-files","title":"Key Files","text":"Component Location CostCalculator <code>fastroai/usage/calculator.py</code> <p>\u2190 FastroAgent Pipelines \u2192</p>"},{"location":"guides/fastro-agent/","title":"FastroAgent","text":"<p>FastroAgent wraps PydanticAI's <code>Agent</code> with automatic cost calculation, optional distributed tracing, and a consistent response format. Every response includes token counts and cost in microcents (exact integer math, no floating-point drift) so you can track usage, set budgets, and debug cost issues in production.</p>"},{"location":"guides/fastro-agent/#how-it-works","title":"How It Works","text":"<pre><code>sequenceDiagram\n    participant You\n    participant FastroAgent\n    participant PydanticAI\n    participant LLM API\n\n    You-&gt;&gt;FastroAgent: run(\"Hello!\")\n    FastroAgent-&gt;&gt;PydanticAI: agent.run()\n    PydanticAI-&gt;&gt;LLM API: HTTP request\n    LLM API--&gt;&gt;PydanticAI: response + token counts\n    PydanticAI--&gt;&gt;FastroAgent: result\n    FastroAgent-&gt;&gt;FastroAgent: calculate cost\n    FastroAgent--&gt;&gt;You: ChatResponse</code></pre> <p>When you call <code>agent.run()</code>, FastroAgent passes your message to the underlying PydanticAI agent, which handles the actual LLM call. When the response comes back, FastroAgent extracts token counts, calculates cost using genai-prices (or your custom pricing), and packages everything into a <code>ChatResponse</code> with usage data.</p> <p>The agent itself is stateless. Conversation history, user context, and retry logic are your responsibility. This keeps FastroAgent simple and predictable.</p>"},{"location":"guides/fastro-agent/#creating-an-agent","title":"Creating an Agent","text":"<p>The simplest agent uses defaults for everything:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(model=\"openai:gpt-4o\")\n</code></pre> <p>Add a system prompt and tweak parameters as needed:</p> <pre><code>agent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a helpful financial advisor.\",\n    temperature=0.3,  # More consistent responses\n    max_tokens=4096,\n)\n</code></pre> <p>If you're passing configuration around (from environment variables, user settings, or a config file), use <code>AgentConfig</code>:</p> <pre><code>from fastroai import FastroAgent, AgentConfig\n\nconfig = AgentConfig(\n    model=\"anthropic:claude-3-5-sonnet\",\n    system_prompt=\"You are a code reviewer.\",\n    temperature=0.2,\n)\n\nagent = FastroAgent(config=config)\n</code></pre>"},{"location":"guides/fastro-agent/#configuration-reference","title":"Configuration Reference","text":"Parameter Default Description <code>model</code> <code>openai:gpt-4o</code> Model identifier with provider prefix <code>system_prompt</code> <code>\"You are a helpful AI assistant.\"</code> Instructions for the model <code>temperature</code> <code>0.7</code> Sampling temperature (0.0-2.0) <code>max_tokens</code> <code>4096</code> Maximum response tokens <code>timeout_seconds</code> <code>120</code> Request timeout <code>max_retries</code> <code>3</code> Retry attempts on failure <p>Model names use PydanticAI's provider prefix format: <code>openai:gpt-4o</code>, <code>anthropic:claude-3-5-sonnet</code>, <code>google:gemini-1.5-pro</code>. The prefix tells PydanticAI which API client to use.</p>"},{"location":"guides/fastro-agent/#running-queries","title":"Running Queries","text":"<p>Use <code>run()</code> to send a message and get a response:</p> <pre><code>response = await agent.run(\"What is the capital of France?\")\n\nprint(response.output)  # \"The capital of France is Paris.\"\nprint(response.cost_dollars)  # 0.000234\n</code></pre> <p>The response includes everything you need for billing and debugging:</p> <pre><code>print(f\"Model: {response.model}\")\nprint(f\"Input tokens: {response.input_tokens}\")\nprint(f\"Output tokens: {response.output_tokens}\")\nprint(f\"Cost: ${response.cost_dollars:.6f}\")\nprint(f\"Time: {response.processing_time_ms}ms\")\n</code></pre>"},{"location":"guides/fastro-agent/#whats-in-a-response","title":"What's in a Response","text":"Field Type What It's For <code>content</code> <code>str</code> The response text <code>output</code> <code>OutputT</code> Typed output (same as content for string agents) <code>model</code> <code>str</code> Model that generated the response <code>input_tokens</code> <code>int</code> Prompt tokens consumed <code>output_tokens</code> <code>int</code> Completion tokens generated <code>cost_microcents</code> <code>int</code> Cost in 1/1,000,000 of a dollar (for calculations) <code>cost_dollars</code> <code>float</code> Cost in dollars (for display) <code>processing_time_ms</code> <code>int</code> Wall-clock time <code>tool_calls</code> <code>list</code> Tools invoked during generation <code>trace_id</code> <code>str</code> Tracing correlation ID <p>Use <code>cost_microcents</code> when you need to aggregate costs across many calls - integer math won't drift. Use <code>cost_dollars</code> when displaying to users.</p>"},{"location":"guides/fastro-agent/#structured-output","title":"Structured Output","text":"<p>Getting strings back means you have to parse them. For anything structured - extracting entities, classifying content, generating schemas - use Pydantic models instead:</p> <pre><code>from pydantic import BaseModel\nfrom fastroai import FastroAgent\n\nclass MovieReview(BaseModel):\n    title: str\n    rating: int\n    summary: str\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    output_type=MovieReview,\n)\n\nresponse = await agent.run(\"Review the movie Inception\")\n\n# response.output is a MovieReview instance, not a string\nprint(response.output.title)   # \"Inception\"\nprint(response.output.rating)  # 9\nprint(response.output.summary) # \"A mind-bending thriller...\"\n</code></pre> <p>PydanticAI handles the structured output extraction and validation. If the LLM output doesn't match your schema, it retries automatically. You pay for those retry tokens, so keep your schemas reasonable - don't ask for 50 required fields in one call.</p>"},{"location":"guides/fastro-agent/#conversation-history","title":"Conversation History","text":"<p>FastroAgent doesn't store conversation history. This sounds annoying until you realize how many ways there are to mess up conversation storage, and how much your storage requirements differ from everyone else's.</p> <p>You load history from your storage, pass it in, and save new messages yourself:</p> <pre><code>from pydantic_ai.messages import ModelMessage\n\n# Load from your storage (database, Redis, files, whatever)\nhistory: list[ModelMessage] = await my_storage.load(user_id)\n\n# Pass to the agent\nresponse = await agent.run(\n    \"Continue our conversation\",\n    message_history=history,\n)\n\n# Save the new user message and response\n# You manage how messages are stored - FastroAgent doesn't dictate the format\nawait my_storage.append(user_id, \"Continue our conversation\", response.content)\n</code></pre> <p>This gives you full control. Store history in a database table, Redis with a TTL, or flat files - whatever fits your architecture. Keep conversations forever or clear them after each session. Store complete messages or compress them into summaries. FastroAI doesn't care; it just takes whatever history you pass in.</p>"},{"location":"guides/fastro-agent/#streaming","title":"Streaming","text":"<p>For long responses, stream chunks as they arrive so users don't stare at a loading spinner:</p> <pre><code>async for chunk in agent.run_stream(\"Write a short story\"):\n    if chunk.is_final:\n        # Last chunk has complete usage data\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        # Print content as it arrives\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>Cost tracking still works - you just get it at the end instead of upfront. The final chunk has <code>is_final=True</code> and includes the complete <code>usage_data</code> with token counts and cost.</p> <p>One gotcha: if your connection drops mid-stream, you lose both the partial response and the usage data. Consider logging partial streams if you need that level of tracking.</p>"},{"location":"guides/fastro-agent/#adding-tools","title":"Adding Tools","text":"<p>Give agents capabilities by passing toolsets. Here's a simple weather tool:</p> <pre><code>from pydantic_ai.toolsets import FunctionToolset\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    # Your weather API call here\n    return f\"Sunny, 72\u00b0F in {city}\"\n\ntoolset = FunctionToolset(tools=[get_weather])\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You can check the weather.\",\n    toolsets=[toolset],\n)\n\nresponse = await agent.run(\"What's the weather in Paris?\")\n# Agent calls get_weather(\"Paris\") and uses the result\n</code></pre> <p>Tool calls are tracked in the response:</p> <pre><code>for call in response.tool_calls:\n    print(f\"Called: {call['tool_name']}\")\n    print(f\"Args: {call['args']}\")\n</code></pre> <p>In production, tools fail. APIs time out. External services go down. Regular tools crash the whole request when that happens. The Safe Tools guide covers <code>@safe_tool</code>, which wraps tools with timeout, retry, and graceful error handling so failures become messages the LLM can work with instead of exceptions that kill your request.</p>"},{"location":"guides/fastro-agent/#dependencies","title":"Dependencies","text":"<p>Pass runtime dependencies to your tools - database connections, API clients, user context:</p> <pre><code>class MyDeps:\n    def __init__(self, api_key: str, user_id: str):\n        self.api_key = api_key\n        self.user_id = user_id\n\nresponse = await agent.run(\n    \"Search for news about AI\",\n    deps=MyDeps(api_key=\"sk-...\", user_id=\"user_123\"),\n)\n</code></pre> <p>Your tools receive these deps as their first argument. This keeps tools testable (inject mock deps) and avoids global state.</p>"},{"location":"guides/fastro-agent/#tracing","title":"Tracing","text":"<p>In production, you need to correlate AI calls with the rest of your request flow. Pass a tracer:</p> <pre><code>from fastroai import SimpleTracer\n\ntracer = SimpleTracer()\nresponse = await agent.run(\"Hello\", tracer=tracer)\n\nprint(response.trace_id)  # \"trace_abc123...\"\n</code></pre> <p><code>SimpleTracer</code> logs to Python's logging module - good for development. For production, implement the <code>Tracer</code> protocol to send spans to your observability platform (Datadog, Honeycomb, Logfire, etc.). The Tracing guide has examples for common platforms.</p>"},{"location":"guides/fastro-agent/#using-existing-pydanticai-agents","title":"Using Existing PydanticAI Agents","text":"<p>Already have a configured PydanticAI agent with custom output types, validators, or complex tooling? Wrap it with FastroAgent to add cost tracking without changing anything else:</p> <pre><code>from pydantic_ai import Agent\nfrom fastroai import FastroAgent\n\n# Your existing PydanticAI agent\npydantic_agent = Agent(\n    model=\"openai:gpt-4o\",\n    output_type=MyCustomType,\n    # ... your configuration\n)\n\n# Wrap it with FastroAgent\nfastro_agent = FastroAgent(agent=pydantic_agent)\n\n# Now you get cost tracking on your existing agent\nresponse = await fastro_agent.run(\"Hello\")\nprint(response.cost_dollars)\n</code></pre> <p>The underlying agent is always accessible if you need it:</p> <pre><code>pydantic_agent = fastro_agent.agent\n</code></pre>"},{"location":"guides/fastro-agent/#key-files","title":"Key Files","text":"Component Location FastroAgent <code>fastroai/agent/agent.py</code> AgentConfig <code>fastroai/agent/schemas.py</code> ChatResponse <code>fastroai/agent/schemas.py</code> StreamChunk <code>fastroai/agent/schemas.py</code> <p>\u2190 Guides Overview Cost Calculator \u2192</p>"},{"location":"guides/pipelines/","title":"Pipelines","text":"<p>A single agent call is one thing. Real applications chain multiple calls: extract entities, classify them, look up related data, generate a response. Each step might need different models, different prompts, different retry behavior. And you want to track costs across the whole flow, not just individual calls.</p> <p>Pipelines orchestrate multi-step AI workflows. You declare your steps and their dependencies. FastroAI figures out what can run in parallel, executes everything in the right order, and aggregates costs and timing across the whole pipeline.</p>"},{"location":"guides/pipelines/#when-to-use-pipelines","title":"When to Use Pipelines","text":"<p>Pipelines make sense when you're chaining multiple AI steps - extract entities, then classify them, then summarize. Or when you have independent steps that should run in parallel, like fetching data from multiple sources at once. They're also useful when you need cost tracking across an entire workflow rather than individual calls, or when you want to enforce cost budgets that stop execution before you blow through money.</p> <p>For a single agent call, use <code>FastroAgent</code> directly. The pipeline machinery isn't worth it for one step.</p>"},{"location":"guides/pipelines/#how-pipelines-work","title":"How Pipelines Work","text":"<p>You define steps and declare which steps depend on which other steps. FastroAI builds a dependency graph, topologically sorts it, and runs steps level by level. Steps at the same level run concurrently.</p> <pre><code>flowchart TB\n    extract --&gt; classify\n    classify --&gt; fetch_market\n    classify --&gt; fetch_user\n    fetch_market --&gt; calculate\n    fetch_user --&gt; calculate\n\n    subgraph parallel[\" \"]\n        fetch_market\n        fetch_user\n    end\n\n    style parallel fill:none,stroke:#666,stroke-dasharray: 5 5</code></pre> <p>In this example, <code>fetch_market</code> and <code>fetch_user</code> both depend on <code>classify</code>, so they run in parallel once <code>classify</code> completes. The <code>calculate</code> step waits for both to finish.</p> <p>Each step gets a <code>StepContext</code> with access to pipeline inputs, outputs from dependency steps, your application deps, and a usage tracker. When you call agents through <code>ctx.run()</code>, costs accumulate automatically.</p>"},{"location":"guides/pipelines/#a-basic-pipeline","title":"A Basic Pipeline","text":"<p>Here's a two-step pipeline that extracts entities and then classifies them:</p> <pre><code>from fastroai import Pipeline, BaseStep, StepContext, FastroAgent\n\nclass ExtractStep(BaseStep[None, str]):\n    def __init__(self):\n        self.agent = FastroAgent(\n            model=\"openai:gpt-4o-mini\",\n            system_prompt=\"Extract key entities from text.\",\n        )\n\n    async def execute(self, ctx: StepContext[None]) -&gt; str:\n        document = ctx.get_input(\"document\")\n        response = await ctx.run(self.agent, f\"Extract entities: {document}\")\n        return response.output\n\nclass ClassifyStep(BaseStep[None, str]):\n    def __init__(self):\n        self.agent = FastroAgent(\n            model=\"openai:gpt-4o-mini\",\n            system_prompt=\"Classify documents.\",\n        )\n\n    async def execute(self, ctx: StepContext[None]) -&gt; str:\n        entities = ctx.get_dependency(\"extract\")\n        response = await ctx.run(self.agent, f\"Classify based on: {entities}\")\n        return response.output\n\npipeline = Pipeline(\n    name=\"document_processor\",\n    steps={\n        \"extract\": ExtractStep(),\n        \"classify\": ClassifyStep(),\n    },\n    dependencies={\n        \"classify\": [\"extract\"],  # classify waits for extract\n    },\n)\n\nresult = await pipeline.execute({\"document\": \"Apple announced...\"}, deps=None)\nprint(result.output)  # Classification result\nprint(f\"Total cost: ${result.usage.total_cost_dollars:.6f}\")\n</code></pre> <p>The <code>dependencies</code> dict says \"classify depends on extract\". FastroAI runs extract first, then classify. If extract fails, classify never runs.</p>"},{"location":"guides/pipelines/#three-ways-to-define-steps","title":"Three Ways to Define Steps","text":"<p>FastroAI offers three approaches, from simplest to most flexible. Use whichever fits your situation.</p>"},{"location":"guides/pipelines/#1-the-step-decorator","title":"1. The <code>@step</code> Decorator","text":"<p>For steps that don't need initialization or complex state:</p> <pre><code>from fastroai import step, StepContext\n\n@step\nasync def transform(ctx: StepContext[None]) -&gt; str:\n    text = ctx.get_input(\"text\")\n    return text.upper()\n\n@step(timeout=30.0, retries=2)\nasync def classify(ctx: StepContext[None]) -&gt; str:\n    text = ctx.get_dependency(\"transform\")\n    response = await ctx.run(classifier_agent, f\"Classify: {text}\")\n    return response.output\n\npipeline = Pipeline(\n    name=\"processor\",\n    steps={\"transform\": transform, \"classify\": classify},\n    dependencies={\"classify\": [\"transform\"]},\n)\n</code></pre> <p>The decorator accepts <code>timeout</code>, <code>retries</code>, <code>retry_delay</code>, and <code>cost_budget</code> parameters.</p>"},{"location":"guides/pipelines/#2-agentas_step","title":"2. <code>agent.as_step()</code>","text":"<p>When a step is just a single agent call with no other logic:</p> <pre><code>from fastroai import FastroAgent, Pipeline\n\nsummarizer = FastroAgent(\n    model=\"openai:gpt-4o-mini\",\n    system_prompt=\"Summarize text concisely.\",\n)\n\npipeline = Pipeline(\n    name=\"summarizer\",\n    steps={\n        \"summarize\": summarizer.as_step(\n            lambda ctx: f\"Summarize: {ctx.get_input('text')}\"\n        ),\n    },\n)\n\nresult = await pipeline.execute({\"text\": \"Long article...\"}, deps=None)\n</code></pre> <p>The prompt can be a static string or a function that builds it from context.</p>"},{"location":"guides/pipelines/#3-basestep-class","title":"3. <code>BaseStep</code> Class","text":"<p>For complex steps with multiple agents, conditional logic, or state:</p> <pre><code>from fastroai import BaseStep, StepContext, FastroAgent\n\nclass ResearchStep(BaseStep[MyDeps, dict]):\n    def __init__(self):\n        self.classifier = FastroAgent(model=\"gpt-4o-mini\", system_prompt=\"Classify.\")\n        self.writer = FastroAgent(model=\"gpt-4o\", system_prompt=\"Write reports.\")\n\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; dict:\n        topic = ctx.get_input(\"topic\")\n\n        # Multiple agent calls with branching logic\n        category = await ctx.run(self.classifier, f\"Classify: {topic}\")\n\n        if \"technical\" in category.output.lower():\n            report = await ctx.run(self.writer, f\"Technical report on: {topic}\")\n        else:\n            report = await ctx.run(self.writer, f\"General summary of: {topic}\")\n\n        return {\"category\": category.output, \"report\": report.output}\n</code></pre>"},{"location":"guides/pipelines/#the-ctxrun-method","title":"The <code>ctx.run()</code> Method","text":"<p>Always call agents through <code>ctx.run()</code> rather than <code>agent.run()</code> directly. This is how FastroAI tracks costs and enforces budgets.</p> <pre><code>response = await ctx.run(agent, \"Your message\")\n</code></pre> <p><code>ctx.run()</code> handles the integration work for you. It passes your deps to the agent, forwards the tracer for distributed tracing, and accumulates usage in <code>ctx.usage</code> so pipeline-level cost tracking works. It also enforces timeout and retry settings from your config, and checks cost budgets - raising <code>CostBudgetExceededError</code> if you've exceeded your limit.</p> <p>You can override config per-call when one step needs different behavior:</p> <pre><code>response = await ctx.run(agent, \"message\", timeout=60.0, retries=5)\n</code></pre>"},{"location":"guides/pipelines/#parallel-execution","title":"Parallel Execution","text":"<p>Steps with different dependencies run in parallel automatically. You don't need to manage asyncio.gather or thread pools.</p> <pre><code>dependencies = {\n    \"classify\": [\"extract\"],\n    \"fetch_market\": [\"classify\"],\n    \"fetch_user\": [\"classify\"],     # Same dependency as fetch_market\n    \"calculate\": [\"fetch_market\", \"fetch_user\"],\n}\n</code></pre> <p>Here's how that executes over time:</p> <pre><code>sequenceDiagram\n    participant Pipeline\n    participant extract\n    participant classify\n    participant fetch_market\n    participant fetch_user\n    participant calculate\n\n    Pipeline-&gt;&gt;extract: execute\n    extract--&gt;&gt;Pipeline: done\n    Pipeline-&gt;&gt;classify: execute\n    classify--&gt;&gt;Pipeline: done\n    par parallel execution\n        Pipeline-&gt;&gt;fetch_market: execute\n        Pipeline-&gt;&gt;fetch_user: execute\n        fetch_market--&gt;&gt;Pipeline: done\n        fetch_user--&gt;&gt;Pipeline: done\n    end\n    Pipeline-&gt;&gt;calculate: execute\n    calculate--&gt;&gt;Pipeline: done</code></pre> <p>FastroAI groups steps by their maximum dependency depth and runs each level concurrently. If a step fails, downstream steps that depend on it won't run, but unrelated steps continue normally.</p>"},{"location":"guides/pipelines/#configuration","title":"Configuration","text":""},{"location":"guides/pipelines/#pipeline-level-defaults","title":"Pipeline-Level Defaults","text":"<p>Set defaults that apply to all steps:</p> <pre><code>from fastroai import Pipeline, PipelineConfig\n\npipeline = Pipeline(\n    name=\"processor\",\n    steps={...},\n    config=PipelineConfig(\n        timeout=30.0,         # Default timeout for all steps\n        retries=2,            # Default retry count\n        cost_budget=100_000,  # $0.10 total budget (in microcents)\n    ),\n)\n</code></pre>"},{"location":"guides/pipelines/#per-step-overrides","title":"Per-Step Overrides","text":"<p>Override configuration for specific steps:</p> <pre><code>from fastroai import Pipeline, PipelineConfig, StepConfig\n\npipeline = Pipeline(\n    name=\"processor\",\n    steps={...},\n    config=PipelineConfig(timeout=30.0),\n    step_configs={\n        \"slow_step\": StepConfig(timeout=120.0),\n        \"expensive_step\": StepConfig(cost_budget=50_000),\n    },\n)\n</code></pre>"},{"location":"guides/pipelines/#config-resolution-order","title":"Config Resolution Order","text":"<p>Most specific wins:</p> <ol> <li><code>PipelineConfig</code> defaults (lowest priority)</li> <li>Step class <code>.config</code> attribute</li> <li><code>step_configs[step_id]</code> override</li> <li>Per-call <code>ctx.run(timeout=..., retries=...)</code> override (highest priority)</li> </ol>"},{"location":"guides/pipelines/#accessing-data","title":"Accessing Data","text":""},{"location":"guides/pipelines/#pipeline-inputs","title":"Pipeline Inputs","text":"<p>Get data passed to <code>pipeline.execute()</code>:</p> <pre><code>async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n    # Required input - raises KeyError if missing\n    document = ctx.get_input(\"document\")\n\n    # Optional input with default\n    format_type = ctx.get_input(\"format\", \"json\")\n</code></pre>"},{"location":"guides/pipelines/#dependency-outputs","title":"Dependency Outputs","text":"<p>Get outputs from steps this step depends on:</p> <pre><code>async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n    # Get output from extract step\n    entities = ctx.get_dependency(\"extract\")\n\n    # With type hint for IDE support\n    entities = ctx.get_dependency(\"extract\", ExtractionResult)\n\n    # Optional dependency (might not exist in the graph)\n    metadata = ctx.get_dependency_or_none(\"fetch_metadata\", dict)\n</code></pre>"},{"location":"guides/pipelines/#application-dependencies","title":"Application Dependencies","text":"<p>Access your deps object (database connections, user context, API clients):</p> <pre><code>async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n    db = ctx.deps.session\n    user_id = ctx.deps.user_id\n    api_client = ctx.deps.external_api\n</code></pre>"},{"location":"guides/pipelines/#error-handling","title":"Error Handling","text":"<p>When a step fails, the pipeline raises <code>StepExecutionError</code>. You can catch this and handle it:</p> <pre><code>from fastroai import StepExecutionError\n\ntry:\n    result = await pipeline.execute(inputs, deps)\nexcept StepExecutionError as e:\n    print(f\"Step '{e.step_id}' failed: {e.original_error}\")\n</code></pre> <p>For cost budget violations specifically, the step raises <code>CostBudgetExceededError</code>:</p> <pre><code>from fastroai import CostBudgetExceededError\n\ntry:\n    result = await pipeline.execute(inputs, deps)\nexcept CostBudgetExceededError as e:\n    print(f\"Budget exceeded: spent {e.actual_microcents} of {e.budget_microcents}\")\n</code></pre>"},{"location":"guides/pipelines/#early-termination","title":"Early Termination","text":"<p>For multi-turn conversations or workflows that need user input, steps can signal that more information is needed:</p> <pre><code>from fastroai import BaseStep, ConversationState, ConversationStatus\n\nclass GatherInfoStep(BaseStep[None, ConversationState[dict]]):\n    async def execute(self, ctx) -&gt; ConversationState[dict]:\n        message = ctx.get_input(\"message\")\n        current_data = ctx.get_input(\"current_data\") or {}\n\n        # Extract info from message\n        if \"email\" in message.lower():\n            current_data[\"email\"] = extract_email(message)\n\n        # Check if we have everything we need\n        required = {\"name\", \"email\"}\n        missing = required - set(current_data.keys())\n\n        if not missing:\n            return ConversationState(\n                status=ConversationStatus.COMPLETE,\n                data=current_data,\n            )\n\n        return ConversationState(\n            status=ConversationStatus.INCOMPLETE,\n            data=current_data,\n            context={\"missing\": list(missing)},\n        )\n</code></pre> <p>When a step returns <code>INCOMPLETE</code>, the pipeline stops early:</p> <pre><code>result = await pipeline.execute(inputs, deps)\n\nif result.stopped_early:\n    missing = result.conversation_state.context[\"missing\"]\n    return {\"status\": \"incomplete\", \"need\": missing}\n\nreturn {\"status\": \"complete\", \"output\": result.output}\n</code></pre> <p>This lets you build conversational flows where the pipeline pauses to ask the user for more information, then resumes when they respond.</p>"},{"location":"guides/pipelines/#usage-tracking","title":"Usage Tracking","text":"<p>The result includes aggregated usage across all steps:</p> <pre><code>result = await pipeline.execute(inputs, deps)\n\nif result.usage:\n    print(f\"Input tokens: {result.usage.total_input_tokens}\")\n    print(f\"Output tokens: {result.usage.total_output_tokens}\")\n    print(f\"Total cost: ${result.usage.total_cost_dollars:.6f}\")\n</code></pre> <p>This is the sum of all agent calls across all steps. If you need per-step breakdowns, log them from within your step implementations.</p>"},{"location":"guides/pipelines/#key-files","title":"Key Files","text":"Component Location Pipeline <code>fastroai/pipelines/pipeline.py</code> BaseStep <code>fastroai/pipelines/base.py</code> StepContext <code>fastroai/pipelines/base.py</code> @step decorator <code>fastroai/pipelines/decorators.py</code> StepConfig <code>fastroai/pipelines/config.py</code> PipelineConfig <code>fastroai/pipelines/config.py</code> ConversationState <code>fastroai/pipelines/base.py</code> <p>\u2190 Cost Calculator Safe Tools \u2192</p>"},{"location":"guides/safe-tools/","title":"Safe Tools","text":"<p>Tools that call external services can fail. APIs time out, return errors, or hang indefinitely. When a regular tool fails, the whole agent request crashes and the user sees an error. The AI never gets a chance to respond gracefully.</p> <p><code>@safe_tool</code> wraps tools with timeout, retry, and graceful error handling. When something goes wrong, the AI receives an error message instead of an exception. It can then respond appropriately - apologize, suggest alternatives, or try a different approach.</p>"},{"location":"guides/safe-tools/#how-it-works","title":"How It Works","text":"<pre><code>from fastroai import safe_tool\n\n@safe_tool(timeout=10, max_retries=2)\nasync def fetch_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{location}\")\n        return response.text\n</code></pre> <p>When the API is slow or fails:</p> <ol> <li>The wrapper waits up to 10 seconds for the first attempt</li> <li>If it times out or errors, it retries (up to 2 more attempts)</li> <li>Retries use exponential backoff (0.1s, 0.2s, 0.4s...)</li> <li>If all attempts fail, it returns an error message string</li> </ol> <p>The AI sees \"Tool timed out after 2 attempts\" and can respond: \"I'm having trouble getting weather data right now. Would you like me to try again, or is there something else I can help with?\"</p> <p>Your request doesn't crash. You don't lose the prompt tokens. The user gets a response.</p>"},{"location":"guides/safe-tools/#decorator-options","title":"Decorator Options","text":"<pre><code>@safe_tool(\n    timeout=30,          # Seconds per attempt (default: 30)\n    max_retries=3,       # Total attempts (default: 3)\n    on_timeout=\"...\",    # Custom timeout message\n    on_error=\"...\",      # Custom error message\n)\n</code></pre> Parameter Default Description <code>timeout</code> <code>30</code> Maximum seconds per attempt <code>max_retries</code> <code>3</code> Total attempts before giving up <code>on_timeout</code> <code>\"Tool timed out after {max_retries} attempts\"</code> Message on timeout <code>on_error</code> <code>\"Tool failed: {error}\"</code> Message on error (use <code>{error}</code> placeholder)"},{"location":"guides/safe-tools/#custom-error-messages","title":"Custom Error Messages","text":"<p>Generic error messages are fine for development, but in production you want messages that help the AI respond well:</p> <pre><code>@safe_tool(\n    timeout=30,\n    on_timeout=\"Web search is taking too long. Suggest the user try a simpler query.\",\n    on_error=\"Web search is currently unavailable: {error}. Apologize and offer alternatives.\",\n)\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    ...\n</code></pre> <p>The AI receives these messages as tool output and incorporates them into its response. Clearer error messages lead to better AI responses.</p>"},{"location":"guides/safe-tools/#retry-behavior","title":"Retry Behavior","text":"<p>Retries use exponential backoff to avoid hammering a struggling service:</p> Attempt Delay Before 1 immediate 2 0.1s 3 0.2s 4 0.4s 5 0.8s <p>The delays are short because you're holding a user request. If a service needs multiple seconds to recover, three quick retries aren't going to help anyway.</p> <p>For services that need longer recovery time, reduce <code>max_retries</code> and increase <code>timeout</code> instead.</p>"},{"location":"guides/safe-tools/#organizing-tools-with-toolsets","title":"Organizing Tools with Toolsets","text":"<p>When you have multiple tools, group them:</p> <pre><code>from fastroai import safe_tool, SafeToolset\n\n@safe_tool(timeout=10)\nasync def get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather.\"\"\"\n    ...\n\n@safe_tool(timeout=30)\nasync def web_search(query: str) -&gt; str:\n    \"\"\"Search the web.\"\"\"\n    ...\n\nclass WebToolset(SafeToolset):\n    def __init__(self):\n        super().__init__(\n            tools=[get_weather, web_search],\n            name=\"web\",\n        )\n</code></pre> <p>Then pass the toolset to your agent:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You can check weather and search the web.\",\n    toolsets=[WebToolset()],\n)\n</code></pre> <p><code>SafeToolset</code> is a marker class that indicates all tools in the set have proper timeout and error handling. It extends PydanticAI's <code>FunctionToolset</code> with a name for logging and debugging.</p>"},{"location":"guides/safe-tools/#when-to-use-safe-tools","title":"When to Use Safe Tools","text":"<p>Use <code>@safe_tool</code> for anything that touches the outside world. Network requests to APIs that might be slow or down. Database queries that could timeout. File system operations on paths that might not exist. Third-party services with rate limits or auth that expires. Basically, if something outside your process can make the tool fail or hang, wrap it.</p> <p>You can skip it for pure computation - math, string manipulation, data transformation. These either work or they don't, and if they don't, something's fundamentally broken anyway. Same for simple lookups in local data structures or cached values.</p>"},{"location":"guides/safe-tools/#what-happens-when-tools-fail","title":"What Happens When Tools Fail","text":"<p>With regular tools, a timeout or error crashes the whole request:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    participant API\n\n    User-&gt;&gt;Agent: \"What's the weather?\"\n    Agent-&gt;&gt;Tool: get_weather(\"Paris\")\n    Tool-&gt;&gt;API: HTTP request\n    API--xTool: Timeout\n    Tool--xAgent: Exception\n    Agent--xUser: 500 Error</code></pre> <p>With safe tools, failures become messages the AI can work with:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    participant API\n\n    User-&gt;&gt;Agent: \"What's the weather?\"\n    Agent-&gt;&gt;Tool: get_weather(\"Paris\")\n    Tool-&gt;&gt;API: HTTP request\n    API--xTool: Timeout\n    Tool--&gt;&gt;Agent: \"Tool timed out\"\n    Agent-&gt;&gt;User: \"I'm having trouble checking weather right now...\"</code></pre> <p>The AI incorporates the failure into its response and the user gets a helpful answer instead of an error page.</p>"},{"location":"guides/safe-tools/#testing-safe-tools","title":"Testing Safe Tools","text":"<p>In tests, you probably want errors to propagate so you can verify your error handling. Mock the underlying service rather than the safe_tool wrapper:</p> <pre><code>@pytest.fixture\ndef mock_weather_api(monkeypatch):\n    async def mock_get(*args, **kwargs):\n        raise httpx.TimeoutException(\"test timeout\")\n    monkeypatch.setattr(httpx.AsyncClient, \"get\", mock_get)\n\nasync def test_weather_timeout(mock_weather_api):\n    result = await fetch_weather(\"London\")\n    assert \"timed out\" in result.lower()\n</code></pre> <p>The safe_tool wrapper returns the error message, so your test can verify the AI receives appropriate feedback.</p>"},{"location":"guides/safe-tools/#complete-example","title":"Complete Example","text":"<pre><code>from fastroai import FastroAgent, safe_tool, SafeToolset\n\n@safe_tool(timeout=10, max_retries=2)\nasync def get_stock_price(symbol: str) -&gt; str:\n    \"\"\"Get current stock price.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(f\"https://api.stocks.com/{symbol}\")\n        data = resp.json()\n        return f\"${data['price']}\"\n\n@safe_tool(\n    timeout=5,\n    on_error=\"Could not get exchange rate: {error}. Suggest using approximate rates.\",\n)\nasync def get_exchange_rate(from_currency: str, to_currency: str) -&gt; str:\n    \"\"\"Get exchange rate between currencies.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(\n            f\"https://api.exchange.com/rate?from={from_currency}&amp;to={to_currency}\"\n        )\n        return resp.json()[\"rate\"]\n\nclass FinanceToolset(SafeToolset):\n    def __init__(self):\n        super().__init__(\n            tools=[get_stock_price, get_exchange_rate],\n            name=\"finance\",\n        )\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a financial assistant with access to stock prices and exchange rates.\",\n    toolsets=[FinanceToolset()],\n)\n\nresponse = await agent.run(\"What's the current price of AAPL in euros?\")\n</code></pre> <p>If the stock API is down, the agent might respond: \"I'm having trouble getting stock prices right now. Based on recent data, AAPL was trading around $180. Would you like me to try again in a moment?\"</p> <p>That's a much better experience than a 500 error.</p>"},{"location":"guides/safe-tools/#key-files","title":"Key Files","text":"Component Location @safe_tool <code>fastroai/tools/decorators.py</code> SafeToolset <code>fastroai/tools/toolsets.py</code> FunctionToolsetBase <code>fastroai/tools/toolsets.py</code> <p>\u2190 Pipelines Tracing \u2192</p>"},{"location":"guides/tracing/","title":"Tracing","text":"<p>AI calls are part of larger request flows. Without tracing, you know a request was slow or expensive, but not which AI call caused it.</p> <p>FastroAI provides a protocol-based tracing interface that integrates with any observability backend. Pass a tracer to correlate AI calls with the rest of your application - each response includes a trace ID you can search for in your observability platform.</p>"},{"location":"guides/tracing/#how-tracing-works","title":"How Tracing Works","text":"<pre><code>sequenceDiagram\n    participant App\n    participant FastroAgent\n    participant Tracer\n    participant LLM\n\n    App-&gt;&gt;FastroAgent: run(message, tracer)\n    FastroAgent-&gt;&gt;Tracer: span(\"agent.run\")\n    Tracer--&gt;&gt;FastroAgent: trace_id\n    FastroAgent-&gt;&gt;LLM: API call\n    LLM--&gt;&gt;FastroAgent: response + tokens\n    FastroAgent-&gt;&gt;Tracer: log_metric(tokens, cost)\n    FastroAgent-&gt;&gt;Tracer: close span\n    FastroAgent--&gt;&gt;App: ChatResponse with trace_id</code></pre> <p>When you pass a tracer to <code>agent.run()</code>, FastroAgent creates a span with a unique trace ID, makes the LLM call, records token usage and cost as metrics, then closes the span. The trace ID comes back in the response so you can correlate it with your logs.</p>"},{"location":"guides/tracing/#quick-start","title":"Quick Start","text":"<p><code>SimpleTracer</code> logs to Python's logging module. Good for development and debugging:</p> <pre><code>from fastroai import FastroAgent, SimpleTracer\n\ntracer = SimpleTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\n\nresponse = await agent.run(\"Hello!\", tracer=tracer)\nprint(response.trace_id)  # \"abc12345-...\"\n</code></pre> <p>Logs output:</p> <pre><code>INFO [abc12345] Starting fastroai.agent.run\nINFO [abc12345] Metric input_tokens=12\nINFO [abc12345] Metric output_tokens=8\nINFO [abc12345] Metric cost_microcents=250\nINFO [abc12345] Completed fastroai.agent.run in 0.847s\n</code></pre> <p>When something fails:</p> <pre><code>ERROR [abc12345] FAILED fastroai.agent.run after 0.456s: Connection timeout\n</code></pre>"},{"location":"guides/tracing/#built-in-tracers","title":"Built-in Tracers","text":""},{"location":"guides/tracing/#simpletracer","title":"SimpleTracer","text":"<p>Logs to Python's logging module. Use for development, debugging, or when you just need basic visibility:</p> <pre><code>from fastroai import SimpleTracer\nimport logging\n\n# Use default logger\ntracer = SimpleTracer()\n\n# Or use your own logger\nlogger = logging.getLogger(\"my_app.ai\")\ntracer = SimpleTracer(logger=logger)\n</code></pre> <p>The trace ID appears in every log line, so you can grep for it:</p> <pre><code>grep \"abc12345\" app.log\n</code></pre>"},{"location":"guides/tracing/#logfiretracer","title":"LogfireTracer","text":"<p>Integrates with Pydantic Logfire, a modern observability platform built by the Pydantic team. Install with:</p> <pre><code>pip install fastroai[logfire]\n</code></pre> <p>Usage:</p> <pre><code>import logfire\nfrom fastroai import FastroAgent, LogfireTracer\n\n# Configure logfire once at startup\nlogfire.configure()\n\ntracer = LogfireTracer()\nagent = FastroAgent(model=\"openai:gpt-4o\")\nresponse = await agent.run(\"Hello!\", tracer=tracer)\n</code></pre> <p>View your traces in the Logfire dashboard at logfire.pydantic.dev.</p>"},{"location":"guides/tracing/#nooptracer","title":"NoOpTracer","text":"<p>Does nothing. Use when tracing is disabled, in tests, or when you need trace IDs for compatibility but don't want actual tracing:</p> <pre><code>from fastroai import NoOpTracer\n\ntracer = NoOpTracer()\n\n# Still generates trace IDs for compatibility\nasync with tracer.span(\"operation\") as trace_id:\n    result = await do_something()\n    # trace_id exists, but no logging happens\n</code></pre> <p>FastroAI uses <code>NoOpTracer</code> internally when you don't pass a tracer. Your code doesn't crash, you just don't get observability.</p>"},{"location":"guides/tracing/#the-tracer-protocol","title":"The Tracer Protocol","text":"<p>To integrate with your observability platform, implement this protocol:</p> <pre><code>from typing import Protocol, Any\nfrom contextlib import AbstractAsyncContextManager\n\nclass Tracer(Protocol):\n    def span(self, name: str, **attributes: Any) -&gt; AbstractAsyncContextManager[str]:\n        \"\"\"Create a traced span. Yields a unique trace ID.\"\"\"\n        ...\n\n    def log_metric(self, trace_id: str, name: str, value: Any) -&gt; None:\n        \"\"\"Log a metric associated with a trace.\"\"\"\n        ...\n\n    def log_error(self, trace_id: str, error: Exception, context: dict | None = None) -&gt; None:\n        \"\"\"Log an error associated with a trace.\"\"\"\n        ...\n</code></pre> <p>Three methods. That's it. <code>span()</code> creates a context manager that yields a trace ID. <code>log_metric()</code> records values during the span. <code>log_error()</code> records failures.</p>"},{"location":"guides/tracing/#platform-integrations","title":"Platform Integrations","text":""},{"location":"guides/tracing/#logfire","title":"Logfire","text":"<p>FastroAI includes a built-in <code>LogfireTracer</code>. See the LogfireTracer section above for usage.</p> <p>Logfire gives you a nice UI to explore traces, and the Pydantic team maintains it, so it plays well with PydanticAI.</p>"},{"location":"guides/tracing/#opentelemetry","title":"OpenTelemetry","text":"<p>OpenTelemetry is the standard for distributed tracing. Most observability platforms (Datadog, Honeycomb, Jaeger, etc.) support OTLP export:</p> <pre><code>import uuid\nfrom contextlib import asynccontextmanager\nfrom opentelemetry import trace as otel_trace\n\nclass OTelTracer:\n    def __init__(self):\n        self.tracer = otel_trace.get_tracer(\"fastroai\")\n\n    @asynccontextmanager\n    async def span(self, name: str, **attrs):\n        trace_id = str(uuid.uuid4())\n        with self.tracer.start_as_current_span(name) as span:\n            span.set_attribute(\"trace_id\", trace_id)\n            for key, value in attrs.items():\n                span.set_attribute(key, value)\n            yield trace_id\n\n    def log_metric(self, trace_id: str, name: str, value):\n        span = otel_trace.get_current_span()\n        span.set_attribute(f\"metric.{name}\", value)\n\n    def log_error(self, trace_id: str, error: Exception, context=None):\n        span = otel_trace.get_current_span()\n        span.record_exception(error)\n        span.set_status(otel_trace.Status(otel_trace.StatusCode.ERROR))\n</code></pre> <p>Configure your OTLP exporter separately, then spans show up in your platform.</p>"},{"location":"guides/tracing/#using-tracers","title":"Using Tracers","text":""},{"location":"guides/tracing/#with-agents","title":"With Agents","text":"<pre><code>response = await agent.run(\"Hello!\", tracer=tracer)\nprint(response.trace_id)\n</code></pre>"},{"location":"guides/tracing/#with-pipelines","title":"With Pipelines","text":"<pre><code>result = await pipeline.execute(\n    {\"document\": doc},\n    deps=my_deps,\n    tracer=tracer,\n)\n</code></pre> <p>The tracer flows through to all steps. Each step's agent calls share the same trace context.</p>"},{"location":"guides/tracing/#custom-spans","title":"Custom Spans","text":"<p>Create your own spans within step execution for operations you want to trace:</p> <pre><code>class MyStep(BaseStep[MyDeps, str]):\n    async def execute(self, ctx: StepContext[MyDeps]) -&gt; str:\n        if ctx.tracer:\n            async with ctx.tracer.span(\"custom_operation\", user_id=ctx.deps.user_id) as trace_id:\n                result = await self.do_something()\n                ctx.tracer.log_metric(trace_id, \"result_size\", len(result))\n                return result\n\n        # Fallback when no tracer\n        return await self.do_something()\n</code></pre>"},{"location":"guides/tracing/#what-gets-traced","title":"What Gets Traced","text":"<p>FastroAI automatically creates spans for agent runs (<code>fastroai.agent.run</code>, <code>fastroai.agent.run_stream</code>) and pipeline execution (<code>pipeline.{name}</code>). Within those spans, it logs token usage metrics: <code>input_tokens</code>, <code>output_tokens</code>, and <code>cost_microcents</code>.</p> <p>The trace ID is included in every <code>ChatResponse</code>:</p> <pre><code>response = await agent.run(\"Hello!\")\nprint(response.trace_id)  # Use this to correlate with your logs\n</code></pre> <p>Log the trace ID in your application code, and you can trace from \"user clicked button\" all the way to \"AI returned 347 tokens\".</p>"},{"location":"guides/tracing/#trace-correlation","title":"Trace Correlation","text":"<p>The real value of tracing is correlation. Here's how to connect AI calls with your request handling:</p> <pre><code>from fastroai import LogfireTracer\n\nasync def handle_request(request):\n    tracer = LogfireTracer()\n\n    # Your business logic span\n    async with tracer.span(\"handle_request\", user_id=request.user_id) as parent_trace:\n        # AI call is nested under your span\n        response = await agent.run(request.message, tracer=tracer)\n\n        # Log business metrics\n        tracer.log_metric(response.trace_id, \"response_length\", len(response.content))\n\n        return response.output\n</code></pre> <p>When something goes wrong, search your observability platform by trace ID to see:</p> <ol> <li>The HTTP request came in</li> <li>You authenticated the user</li> <li>The AI call started</li> <li>Token usage and cost</li> <li>Where it failed or how long it took</li> <li>The response went out</li> </ol> <p>Without tracing, you're debugging blind.</p>"},{"location":"guides/tracing/#production-considerations","title":"Production Considerations","text":"<p>Sampling: In high-volume production, you might not want to trace every request. Most observability platforms support sampling - trace 10% of requests, or always trace errors.</p> <p>Costs: Tracing adds some overhead. The overhead is small (microseconds), but the data you send to your observability platform costs money. Consider what you actually need.</p> <p>Sensitive data: Don't log prompts or responses in production traces. They might contain PII or sensitive business data. Log token counts and costs, not content.</p>"},{"location":"guides/tracing/#key-files","title":"Key Files","text":"Component Location Tracer protocol <code>fastroai/tracing/tracer.py</code> SimpleTracer <code>fastroai/tracing/tracer.py</code> LogfireTracer <code>fastroai/tracing/tracer.py</code> NoOpTracer <code>fastroai/tracing/tracer.py</code> <p>\u2190 Safe Tools Recipes \u2192</p>"},{"location":"learn/","title":"Learn FastroAI","text":"<p>Build AI applications by understanding how they actually work.</p> <p>This isn't a feature tour. We start from how language models work, build up to agents and tools, and end with production patterns. Each section builds on the previous, and by the end you'll understand not just how to use FastroAI, but why it's designed the way it is.</p> <p>Choose Your Starting Point</p> <p>New to LLMs? Start from the beginning. We cover tokenization, transformers, and what's actually happening when you call an API.</p> <p>Already understand LLMs? Jump to Section 2 (Your First Agent) to start building.</p> <p>Know PydanticAI? Skip to Section 3 (Letting Agents Do Things) for tools and what FastroAI adds.</p> <p>Just want code? Check the Quick Start for the 2-minute setup.</p>"},{"location":"learn/#what-youll-learn","title":"What You'll Learn","text":"FoundationsBuilding ApplicationsProduction Patterns <p>Understand what you're building on</p> <ul> <li>How language models actually work (not just API calls)</li> <li>Tokenization and why it matters for costs</li> <li>What LLMs do well and where they fail</li> <li>The mechanics behind \"AI agents\"</li> </ul> <p>Create real AI features</p> <ul> <li>Agents with system prompts</li> <li>Tools that let agents interact with the world</li> <li>Structured output for type-safe responses</li> <li>Cost tracking and error handling</li> </ul> <p>Ship with confidence</p> <ul> <li>Multi-step workflows with pipelines</li> <li>Tracing and observability</li> <li>RAG for when the model doesn't know enough</li> </ul>"},{"location":"learn/#the-learning-path","title":"The Learning Path","text":"<ul> <li> <p> 0. How Language Models Work</p> <p>Understanding what you're actually calling</p> <p>From early NLP to modern transformers. Tokenization, attention, and why \"predict the next token\" leads to surprisingly capable systems. You'll understand what happens between your API call and the response.</p> <p> Start here</p> </li> <li> <p> 1. What LLMs Can and Can't Do</p> <p>Capabilities and limitations</p> <p>LLMs are good at specific things and terrible at others. Understanding this gap is what separates working applications from impressive demos that break in production.</p> <p> Continue</p> </li> <li> <p> 2. Your First Agent</p> <p>From API calls to agents</p> <p>Creating a FastroAgent, writing system prompts that work, running queries, and understanding what comes back. The foundation for everything else.</p> <p> Continue</p> </li> <li> <p> 3. Letting Agents Do Things</p> <p>Tools and function calling</p> <p>An agent that can only talk isn't very useful. Tools let agents call APIs, query databases, and interact with the world. <code>@safe_tool</code> makes them production-safe.</p> <p> Continue</p> </li> <li> <p> 4. Getting Data Back, Not Just Text</p> <p>Structured output</p> <p>Your agent returns text, but your application needs data. Using Pydantic models to get type-safe responses you can actually work with.</p> <p> Continue</p> </li> <li> <p> 5. Tracking What You Spend</p> <p>Tokens cost money</p> <p>Remember tokens from Section 0? Each one costs money. Why microcents matter for billing, how to track usage across calls, and setting budgets before you get surprised.</p> <p>(Coming soon)</p> </li> <li> <p> 6. When Things Go Wrong</p> <p>Error handling</p> <p>APIs time out. External services fail. The model returns something unexpected. Building AI applications that handle failures gracefully instead of crashing.</p> <p>(Coming soon)</p> </li> <li> <p> 7. Multi-Step Workflows</p> <p>Pipelines</p> <p>Real tasks need multiple steps: classify, then research, then write. Pipelines handle dependencies, run independent steps in parallel, and aggregate costs across the workflow.</p> <p>(Coming soon)</p> </li> <li> <p> 8. Finding Problems in Production</p> <p>Tracing and observability</p> <p>Something's slow. Something's expensive. But what? Tracing lets you see inside your AI calls and correlate them with the rest of your application.</p> <p>(Coming soon)</p> </li> <li> <p> 9. Retrieval Augmented Generation</p> <p>When the model doesn't know enough</p> <p>LLMs have knowledge cutoffs and don't know your data. RAG combines retrieval (finding relevant documents) with generation (answering based on them). Embeddings, vector search, and grounding responses in real data.</p> <p>(Coming soon)</p> </li> </ul>"},{"location":"learn/#alternative-learning-paths","title":"Alternative Learning Paths","text":"By TimeBy BackgroundBy Goal <ul> <li>1 hour: Sections 0-2 \u2192 Understand LLMs and build your first agent</li> <li>Half day: Sections 0-6 \u2192 Build a complete AI feature with tools</li> <li>Full day: All sections \u2192 Production-ready with pipelines, tracing, and RAG</li> </ul> <ul> <li>New to AI: Start from Section 0, don't skip the foundations</li> <li>Know ML, new to LLMs: Skim Section 0, focus on 1-2</li> <li>Know LLMs, new to PydanticAI: Start at Section 2</li> <li>Know PydanticAI: Jump to Section 3 for tools and <code>@safe_tool</code></li> </ul> <ul> <li>\"I want to understand how LLMs work\" \u2192 Sections 0-1</li> <li>\"I want to build an AI feature\" \u2192 Sections 2-6</li> <li>\"I want to build complex workflows\" \u2192 Sections 2-5, then 7</li> <li>\"I want to add RAG to my app\" \u2192 Sections 0-2, then 9</li> </ul>"},{"location":"learn/#prerequisites","title":"Prerequisites","text":"<p>You should be comfortable with:</p> <ul> <li>Python async/await syntax</li> <li>Basic Pydantic models</li> <li>Environment variables and API keys</li> </ul> <p>You don't need prior experience with:</p> <ul> <li>Machine learning or NLP (we start from the beginning)</li> <li>PydanticAI (we cover what you need)</li> <li>Production infrastructure (we build up to it)</li> </ul> <p>Start Learning \u2192 Browse Guides \u2192</p>"},{"location":"learn/0-how-language-models-work/","title":"How Language Models Work","text":"<p>Understanding what happens between your code and the AI response.</p> <p>When you call an LLM API, something happens between your text and the response. Hopefully understanding what's actually happening will help you build better applications and debug problems when things go wrong. Either way it's a really cool thing to learn.</p> <p>You don't need to understand all of the math, but you do need to understand the mechanics well enough to know why prompts work the way they do, why costs scale with tokens, and why models sometimes produce nonsense.</p>"},{"location":"learn/0-how-language-models-work/#the-challenge-computers-and-text","title":"The Challenge: Computers and Text","text":"<p>Computers have been working with text since the beginning. Early systems stored characters as numbers (ASCII gave us 'A' = 65, 'B' = 66, and so on), and we could search for exact strings, count word frequencies, and match patterns with regular expressions.</p> <p>This worked fine for many tasks. Database queries, log parsing, form validation - if you know exactly what you're looking for, exact string matching is fast and reliable.</p> <p>But language is messy. Someone reports their app is \"broken\" when your error logs say \"connection timeout.\" A user asks about \"cheap options\" when your pricing page says \"affordable plans.\" Exact string matching doesn't understand that these refer to the same things.</p> <p>The field of Natural Language Processing (NLP) spent decades trying to solve this. Early approaches used rules and dictionaries - manually curated synonym lists, grammar parsers, sentiment lexicons. These worked for narrow domains but getting nuance right is hard. You can't anticipate every way someone might phrase a question.</p> <p>What we actually need is a system that understands meaning, not just matches characters. But there's a fundamental constraint: computers work with numbers. They can add, compare, and transform numbers easily. They're terrible at understanding that \"I'm furious\" and \"I'm angry\" mean roughly the same thing, or that \"bank\" means something different in \"river bank\" versus \"bank account.\"</p> <p>If we want computers to work with language meaningfully, we need to turn text into numbers somehow. But how?</p>"},{"location":"learn/0-how-language-models-work/#representing-text-as-numbers","title":"Representing Text as Numbers","text":"<p>If we want to find text that's semantically similar (not just string-matching), we need some way to measure how \"close\" two pieces of text are. We need a distance metric.</p> <p>Let's start simple. The words \"king\" and \"queen\" are related - both are royalty. Maybe we can use letter patterns? Compare characters, count letters, look at structure.</p> <p>But then \"king\" and \"ring\" share three out of four letters. Structurally they're almost identical. Yet semantically, \"king\" and \"queen\" are far more related than \"king\" and \"ring.\"</p> <p>Letter structure doesn't capture meaning. The only thing computers can measure distance between is numbers. So we have two problems:</p> <ol> <li>How do we turn words into numbers?</li> <li>How do we make the distance between those numbers represent semantic similarity?</li> </ol> <p>The most obvious approach to turn words into numbers is giving each word a unique identifier. \"Apples\" is word #1, \"oranges\" is word #2, \"trucks\" is word #3. We can represent this as a vector where only one position is \"hot\" (set to 1):</p> I like apples oranges trucks I like apples 1 1 1 0 0 I like oranges 1 1 0 1 0 I like trucks 1 1 0 0 1 <p>Now we have text as numbers. But if you have 50,000 words in your vocabulary, each word becomes a vector of 50,000 numbers with a single 1 and 49,999 zeros. These vectors are huge and mostly empty - we call them \"sparse.\"</p> <p>Also, the representation tells us nothing about meaning (the second problem we need to tackle). The distance between \"apples\" and \"oranges\" might be exactly the same as the distance between \"apples\" and \"trucks.\" We've turned text into numbers, but those numbers don't capture any semantic information.</p> <p>Think about your phone's autocomplete. When you type \"I had a delicious,\" it suggests \"meal,\" \"dinner,\" or \"breakfast\" - not \"quantum\" or \"democracy.\" It learned that certain words follow \"delicious\" more often than others by analyzing lots of text.</p> <p>This next-word prediction has been around for years, and it partially solves our problem. If a model can predict that \"meal\" is likely after \"I had a delicious,\" it must have learned something about what \"delicious\" means and what things can be delicious.</p> <p>Word embeddings exploit this: words that appear in similar contexts have similar meanings.</p> <p>\"Apple\" and \"orange\" both show up near words like \"fruit,\" \"eat,\" \"juice,\" \"fresh.\" They appear in similar sentence structures: \"I ate an ,\" \"The ___ was ripe,\" \"A fresh ___.\" So if you train a model to predict words from their surrounding context, words that fit the same contexts will develop similar internal representations. Process billions of sentences, and the model learns number patterns where semantically similar words end up close together.</p> <p>The training works like this: start with random real numbers for each word's vector. Read millions of sentences and try to predict words from context. When the model sees \"I bought some ___ at the market,\" it should predict \"apples\" or \"oranges\" are likely, while \"trucks\" is not.</p> <p>Each wrong prediction adjusts the vectors slightly. After processing billions of sentences, words that appear in similar contexts end up with similar number patterns. Instead of sparse vectors with mostly zeros, you get dense vectors - maybe 768 or 1536 dimensions - where every number carries some meaning.</p> <pre><code>block-beta\n    columns 2\n    A[\"One-hot (sparse)\"]:1 B[\"Embedding (dense)\"]:1\n    C[\"apple \u2192 [1,0,0,0,0...]\"]:1 D[\"apple \u2192 [0.23, -0.15, 0.89...]\"]:1\n    E[\"orange \u2192 [0,1,0,0,0...]\"]:1 F[\"orange \u2192 [0.19, -0.14, 0.91...]\"]:1</code></pre> <p>The word \"apple\" might become <code>[0.23, -0.15, 0.89, ...]</code> while \"orange\" becomes <code>[0.19, -0.14, 0.91, ...]</code>. The numbers are similar because these words appeared in similar contexts. A word like \"democracy\" would have completely different numbers - it shows up near \"vote,\" \"government,\" \"freedom.\"</p> <p>We don't interpret individual dimensions - what does dimension 347 represent? We don't know and don't need to. What matters is that similar concepts end up pointing in similar directions in this high-dimensional space.</p> <p>These vectors capture nuanced relationships. The famous example: <code>king - man + woman \u2248 queen</code>. Vector arithmetic works because the model \"learned\" gender relationships from context.</p>"},{"location":"learn/0-how-language-models-work/#tokenization","title":"Tokenization","text":"<p>Before a model can work with your text, it needs to break it into pieces. Each piece gets an embedding, and those embeddings flow through the model. But how do we split text?</p> <p>The obvious approach: split on spaces. \"The cat sat\" becomes [\"The\", \"cat\", \"sat\"]. Simple.</p> <p>But this creates problems. English has hundreds of thousands of words. Add technical terms, names, typos, and other languages - the vocabulary would be enormous. Every word needs its own embedding vector. And what happens when you encounter a word you've never seen? \"Cryptocurrency\" probably wasn't in any training data from 2005.</p> <p>What about the opposite extreme? Split into individual characters. The vocabulary shrinks to maybe 100 characters - letters, digits, punctuation. No out-of-vocabulary problem. Every possible word can be spelled.</p> <p>But now \"cat\" becomes [\"c\", \"a\", \"t\"]. Three tokens instead of one. A sentence becomes dozens of tokens. The model has to learn that \"c-a-t\" means a furry animal - the relationship between characters and meaning isn't obvious. Sequences get very long, and longer sequences are slower and more expensive to process.</p> <p>Subword tokenization finds a middle ground. Instead of words or characters, you learn a vocabulary of useful pieces. Common words like \"the\" stay whole. Rare words get split into recognizable chunks:</p> <pre><code>\"Hello, world!\" \u2192 [\"Hello\", \",\", \" world\", \"!\"]\n\"unhappiness\" \u2192 [\"un\", \"happiness\"]\n\"cryptocurrency\" \u2192 [\"crypt\", \"ocur\", \"rency\"]\n</code></pre> <p>The model learns embeddings for these subwords. Linguists call these meaningful units \"morphemes\" - \"un\" often means negation, \"ness\" turns adjectives into nouns, \"ed\" marks past tense. Subword tokenization rediscovers this structure from data. The model can handle words it's never seen by combining pieces it knows.</p> <p>This is important for you in two ways (beyond the fact that it's cool):</p> <p>Costs are per-token, not per-word. A short message with unusual words might cost more than a longer message with common words. \"Supercalifragilisticexpialidocious\" is 7 tokens, while \"the cat sat on the mat\" is 6.</p> <p>Context windows are measured in tokens. When a model has a \"128K context window,\" that's tokens, not words. A document with lots of code, technical terms, or non-English text uses more tokens than you might expect.</p> <p>You can experiment with this using OpenAI's tiktoken library or their online tokenizer to see exactly how your text gets split.</p>"},{"location":"learn/0-how-language-models-work/#how-language-models-work_1","title":"How Language Models Work","text":"<p>We now have a way to turn text into meaningful numbers. But embeddings alone just give us word vectors - they don't generate text.</p> <p>The next step was predicting not just similar words, but the actual next word in a sequence. This is the core insight behind modern LLMs: predict the next token.</p> <p>Given \"The cat sat on the,\" what comes next? Probably \"mat\" or \"floor\" or \"couch.\" Almost certainly not \"democracy\" or \"quantum.\"</p> <p>Training a language model means showing it billions of sentences and asking it to predict what comes next. When it guesses wrong, it adjusts its internal weights slightly. After seeing enough text, the model becomes very good at predicting likely continuations.</p> <p>This sounds almost too simple. But predicting the next token requires a lot of implicit knowledge:</p> <ul> <li>Grammar: \"The cat sat on the ___\" needs a noun</li> <li>Semantics: The noun should be something a cat can sit on</li> <li>World knowledge: Cats sit on mats, floors, laps, keyboards</li> <li>Context: If earlier text mentioned a garden, \"bench\" becomes more likely</li> </ul> <p>When you prompt a model with \"Write a poem about the ocean,\" you're not giving it a special instruction. You're giving it tokens, and it's predicting what tokens would likely follow. Text that starts with \"Write a poem\" is usually followed by... a poem. So it generates one.</p> <p>Early language models processed text sequentially, one word at a time, carrying forward a compressed representation. They struggled with long-range dependencies. In \"The book that the student borrowed was overdue,\" which word does \"was\" refer to? The book, not the student. Sequential models often got confused.</p> <p>Attention mechanisms changed this. Instead of processing tokens in order, attention lets the model look at all tokens simultaneously and decide which ones are relevant to each other.</p> <p>When processing \"was\" in our example, the model learns to pay attention to \"book\" (the subject) while mostly ignoring \"student\" (inside a relative clause). This attention pattern is learned from data, not programmed.</p> <p>\"Bank\" in \"river bank\" attends strongly to \"river.\" \"Bank\" in \"bank account\" attends to \"account.\" Same word, different attention patterns, different meanings.</p> <pre><code>flowchart LR\n    subgraph s1[\"river bank\"]\n        A[The] -.-&gt; B[river]\n        B ==&gt; C[bank]\n        D[is] -.-&gt; C\n        E[muddy] -.-&gt; C\n    end\n\n    subgraph s2[\"bank account\"]\n        F[My] -.-&gt; I[bank]\n        I ==&gt; G[account]\n        H[is] -.-&gt; I\n        J[empty] -.-&gt; I\n    end</code></pre> <p>The thick arrows show where \"bank\" pays the most attention - it looks at \"river\" in one context and \"account\" in the other to figure out its meaning.</p> <p>The Transformer architecture (2017's \"Attention Is All You Need\" paper) stacks many layers of attention. Each layer refines the representation, building increasingly abstract understanding of the text.</p> <p>The key advantage is parallelization. Unlike sequential models that process one token at a time, transformers process all tokens simultaneously. This made it possible to train on vastly more data than before.</p> <p>GPT models are transformers trained on internet-scale text. GPT-3 was trained on hundreds of billions of tokens. GPT-4 and Claude on even more. The architecture is conceptually simple: stack attention layers, train on lots of text, predict the next token.</p> <p>What's interesting is that this simple objective leads to emergent capabilities. Models trained this way learn to answer questions (because Q&amp;A exists in training data), write code (because code exists), reason through problems (because step-by-step reasoning exists), and follow instructions (because instructional text exists).</p> <p>Nobody explicitly programmed these capabilities. They emerged from scale and the training process.</p>"},{"location":"learn/0-how-language-models-work/#the-full-pipeline","title":"The Full Pipeline","text":"<p>When you call <code>agent.run(\"What's the weather in Paris?\")</code>, this is the sequence:</p> <pre><code>flowchart TB\n    A[Your text] --&gt; B[Tokenize]\n    B --&gt; C[Embed tokens]\n    C --&gt; D[Transformer layers]\n    D --&gt; E[Probability distribution]\n    E --&gt; F[Sample next token]\n    F --&gt; G{Done?}\n    G --&gt;|No| D\n    G --&gt;|Yes| H[Response]</code></pre> <p>Your text gets split into tokens, each token becomes an embedding vector, those vectors pass through transformer layers with attention refining the representation at each layer. The final layer produces a probability distribution over all possible next tokens, the model samples one (with randomness controlled by temperature), and then it repeats until a stop condition.</p> <p>The response is built token by token, left to right. The model doesn't plan the whole response and then write it. It generates each token based on everything before it - your prompt plus whatever it has generated so far.</p> <p>This explains why streaming works: each token can be sent as it's generated. It also explains why models sometimes contradict themselves or forget constraints - they're predicting locally, one token at a time, without a global plan.</p>"},{"location":"learn/0-how-language-models-work/#practical-implications","title":"Practical Implications","text":"<p>Understanding this architecture changes how you think about building with LLMs:</p> <p>Prompts aren't instructions in a special language. You're providing context that makes certain continuations more likely. \"You are a helpful assistant\" works because text written by helpful assistants looks different from text written by unhelpful ones.</p> <p>Token limits are fundamental, not artificial. The model can only attend to tokens in its context window. If your conversation exceeds the limit, older messages get cut. This isn't a bug - it's how transformers work.</p> <p>Cost scales with tokens. Longer prompts, longer responses, more money. Every token matters when you're paying for millions of them.</p> <p>Temperature controls randomness. At temperature 0, the model always picks the most likely next token. Higher temperatures make less likely tokens more probable - more creative, or more chaotic.</p> <p>Models don't remember between calls. Each API call is independent. What seems like memory is you sending previous messages as part of the current prompt.</p> <p>These aren't arbitrary API limitations, they're direct consequences of how language models work, their architecture.</p> <p>You now understand what happens between your code and the response. But knowing how models work doesn't tell you when to use them. In the next section, we'll look at what LLMs are actually good at - and more importantly, where they fail.</p> <p>What LLMs Can and Can't Do \u2192</p>"},{"location":"learn/1-what-llms-can-and-cant-do/","title":"What LLMs Can and Can't Do","text":"<p>Knowing when to use them - and when not to.</p> <p>LLMs pass medical licensing exams at 90%. They solve International Mathematical Olympiad problems at gold-medal level. They write code that passes 96% of programming challenges.</p> <p>Then you add one irrelevant sentence to a math problem, and accuracy drops by 65%.</p> <p>This is why you can't just point an LLM at a problem and expect reliable results. Production AI applications need pipelines: the LLM handles what it's good at (understanding language, deciding what to do), code handles what it's good at (computation, verification, external systems), and safeguards catch the failures that will happen. You should understand what LLMs are good at and where they break to structure these pipelines.</p>"},{"location":"learn/1-what-llms-can-and-cant-do/#what-theyre-good-at","title":"What They're Good At","text":"<p>LLMs excel when the answer exists somewhere in the patterns of human text.</p> <p>Text transformation. Summarization, paraphrasing, translation, style transfer. These tasks have massive training data, and models have seen countless examples of the same content expressed differently.</p> <p>Following formats. JSON, markdown tables, formal emails. The model has seen every common format millions of times.</p> <p>Simple code generation. Common patterns in popular languages. CRUD operations, API calls, data transformations.</p> <p>Classification and extraction. Spam detection, sentiment analysis, entity extraction. The model reads the input and predicts a label that fits the pattern.</p> <p>The common thread: pattern matching on training data produces useful results. The model isn't reasoning from first principles - it's recognizing familiar patterns and generating text that fits.</p>"},{"location":"learn/1-what-llms-can-and-cant-do/#where-they-break","title":"Where They Break","text":"<p>The failures connect directly to the architecture from Section 0.</p> <p>Arithmetic. Ask \"What's 78498743 \u00d7 3271123?\" and the model might get it wrong. It's predicting tokens, not computing.</p> <p>Factual accuracy. Models hallucinate - they generate plausible-sounding text that's completely false. Hallucination rates have improved from ~38% in 2021 to ~8% in 2025, but research suggests they're theoretically inevitable given how transformers work.</p> <p>Long context. Models claiming 128K token support degrade beyond 10% of their capacity. On multi-document summarization without retrieval, models score below 20% compared to a 56% human baseline.</p> <p>Reasoning. Complex logical chains are fragile. Add an irrelevant clause to a math problem, accuracy drops up to 65%. Chain-of-thought prompting helps some tasks but hurts others. Research found replacing the \"reasoning\" with meaningless filler tokens still produces good results - suggesting it's pattern retrieval, not actual reasoning.</p> <p>Real software engineering. The 96.2% HumanEval score tests isolated functions. On benchmarks requiring code reuse and progressive reasoning, scores drop to 76%. On real GitHub issues, much lower. AI-generated code introduces security vulnerabilities in 45% of cases. Code churn is projected to double compared to pre-AI codebases.</p> <pre><code>block-beta\n    columns 2\n    A[\"Junior devs with AI\"]:1 B[\"Senior devs with AI\"]:1\n    C[\"+27-39% productivity\"]:1 D[\"+8-13% productivity\"]:1</code></pre> <p>Productivity gains favor juniors. One study found experienced developers on familiar codebases took 19% longer with AI tools - prompting overhead, context mismatch, poor task-model fit.</p> <p>Autonomous agents. In controlled tests, the best AI agents completed only 24% of normal office tasks. Some got stuck on popup windows. Others renamed users to \"simulate\" task completion. Most enterprise AI pilot programs fail to deliver measurable business impact.</p>"},{"location":"learn/1-what-llms-can-and-cant-do/#the-confidence-problem","title":"The Confidence Problem","text":"<p>LLMs hardly ever say \"I don't know.\" They produce text with the same confident tone whether they're correct or fabricating.</p> <p>You can't tell from the output which is which. And you can't trust your intuition about whether AI is helping. Studies found developers using AI wrote less secure code while believing it was more secure. In one study, developers predicted AI would save them 24% time. After experiencing a 19% slowdown, they still estimated a 20% improvement.</p> <p>This matters for application design. Never trust model output for high-stakes decisions without verification. And measure whether AI is actually helping rather than assuming.</p>"},{"location":"learn/1-what-llms-can-and-cant-do/#working-with-limitations","title":"Working With Limitations","text":"<p>Even though LLMs are so unreliable at so many tasks, they can be used. The main idea is using LLMs for what they are good at, and using deterministic code for the rest.</p> <p>For arithmetic: Don't ask the model to calculate. Have it call code calculates. Let the model decide what to compute, but don't trust it to compute.</p> <p>For facts: Use Retrieval-Augmented Generation. Give the model source documents and have it answer based on those, not its training data. We cover this in Section 9.</p> <p>For long context: Don't rely on the model's ability to attend to everything. Chunk documents, retrieve relevant sections, pass only what's needed.</p> <p>For code: Use AI for boilerplate, documentation, and unfamiliar syntax. Review everything. Run security scans.</p> <p>For confidence: Add citations, fact-checking, or human review. Measure actual outcomes.</p> <p>The pattern: use LLMs for what they're good at, and use code for everything else.</p> <pre><code>flowchart LR\n    A[Task] --&gt; B{Language-based?}\n    B --&gt;|Yes| C{Can verify or&lt;br/&gt;approximate OK?}\n    B --&gt;|No| D[Use code]\n    C --&gt;|Yes| E[Use LLM]\n    C --&gt;|No| F{Can add&lt;br/&gt;retrieval?}\n    F --&gt;|Yes| E\n    F --&gt;|No| D</code></pre> <p>Real applications combine both. The LLM handles the fuzzy language parts. Traditional code handles everything else.</p> <p>You now understand what LLMs can and can't do. The next section moves from theory to practice: creating your first agent with FastroAI.</p> <p>Your First Agent \u2192</p>"},{"location":"learn/2-your-first-agent/","title":"Your First Agent","text":"<p>From API calls to something that acts.</p> <p>The word \"agent\" gets thrown around a lot in AI. Sometimes it means a system that can browse the web and write code autonomously. Sometimes it means a chatbot with a system prompt. The term is fuzzy because it comes from philosophy, got borrowed by AI researchers, and now means different things to different people.</p> <p>Before we build one, let's understand what we're actually talking about.</p>"},{"location":"learn/2-your-first-agent/#what-is-an-agent","title":"What Is an Agent?","text":"<p>The concept has deep roots. Aristotle linked agency to voluntary action arising from deliberate choice, distinguishing intentional behavior from involuntary reactions. Kant positioned autonomy (the capacity for self-governance) at the core of what it means to be an agent.</p> <p>The main concept: an agent is the source of action, not merely reacting to stimuli.</p> <p>In AI research, the standard textbook definition comes from Russell and Norvig: \"An agent is anything that perceives its environment (using sensors) and acts upon it (using actuators).\" This is deliberately broad and pragmatic - designed for engineering, not philosophy.</p> <p>For a more rigorous framework, Barandiaran, Di Paolo, and Rohde (2009) proposed three necessary conditions for genuine agency:</p> <ol> <li>Individuality - the system defines its own boundaries</li> <li>Interactional asymmetry - it acts on its environment, not just responds to it</li> <li>Normativity - it regulates activity according to internal goals</li> </ol> <p>Their synthesis: \"Agency is an autonomous organization that adaptively regulates its coupling with its environment and contributes to sustaining itself.\"</p> <p>Try applying this to an LLM and things get fuzzy. Is the system prompt part of the agent's goals? Is the context window its \"individuality\"? Where does the LLM end and the environment begin? These questions don't have principled answers because LLMs weren't designed with this framework in mind.</p>"},{"location":"learn/2-your-first-agent/#agents-in-the-llm-context","title":"Agents in the LLM Context","text":"<p>Yoav Goldberg cuts through the philosophical ambiguity with a pragmatic definition. An LLM agent has:</p> <ol> <li>A prompt defining its behavior</li> <li>Tools it can call</li> <li>A multi-step process with memory between calls</li> </ol> <p>That's what we'll use.</p> <pre><code>flowchart LR\n    A[User message] --&gt; B[Agent]\n    B &lt;--&gt; M[(Memory)]\n    B --&gt; C{Needs tool?}\n    C --&gt;|Yes| D[Call tool]\n    D --&gt; B\n    C --&gt;|No| E[Generate response]\n    E --&gt; F[Return to user]</code></pre> <p>The LLM decides what to do. Memory holds the conversation history and previous reasoning steps. If the agent needs information or capabilities it doesn't have, it calls a tool. The result goes back to the LLM, which continues reasoning. This loop repeats until the agent has an answer.</p> <p>This is different from a simple API call. With a raw LLM call, you send a prompt and get text back. With an agent, the LLM can take multiple steps, use external tools, and maintain context across those steps. The \"agency\" is in that loop - the system acts on its environment rather than just responding.</p>"},{"location":"learn/2-your-first-agent/#fastroagent","title":"FastroAgent","text":"<p>FastroAgent provides the tools to implement this loop. Looking at Goldberg's three components:</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You are a financial advisor assistant.\",  # 1. Prompt\n)\n</code></pre> <p>The prompt defines behavior. The model is the transformer from Section 0 - it predicts tokens based on the patterns it learned. When you call <code>run()</code>, you start the loop:</p> <pre><code>response = await agent.run(\"How should I budget $5000/month?\")\n</code></pre> <p>This single call might involve multiple steps internally. If the agent has tools (covered in Section 5), it can decide to call them, get results, and continue reasoning. The <code>run()</code> method handles that loop until the agent produces a final answer.</p> <p>Memory between calls is your responsibility. We'll see this shortly.</p> <p>FastroAgent wraps PydanticAI's Agent. The API is intentionally similar:</p> <pre><code># PydanticAI\nfrom pydantic_ai import Agent\n\nagent = Agent(model=\"openai:gpt-4o\", system_prompt=\"...\")\nresult = await agent.run(\"What is the capital of France?\")\nprint(result.output)\n\n# FastroAI\nfrom fastroai import FastroAgent\n\nagent = FastroAgent(model=\"openai:gpt-4o\", system_prompt=\"...\")\nresponse = await agent.run(\"What is the capital of France?\")\nprint(response.output)\n</code></pre> <p>FastroAgent creates a PydanticAI Agent internally. You can also pass your own:</p> <pre><code>pydantic_agent = Agent(model=\"openai:gpt-4o\", ...)\nagent = FastroAgent(agent=pydantic_agent)  # Wrap for cost tracking\n</code></pre> <p>The difference is what comes back. Remember from Section 0: every token costs money. And from Section 1: you need to verify and iterate. FastroAgent tracks both:</p> <pre><code>response.output          # Typed output (string here, Pydantic model with structured output)\nresponse.content         # Always the raw text\nresponse.input_tokens    # Tokens in your prompt\nresponse.output_tokens   # Tokens in the response\nresponse.cost_microcents # Cost in 1/1,000,000 of a dollar\nresponse.cost_dollars    # Cost as float for display\n</code></pre> <p>For text responses, <code>.output</code> and <code>.content</code> are the same. When you add structured output in Section 4, <code>.output</code> becomes your typed Pydantic model while <code>.content</code> stays as the raw text.</p> <p>Why microcents? Floating-point math has precision errors. <code>0.1 + 0.2</code> equals <code>0.30000000000000004</code> in Python. When you're tracking costs across thousands of calls, that matters. Integers are exact.</p>"},{"location":"learn/2-your-first-agent/#system-prompts","title":"System Prompts","text":"<p>Remember from Section 0: prompts aren't instructions in a special language. You're providing context that makes certain continuations more likely. \"You are a financial advisor\" works because text written by financial advisors looks different from text written by novelists.</p> <pre><code>agent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"\"\"You are a financial advisor assistant.\n\nYou help users understand their spending patterns and suggest budgets.\nYou speak in plain language, avoiding jargon.\nYou never give specific investment advice or stock recommendations.\nIf asked about investments, explain that you focus on budgeting and\nsuggest they consult a licensed financial advisor.\"\"\",\n)\n</code></pre> <p>Specific constraints work better than vague intentions. \"Never give investment advice\" is easier for the model to follow than \"be careful about financial advice.\" The model has seen many examples of what \"never do X\" looks like - it's a clear pattern to match.</p>"},{"location":"learn/2-your-first-agent/#temperature","title":"Temperature","text":"<p>Section 0 covered how the model produces a probability distribution over possible next tokens. Temperature controls how it samples from that distribution.</p> <pre><code># Consistent, predictable outputs\nagent = FastroAgent(model=\"openai:gpt-4o\", temperature=0.0)\n\n# More varied, creative outputs\nagent = FastroAgent(model=\"openai:gpt-4o\", temperature=1.0)\n</code></pre> <p>At temperature 0, the model always picks the most likely token. Higher values flatten the distribution, giving less likely tokens a better chance. For classification and extraction, use low temperature (0.0-0.3). For creative tasks, higher values (0.7-1.0) add variety.</p>"},{"location":"learn/2-your-first-agent/#the-agent-is-stateless","title":"The Agent Is Stateless","text":"<p>This is Goldberg's third component: memory between calls. But the memory isn't in the agent - it's yours to manage.</p> <pre><code>from pydantic_ai.messages import ModelMessage\n\nhistory: list[ModelMessage] = await my_storage.load(user_id)\n\nresponse = await agent.run(\n    \"What did I ask you earlier?\",\n    message_history=history,\n)\n\n# You decide what to save - typically the exchange\nawait my_storage.save(user_id, \"What did I ask you earlier?\", response.content)\n</code></pre> <p>Each <code>run()</code> call is independent. There's no hidden state. This connects directly to Section 0: the transformer has no memory between calls. What looks like continuous conversation is you assembling the context window - including previous messages - and sending it fresh each time.</p> <p>FastroAgent makes this explicit rather than hiding it. Where history lives (database, Redis, session) is your decision. How long to keep it, when to summarize, what to include - all your decisions.</p>"},{"location":"learn/2-your-first-agent/#streaming","title":"Streaming","text":"<p>Remember the generation pipeline from Section 0? The model produces tokens one at a time. Streaming lets you see each token as it's generated rather than waiting for the complete response:</p> <pre><code>async for chunk in agent.run_stream(\"Tell me a story\"):\n    if chunk.is_final:\n        print(f\"\\nCost: ${chunk.usage_data.cost_dollars:.6f}\")\n    else:\n        print(chunk.content, end=\"\", flush=True)\n</code></pre> <p>The final chunk includes complete usage data. Earlier chunks just have content as it arrives.</p>"},{"location":"learn/2-your-first-agent/#whats-next","title":"What's Next","text":"<p>You now have an agent that can respond to messages. But remember Section 1: LLMs can't calculate, can't look things up, can't interact with external systems. Right now your agent has the same limitations.</p> <p>Section 3 fixes that with tools - the code that handles what LLMs can't.</p> <p>Letting Agents Do Things \u2192</p>"},{"location":"learn/3-letting-agents-do-things/","title":"Letting Agents Do Things","text":"<p>Tools turn text generators into systems that act.</p> <p>Section 1 laid out the problem: LLMs can't calculate, can't look things up, can't interact with external systems. Section 2 introduced agents and mentioned tools as part of Goldberg's definition. Now we make agents useful.</p>"},{"location":"learn/3-letting-agents-do-things/#what-tools-do","title":"What Tools Do","text":"<p>Remember the pattern from Section 1: LLMs for language-based decisions, code for everything else. Tools are that \"everything else.\"</p> <pre><code>flowchart TB\n    A[User: What's 78498743 \u00d7 3271123?] --&gt; B[Agent]\n    B --&gt; C{Can I answer this?}\n    C --&gt;|No| D[Call calculator tool]\n    D --&gt; E[256,740,438,594,889]\n    E --&gt; B\n    C --&gt;|Yes| F[Response]\n    F --&gt; G[The result is 256,740,438,594,889]</code></pre> <p>The LLM doesn't compute the answer. It recognizes \"this needs calculation,\" calls a tool, and incorporates the result. The tool does the actual work.</p> <p>This is Goldberg's second component in action. An agent with tools can query databases, call APIs, read files, run calculations, and really interact with any system you give it access to</p> <p>The LLM decides what to do. The tool does it.</p>"},{"location":"learn/3-letting-agents-do-things/#defining-tools","title":"Defining Tools","text":"<p>PydanticAI tools are async functions with docstrings. The docstring tells the LLM what the tool does:</p> <pre><code>async def get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather for a location.\n\n    Args:\n        location: City name, e.g. \"Paris\" or \"New York\"\n    \"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{location}\")\n        return response.json()[\"description\"]\n</code></pre> <p>The function signature tells the LLM what arguments to pass. The return value goes back to the LLM for its next step.</p> <p>Register tools with an agent using toolsets:</p> <pre><code>from pydantic_ai.toolsets import FunctionToolset\nfrom fastroai import FastroAgent\n\ntoolset = FunctionToolset(tools=[get_weather])\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You can check the weather.\",\n    toolsets=[toolset],\n)\n\nresponse = await agent.run(\"What's the weather in Tokyo?\")\n</code></pre> <p>The agent sees the tool's name and docstring, decides whether to use it, and calls it with appropriate arguments.</p>"},{"location":"learn/3-letting-agents-do-things/#when-tools-fail","title":"When Tools Fail","text":"<p>That weather API will eventually time out because the server is slow, return an error because you hit a rate limit or just hang indefinitely because something's broken.</p> <p>With a regular tool, this crashes your entire request. The user sees an error page, you've wasted the prompt tokens. The conversation is dead (unless you deal with it).</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    participant API\n\n    User-&gt;&gt;Agent: What's the weather?\n    Agent-&gt;&gt;Tool: get_weather(\"Paris\")\n    Tool-&gt;&gt;API: HTTP request\n    API--xTool: Timeout\n    Tool--xAgent: Exception\n    Agent--xUser: 500 Error</code></pre> <p>The AI never gets a chance to respond gracefully.</p>"},{"location":"learn/3-letting-agents-do-things/#safe_tool","title":"@safe_tool","text":"<p><code>@safe_tool</code> changes this. When something fails, the AI receives an error message instead of an exception:</p> <pre><code>from fastroai import safe_tool\n\n@safe_tool(timeout=10, max_retries=2)\nasync def get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather for a location.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{location}\")\n        return response.json()[\"description\"]\n</code></pre> <p>Now when the API times out,</p> <p>the wrapper waits up to 10 seconds. If it fails, it retries (up to 2 times) using exponential backoff. If all attempts fail, it returns an error message</p> <pre><code>sequenceDiagram\n    participant User\n    participant Agent\n    participant Tool\n    participant API\n\n    User-&gt;&gt;Agent: What's the weather?\n    Agent-&gt;&gt;Tool: get_weather(\"Paris\")\n    Tool-&gt;&gt;API: HTTP request\n    API--xTool: Timeout\n    Tool--&gt;&gt;Agent: \"Tool timed out after 2 attempts\"\n    Agent-&gt;&gt;User: I'm having trouble checking weather right now...</code></pre> <p>The AI incorporates the failure into its response. The user gets something useful instead of an error page.</p> <p>The decorator options:</p> <pre><code>@safe_tool(\n    timeout=30,          # Seconds per attempt\n    max_retries=3,       # Total attempts\n    on_timeout=\"...\",    # Custom timeout message\n    on_error=\"...\",      # Custom error message\n)\n</code></pre> <p>Custom messages help the AI respond better:</p> <pre><code>@safe_tool(\n    timeout=30,\n    on_timeout=\"Weather service is slow. Suggest trying again later.\",\n    on_error=\"Weather unavailable: {error}. Offer to help with something else.\",\n)\nasync def get_weather(location: str) -&gt; str:\n    ...\n</code></pre> <p>The AI sees these messages as tool output. Clearer messages lead to better responses.</p>"},{"location":"learn/3-letting-agents-do-things/#when-to-use-safe_tool","title":"When to Use @safe_tool","text":"<p>Use it for anything that touches the outside world:</p> <ul> <li>Network requests to APIs</li> <li>Database queries</li> <li>File system operations</li> <li>Third-party services</li> </ul> <p>Skip it for pure computation - math, string manipulation, local data lookups. These either work or they don't, and if they don't, something's fundamentally broken.</p>"},{"location":"learn/3-letting-agents-do-things/#multiple-tools","title":"Multiple Tools","text":"<p>Group related tools into a toolset:</p> <pre><code>from pydantic_ai.toolsets import FunctionToolset\nfrom fastroai import FastroAgent, safe_tool\n\n@safe_tool(timeout=10)\nasync def get_weather(location: str) -&gt; str:\n    \"\"\"Get current weather.\"\"\"\n    ...\n\n@safe_tool(timeout=5)\nasync def get_time(timezone: str) -&gt; str:\n    \"\"\"Get current time in a timezone.\"\"\"\n    ...\n\ntoolset = FunctionToolset(tools=[get_weather, get_time])\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You can check weather and time.\",\n    toolsets=[toolset],\n)\n</code></pre> <p>The agent now has access to both tools and will choose which to call based on the user's question.</p>"},{"location":"learn/3-letting-agents-do-things/#a-complete-example","title":"A Complete Example","text":"<p>Let's create a very simple personal finance assistant that analyzes spending. This tool embeds some domain knowledge - it knows what percentage of income is reasonable for different categories:</p> <pre><code>async def analyze_spending(\n    monthly_income: float,\n    amount: float,\n    category: str\n) -&gt; str:\n    \"\"\"Analyze spending in a category relative to income.\n\n    Args:\n        monthly_income: Monthly income in dollars\n        amount: Amount spent in this category\n        category: Category name like \"food\", \"housing\", \"transportation\"\n    \"\"\"\n    percentage = (amount / monthly_income) * 100\n\n    # Financial guidelines by category\n    guidelines = {\n        \"food\": {\"reasonable\": 15, \"high\": 20},\n        \"housing\": {\"reasonable\": 30, \"high\": 35},\n        \"transportation\": {\"reasonable\": 15, \"high\": 20},\n    }\n\n    limits = guidelines.get(category.lower(), {\"reasonable\": 10, \"high\": 15})\n\n    if percentage &lt;= limits[\"reasonable\"]:\n        assessment = \"reasonable\"\n    elif percentage &lt;= limits[\"high\"]:\n        assessment = \"a bit high but manageable\"\n    else:\n        assessment = \"above recommended guidelines\"\n\n    return f\"{percentage:.1f}% of income on {category} - {assessment}\"\n</code></pre> <p>This is pure computation, no network calls, no external dependencies. It doesn't need <code>@safe_tool</code> because it can't fail in ways that would crash the request. Now let's create the agent with access to this tool:</p> <pre><code>from pydantic_ai.toolsets import FunctionToolset\nfrom fastroai import FastroAgent\n\ntoolset = FunctionToolset(tools=[analyze_spending])\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"\"\"You are a personal finance assistant.\n\nYou help people understand their spending patterns.\nUse the analyze_spending tool to calculate percentages and assessments.\nBe encouraging but realistic.\"\"\",\n    toolsets=[toolset],\n)\n</code></pre> <p>When someone asks:</p> <pre><code>response = await agent.run(\n    \"I earn $5000/month and spent $800 on food. Is that too much?\"\n)\n</code></pre> <p>The agent calls <code>analyze_spending(5000, 800, \"food\")</code>, gets back <code>\"16.0% of income on food - a bit high but manageable\"</code>, and generates a helpful response using that assessment.</p> <p>The LLM couldn't reliably calculate 800/5000 = 16%, and it definitely doesn't know financial guidelines. But it can recognize when analysis is needed and call a tool that does.</p>"},{"location":"learn/3-letting-agents-do-things/#whats-next","title":"What's Next","text":"<p>Tools let agents do things. But the agent's response is still just text - if you need to extract data from it, you're back to parsing strings.</p> <p>Section 4 adds structured output: the agent returns typed Pydantic models instead of text, so your code can work with real data.</p> <p>Getting Data Back, Not Just Text \u2192</p>"},{"location":"learn/4-getting-data-back/","title":"Getting Data Back, Not Just Text","text":"<p>From parsing strings to working with real data.</p> <p>Section 3 gave your agent capabilities: tools that calculate, fetch data, and interact with external systems. But when you ask \"Is my $800 food spending reasonable on $5000/month?\", the agent returns a string like \"Your spending of 16% on food is a bit high but manageable.\"</p> <p>Your code now has to figure out what to do with that. The answer is buried in natural language (is it reasonable? What was the percentage?).</p>"},{"location":"learn/4-getting-data-back/#the-problem-with-parsing-text","title":"The Problem With Parsing Text","text":"<p>Let's say you're building an invoice processor. The agent analyzes invoices and you need to route them based on amount and category:</p> <pre><code>response = await agent.run(\"Analyze this invoice: $2,340.50 for office supplies\")\n</code></pre> <p>The agent might return:</p> <p>\"This invoice is for $2,340.50 in the office supplies category. The amount exceeds the typical threshold for automatic approval.\"</p> <p>Now what? You could regex out the dollar amount, look for keywords like \"office supplies\", check for \"exceeds threshold\". But then the agent changes its phrasing - \"totaling $2,340.50\" instead of \"for $2,340.50\", or \"classified under office equipment\" instead of \"office supplies category.\" Your parsing breaks.</p> <p>You're fighting the model. LLMs are optimized to generate natural, varied language - not to maintain consistent output formats. Every regex you write is betting against that.</p>"},{"location":"learn/4-getting-data-back/#structured-output","title":"Structured Output","text":"<p>PydanticAI (which FastroAI builds on) solves this by constraining the model to return valid instances of Pydantic models. Instead of free-form text, you get typed data:</p> <pre><code>from pydantic import BaseModel\n\nclass InvoiceAnalysis(BaseModel):\n    amount: float\n    category: str\n    requires_approval: bool\n    reason: str\n</code></pre> <p>The model's output must conform to this schema. Not \"try to match it\" - the output is validated and typed.</p> <pre><code>from fastroai import FastroAgent\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You analyze invoices and determine if they need approval.\",\n    output_type=InvoiceAnalysis,\n)\n\nresponse = await agent.run(\"Analyze: $2,340.50 for office supplies\")\n</code></pre> <p>Now <code>response.output</code> is an <code>InvoiceAnalysis</code> instance:</p> <pre><code>print(response.output.amount)           # 2340.5\nprint(response.output.category)         # \"office supplies\"\nprint(response.output.requires_approval) # True\nprint(response.output.reason)           # \"Amount exceeds $1000 threshold\"\n</code></pre> <p>No parsing, regex or hoping the model used the right words. The model's natural language ability is still there - it figured out the category, determined whether approval is needed, and explained why. But the output is structured data your code can work with.</p>"},{"location":"learn/4-getting-data-back/#how-it-works","title":"How It Works","text":"<p>When you specify <code>output_type</code>, PydanticAI sends the model a JSON schema derived from your Pydantic model. The model generates JSON that conforms to that schema, and PydanticAI validates and instantiates it.</p> <p>This connects back to Section 0: the model is still predicting tokens, but now it's predicting tokens that form valid JSON matching your schema. Models are trained on vast amounts of structured data - JSON, code, formatted documents - so generating structured output is something they're already good at.</p> <p>The <code>ChatResponse</code> gives you both forms:</p> <pre><code>response.output   # InvoiceAnalysis instance - use this\nresponse.content  # JSON string representation - rarely needed\n</code></pre> <p>For text agents (no <code>output_type</code>), <code>.output</code> and <code>.content</code> are both the string response. With structured output, <code>.output</code> is your typed model instance.</p>"},{"location":"learn/4-getting-data-back/#defining-good-schemas","title":"Defining Good Schemas","text":"<p>Your schema teaches the model what you want. Field names and types matter:</p> <pre><code>class SpendingAnalysis(BaseModel):\n    percentage_of_income: float\n    category: str\n    assessment: str  # What does this mean? Vague.\n</code></pre> <p>The model has to guess what \"assessment\" should contain. Is it a grade? A description? A recommendation?</p> <p>Better:</p> <pre><code>from typing import Literal\n\nclass SpendingAnalysis(BaseModel):\n    percentage_of_income: float\n    category: str\n    rating: Literal[\"reasonable\", \"high\", \"excessive\"]\n    recommendation: str\n</code></pre> <p>Now the model knows exactly what <code>rating</code> can be. It can't return \"pretty good\" or \"could be better\" - it must pick from the specified values. Your code can switch on <code>rating</code> without parsing.</p> <p>Field descriptions and examples make it even clearer:</p> <pre><code>from pydantic import Field\n\nclass SpendingAnalysis(BaseModel):\n    \"\"\"Analysis of spending in a budget category.\"\"\"\n\n    percentage_of_income: float = Field(\n        description=\"Spending as percentage of monthly income (0-100)\",\n        examples=[16.0, 32.5],\n    )\n    category: str = Field(\n        description=\"Budget category\",\n        examples=[\"food\", \"housing\", \"transportation\"],\n    )\n    rating: Literal[\"reasonable\", \"high\", \"excessive\"] = Field(\n        description=\"Assessment based on standard financial guidelines\",\n    )\n    recommendation: str = Field(\n        description=\"One actionable suggestion for this category\",\n        examples=[\"Consider meal planning to reduce costs\"],\n    )\n</code></pre> <p>PydanticAI includes these in the JSON schema sent to the model. Clearer descriptions and concrete examples lead to better output.</p>"},{"location":"learn/4-getting-data-back/#a-complete-example","title":"A Complete Example","text":"<p>Let's combine structured output with the spending analysis from Section 3. The tool does the calculation, and structured output ensures we get data we can use:</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nclass SpendingReport(BaseModel):\n    \"\"\"Report on spending analysis.\"\"\"\n\n    category: str\n    amount: float\n    percentage: float\n    rating: Literal[\"reasonable\", \"high\", \"excessive\"]\n    suggestion: str\n</code></pre> <p>The agent and tool work together:</p> <pre><code>from pydantic_ai.toolsets import FunctionToolset\nfrom fastroai import FastroAgent\n\nasync def analyze_spending(\n    monthly_income: float,\n    amount: float,\n    category: str\n) -&gt; str:\n    \"\"\"Analyze spending in a category relative to income.\"\"\"\n    percentage = (amount / monthly_income) * 100\n\n    guidelines = {\n        \"food\": {\"reasonable\": 15, \"high\": 20},\n        \"housing\": {\"reasonable\": 30, \"high\": 35},\n        \"transportation\": {\"reasonable\": 15, \"high\": 20},\n    }\n\n    limits = guidelines.get(category.lower(), {\"reasonable\": 10, \"high\": 15})\n\n    if percentage &lt;= limits[\"reasonable\"]:\n        assessment = \"reasonable\"\n    elif percentage &lt;= limits[\"high\"]:\n        assessment = \"high but manageable\"\n    else:\n        assessment = \"above recommended guidelines\"\n\n    return f\"{percentage:.1f}% of income on {category} - {assessment}\"\n\ntoolset = FunctionToolset(tools=[analyze_spending])\n\nagent = FastroAgent(\n    model=\"openai:gpt-4o\",\n    system_prompt=\"You analyze personal finances and provide actionable advice.\",\n    output_type=SpendingReport,\n    toolsets=[toolset],\n)\n</code></pre> <p>When we run the agent:</p> <pre><code>response = await agent.run(\n    \"I make $5000/month and spent $800 on food. How am I doing?\"\n)\n</code></pre> <p>The agent calls the tool, gets the calculation result, and returns a structured report:</p> <pre><code>print(response.output.category)     # \"food\"\nprint(response.output.percentage)   # 16.0\nprint(response.output.rating)       # \"high\"\nprint(response.output.suggestion)   # \"Consider meal planning to reduce food costs\"\n</code></pre> <p>The tool did the math reliably (Section 1: use code for computation). The model interpreted the result and generated advice (Section 1: use LLMs for language). Structured output made the response programmatically useful.</p>"},{"location":"learn/4-getting-data-back/#when-to-use-structured-output","title":"When to Use Structured Output","text":"<p>Use it when your code needs to act on the response:</p> <ul> <li>Routing decisions (send this to approval, file it here)</li> <li>Data extraction (pull fields from documents)</li> <li>Multi-step workflows (pass data to the next step)</li> <li>API responses (return JSON to clients)</li> <li>Validation (check that required fields exist)</li> </ul> <p>Skip it when you just need text:</p> <ul> <li>Conversational responses to users</li> <li>Creative writing</li> <li>Explanations meant for humans</li> </ul> <p>The overhead is minimal, but if you're just showing the response to a user, plain text is simpler.</p>"},{"location":"learn/4-getting-data-back/#whats-next","title":"What's Next","text":"<p>You now have agents that can do things (tools) and return structured data (output types). But every token costs money - Section 0 explained why. When you're running thousands of requests, those costs add up fast.</p> <p>Section 5 covers cost tracking: measuring what you spend, understanding where the money goes, and setting budgets before you get surprised.</p> <p>Tracking What You Spend \u2192</p>"},{"location":"recipes/","title":"Recipes","text":"<p>Copy-paste solutions for common patterns.</p> <p>Recipes are complete, working examples you can adapt for your use case. Unlike Guides (which explain how things work), Recipes give you code you can drop into your project immediately.</p> <p>Coming Soon</p> <p>Recipes are being written. In the meantime, check out the Guides for detailed explanations of each feature, or see the examples in the GitHub repo.</p> <ul> <li> <p> More Recipes Coming Soon</p> <p>We're working on recipes for common patterns like document processing, parallel research, conversation bots, and cost budget enforcement.</p> </li> </ul>"},{"location":"recipes/#what-makes-a-good-recipe","title":"What Makes a Good Recipe","text":"<p>Each recipe includes:</p> Section What It Contains Problem What you're trying to solve Solution Complete, runnable code How It Works Brief explanation of key parts Variations Common modifications <p>Recipes assume you've read the Quick Start and understand FastroAgent basics.</p>"},{"location":"recipes/#contributing","title":"Contributing","text":"<p>Have a pattern that would help others? We welcome recipe contributions. Open a PR on GitHub with your recipe following the format above.</p> <p>\u2190 Guides API Reference \u2192</p>"}]}